\documentclass{article}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\usepackage{geometry}
\geometry{margin=1in}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
}

\usepackage{wrapfig}
\usepackage{graphicx}

\usepackage{array}
\graphicspath{ {./images/} }

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}

\usepackage{xcolor,soul,lipsum}

\makeatletter
\newcommand\subsubsubsection{\@startsection{paragraph}{4}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}}
\newcommand\subsubsubsubsection{\@startsection{subparagraph}{5}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}}
\newcommand\subsubsubsubsubsection{\@startsection{subparagraph}{6}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize}}
\newcommand{\myhy}[2]{\hyperref[#1]{\color{black}\setulcolor{black}\ul{#2}}}
\makeatother


\begin{document}
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
            
        \Huge
        \textbf{Playing draughts using reinforcement learning}
            
        \vspace{0.5cm}
        \LARGE
        An investigation into playing draughts using a reinforcement learning via a neural network library, with an implementation of
        automatic differentiation, built from scratch.
            
        \vspace{1.5cm}
            
        \textbf{Edgar Maddocks}
        \\
        \textbf{1.3.3.3, 1.4.3+4}
            
        \vfill
            
        \vspace{0.8cm}
                        
        \Large
        Bedford School\\
        03/05/2024\\
            
    \end{center}
\end{titlepage}

    \pagebreak

    \tableofcontents

    \section{Analysis}
    \subsection{Background}
    This project is an investigation into the use of reinforcement learning to play games (in this case draughts/checkers).
    The model will use self-play and Monte Carlo tree search algorithms, coupled with a multi-headed neural network to
    understand the game and estimate optimal actions.

    The neural network will be built using a library, that has an autograd engine implemented, all built from scratch. This
    investigation will however use some scientific computing libraries for faster simple matrix operations (such as numpy). Some more
    complex functions (such as cross-correlation and its derivative) will also be built from scratch.

    \subsection{Evidence of Analysis}

    \subsubsection{Current systems}

    \subsubsubsection{Draughts}
    Draughts is an English board game played on an 8x8 checkered board, identical to a chessboard.
    Each player begins a game with 12 pieces, usually flat round discs.
    The pieces and board are usually black and white, and will be referred to as such.
    The board is first placed between the two players such that the bottom right-hand corner is a white square,
    for both players.
    
    A coin is tossed to decide who plays black, and that player has the first move. Each player places their pieces
    on the 12 black squares closest to themselves. The setup of the board can be seen in Figure 1. 

    \begin{wrapfigure}[15]{l}{0.3\linewidth}
        \centering
        \includegraphics[scale=0.35]{The-starting-position-for-checkers.png}
        \caption{An image showing the starting position of a game of draughts}
    \end{wrapfigure}
    The pieces only move diagonally (so will always be on black squares)
    and the aim is to take all of the opposing players pieces, or to put the opposing player in a position with no possible moves.
    Players take turns moving their shade of pieces. If at any point of the game, a player's piece reaches the opposing players edge
    of the board, the piece becomes a 'King', and another piece should be placed on top of said piece to indicate so.
    Unless a piece is crowned and a 'King' it may only move and take pieces diagonally forwards. Kings may move and take both forwards and backwards.
    
    \begin{wrapfigure}[12]{r}{0.5\linewidth}
        \centering
        \includegraphics[scale=1.15]{piece being taken.png}
        \caption{Example of a piece being taken in draughts}
    \end{wrapfigure}
    If an adjacent square has an opponents piece and the square immediately beyond the oppositions piece is empty, the opponents piece may be captured.
    If the player whose go it is, has the opportunity to capture one or more pieces, then they must do so. 
    A piece is taken by moving your own piece over the opposing player's, into the vacant square, and then removing the opposing piece from the board.
    An example of this process can be seen in Figure 3.

    Unlike a regular move, a capturing move may make more than one 'hop'. This is if the capture places the piece in a position where another capture is possible.
    In this case, the additional capture must be made. The capture sequence can only be made by one piece per move. i.e. You cannot make one capture with one piece, 
    and then another capture with another piece in the same move.
    
    However, if more than one piece can capture, the player has free choice over which piece to move. Likewise, if one piece can capture in multiple
    directions then the player has the choice in which direction to move. 
    
    
    \textbf{Note:} it is not compulsory for the player to move in the direction, or with the piece,
    that will lead to the greatest number of captures in that move.
    
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.6]{double hop.png}
        \caption{Visualization of multiple captures in one move}
    \end{figure}

    A move may only end when the position has no more captures available or an uncrowned piece reaches the opposing edge
    of the board and becomes a King. 

    The game ends when all of a players piece's have been captured, or a player has no available moves.

    \pagebreak
    \subsubsubsection{Neural Networks}
    \subsubsubsubsection{What is a neural Network}
    A neural network is a machine learning model which aims to mimic the processes of the human brain.
    Each network contains inputs and outputs, as well as one or more layers of hidden nodes - which act as artificial neurons.

    Neural networks are a supervised learning model, meaning that they learn from labeled data (which has the objective correct answer in the data).
    They are sometimes referred to as artificial neural networks (ANNs) or simulated neural networks (SNNs).\\

    In a fully connected network, each node is connected once to each node in the next layer - an example of how one node connects
    to the next layer can be seen below.
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.2]{ConnectedNode.png}
        \caption{Example of a fully connected node and layer}
    \end{figure}

    Neural networks can be modelled as a collection linear regression units.

    \subsubsubsubsection{Linear Regression Unit}
    A single linear regression unit output has the formula:
    \begin{align}
        \hat{y} = \sum_{i=0}^{n} w_ix_i + b
    \end{align}

    Where $\hat{y}$ is the predicted output, $n$ is the number of inputs, $x_i$ is the $i$th input, $w_i$ is the weight of $x_i$, and $b$ is a bias.
    If, for example, there were 3 inputs the full equation for $\hat{y}$ would be:
    \begin{align}
        \hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + b
    \end{align}

    \subsubsubsubsection{Vectorization of processes}
    This calculation can be vectorized to improve efficiency and would be notated:
    \begin{align}
        \hat{y} = XW + b
    \end{align}

    Where we let
    \begin{align}
        W = \begin{bmatrix}
            w_1\\
            w_2\\
            \vdots\\
            w_n
        \end{bmatrix}\\
        \hspace{25px}
        X = \begin{bmatrix}
            x_1&
            x_2&
            \hdots&
            x_n
        \end{bmatrix}
    \end{align}

    $X$ here is a row vector as this is the most common format for data as an input to a network (e.g. being read from a csv).
    This parallelized computation is much faster, and can be parallelized using the GPU to further improve speed and efficiency.

    \subsubsubsubsection{Forward Pass of Dense Layer}
    In the case of neural networks, lots of these linear regression units can be combined to form a vector of outputs.
    Each of these regression units will have the same inputs, therefore $X$ can have the same definition. However, $W$ will now be
    composed of multiple vectors of weights, instead of just one.

    Here we let
    \begin{align}
        W = \begin{bmatrix}
            w_{11} & w_{21} & \dots & w_{j1}\\
            w_{12} & w_{22} & \dots & w_{j2}\\
            \vdots & \vdots & \ddots & \vdots\\
            w_{1n} & w_{2n} & \dots & w_{jn}
        \end{bmatrix}
    \end{align}

    Where $j$ is now the number of nodes in the layer. If we rewrite our forward pass equation to use this weight matrix,
    with each node as its own regression unit:
    \begin{align}
        Y = XW + B
    \end{align}

    Where:
    \begin{align}
        Y = \begin{bmatrix}
            \hat{y_1}&
            \hat{y_2}&
            \hdots&
            \hat{y_n}
        \end{bmatrix} \hspace{15mm}
        B = \begin{bmatrix}
           b_1 & b_2 & \dots & b_j
        \end{bmatrix}
    \end{align}

    With $W$ and $X$ having the same definition as most recently defined.

    \subsubsubsubsection{Forward Pass of a Convolutional Layer}
    The main function of a convolutional layer is to extract key features and they are often used in image recognition and classification solutions.

    \begin{wrapfigure}[15]{r}{0.5\linewidth}
        \centering
        \includegraphics[scale=0.2]{ConvInputIndependentCode.png}
        \caption{Visualization of a Convolutional layer}
    \end{wrapfigure}

    A convolutional layer takes in a 3 dimensional block of data. This can be imagined as the first 2 dimensions being a grid of pixels, and the third dimension
    being the colour channels for each grid. The trainable parameters in a convolutional network are the kernels/filters and the biases. The kernels can be of any size
    but must have the same depth as the input data. Each kernel also has a bias matrix, which has the same shape as the output.

    After the forward pass, the output will also be 3 dimensional, but here the depth is equal to the number of kernels, and the size is equal to

    \begin{align}
        I - K + 1
    \end{align}

    Where $I$ is the size of the input grid and $K$ is the size of the kernel grids.

    The forward pass can then be simplified, with each output equating to the bias for that kernel plus the sum of the sum of cross correlations between a single channel
    of the input and its respective channel in the kernel. This can be seen in the figure below, where $d$ is equal to the number of kernels, and $n$ is the number of channels in the input data.


    \pagebreak
    \begin{figure}
        \centering
        \includegraphics[scale=0.3]{ConvForwardPropIndependentCode.png}
        \caption{Representation of the forward pass of a convolutional layer}
    \end{figure}

    This can be written more mathematically using a summation.

    \begin{align}
        Y_i = B_i + \sum_{j=1}^{n}X_j \star K_{ij}, \hspace{1cm} i = 1 \dots d
    \end{align}

    \subsubsubsubsection{Gradient Descent}

    Now, to update the weights of our model, we can compute these values in closed form, however, it comes with a large time complexity (greater than $O(n^3)$), and therefore
    a process called gradient descent is usually employed.

    Gradient descent works to minimize the error of a model by iteratively locating a minimum in the error function.
    For example, a common cost (error) function is mean squared error.
    \begin{align}
        MSE(Y, \hat{Y}) = \frac{1}{n}\sum_{i=0}^{n}\left(y_i - \hat{y}_i\right)^2
    \end{align}

    This function effectively calculates the absolute distance between the predicted value of one data point ($\hat{y_i}$) and the true value at that same point ($y_i$),
    for every value in the dataset (of size $n$) and takes the mean of these distances.
    Again, this calculation can be vectorized using the equation below.
    \begin{align}
        MSE(Y, \hat{Y}) = \frac{1}{n}((Y - \hat{Y})^T \cdot (Y - \hat{Y}))
    \end{align}

    If we plot the MSE curve of different weights and biases - of a single regression unit, it looks as follows.
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{MSEplot.png}
        \caption{Plot of MSE in a single regression unit}
    \end{figure}

    Here the weight has been labelled $a$, but the bias has remained as $b$. As we can see, there is a clear local minimum of this error function,
    and the values that obtain this minimum, is what gradient descent aims to achieve. 
    
    \begin{wrapfigure}[14]{l}{0.4\linewidth}
        \centering
        \includegraphics[scale=0.2]{image.png}
        \caption{MSE with respect to one parameter}
    \end{wrapfigure}
    In figure 6 we can see a representation of the MSE plot with respect to only one of the parameters. This makes understanding the process of gradient
    descent much simpler. For example, the minimum of this function ($f(x) = \frac{x^2}{25}$) is at $x = 0$. So, if we were to have an initial $x$ value of say $-40$,
    and we calculated the gradient to be $3.2$, we could then subtract this from $40$. Resulting in a new value of $36.8$, which is closer to our optimal value of $0$.
    This process is completed iteratively until the minimum, or near to it, is reached.

    To denote this mathematically, we can say that 
    \begin{align}
        w_{t+1} = w_t - \alpha \frac{\partial E}{\partial w}\\
        b_{t+1} = b_t - \alpha \frac{\partial E}{\partial b}
    \end{align}

    Where $w_{t+1}$ and $b_{t+1}$ are the parameters at the next timestep, $w_t$ and $b_t$ is the value of the parameters at the current timestep, 
    $\frac{\partial E}{\partial w}$ and $\frac{\partial E}{\partial b}$ are the derivatives of the error with respect to each parameter, 
    and $\alpha$ is the learning rate. This learning rate controls the size of our 'jumps' and prevents the process from beginning to spiral away
    from the optimal values.

    That is a basic overview of gradient descent in a single regression unit, but in a dense network, the process is almost identical. An error function is evaluated, 
    the derivative with respect to the inputs calculated, and the new values updated. The main expense computation-wise is calculating all the derivatives, with respect to
    each set of parameters, as this requires passing the output from the error function backwards through all the layers and processes. To combat this, automatic differentiation
    can be used, which tracks computation dynamically at run time to compute derivatives.

    \subsubsubsection{Automatic Differentiation}
    As mentioned, automatic differentiation tracks computation dynamically and then computes derivatives using a computational graph. This allows models to
    have much more complex forward passes, including decision branches and loops where the length of the loop is decided at runtime.

    Automatic differentiation operates by differentiating a complex function (that we don't know the derivative of), by treating it as a composition of more elementary functions
    (of which we do know the derivatives). Additionally, it treats each of these elementary functions as though their output is an intermediate variable when computing the
    complex function. This becomes very useful when there are multiple inputs to the function, and we only want the derivative to one of these variables. Finally, this process
    makes use of the chain rule to compute the derivative with respect to the inputs, as will be shown later on.

    Firstly, I will walk through an example of the process of automatic differentiation.\\
    \noindent Say we have a function: \begin{align}
        f(x) = ln\left(\frac{sin(x_1)}{cos(x_2)}\right)
    \end{align}

    This function has two arbitrary inputs (perhaps in the case of a neural network these could be our parameters or outputs for example). At first, this is quite a complex
    function to differentiate but, we can simplify it using intermediate variables. For example \begin{align}
        v_1 &= sin(x_1)\\
        v_2 &= cos(x_2)\\
        v_3 &= \frac{v_1}{v_2}\\
        v_4 &= ln(v_3)\\
        y &= v_4
    \end{align}

    \begin{wrapfigure}[23]{l}{0.5\linewidth}
        \centering
        \includegraphics[scale = 0.34]{ExampleCompGraph.png}
        \caption{Example Computational Graph}
    \end{wrapfigure}
    Now that we have declared our intermediate variables, we can see how automatic differentiation splits up the original function into elementary ones.
    This then allows use to easily take derivatives. Firstly, we can draw out the computational graph for this function, using our intermediate variables,
    and this can be seen in Figure 7.
    This graph visualizes the forward pass of the function, and how the intermediate variables compose to form our original function.

    Now, when we want to take the derivative with respect to one of these inputs, we can simply reverse through the graph, taking the gradient of each intermediate
    variable, and using the chain rule to give us our final derivative.

    This process would looks as follows: \begin{align}
        \frac{\partial y}{\partial x_1} = \frac{\partial y}{\partial v_4} \cdot \frac{\partial v_4}{\partial v_3} \cdot \frac{\partial v_3}{\partial v_1} \cdot \frac{\partial v_1}{\partial x_1}
    \end{align}

    If we computed each of these derivatives:
    \begin{align}
        \frac{\partial y}{\partial v_4} &= 1\\
        \frac{\partial v_4}{\partial v_3} &= \frac{1}{v_3}\\
        \frac{\partial v_3}{\partial v_1} &= \frac{1}{v_2}\\
        \frac{\partial v_1}{\partial x_1} &= cos(x_1)
    \end{align}

    And finally, evaluating equation $(8)$, as well as substituting our intermediate variables in terms of $x_1$, to obtain $\frac{\partial y}{\partial x_1}$:
    \begin{align}
        \frac{\partial y}{\partial x_1} &= \frac{cos(x_1)}{v_3 \cdot v_2}\\
        &= \frac{cos(x_1)}{v_1}\\
        &= \frac{cos(x_1)}{sin(x_1)}
    \end{align}

    This process of traversing through the graph backwards is known as reverse accumulation auto differentiation, and in practice each of the gradients would be
    stored numerically at computation, rather than computing a symbolic version of the final derivative and plugging the values in.

    \subsubsubsection{Reinforcement Learning (RL)}
    Reinforcement learning aims to capitalize on learning through an environment (similar to how infants learn by playing), without a teacher, to improve computer's
    ability at a given task. Simply put, problems in reinforcement learning involve learning how to map certain situations to certain actions to maximize a reward signal.
    The models are not told what actions to take and instead must explore the environment and available actions to learn what yields the most rewards. In some cases, actions
    may not only affect immediate rewards, but also the possibility and/or size of future rewards.

    \begin{wrapfigure}{l}{0.5\linewidth}
        \centering
        \includegraphics[scale =0.5]{markovprocess.png}
        \caption{Diagram of Markov Decision Process}
    \end{wrapfigure}
    The solution to reinforcement learning problems can be modelled using a markov decision process (A diagram of which can be seen in Figure _). This diagram shows the process of
    one iteration of training. The agent will recieve a state ($S_t$) and then decide on an action to take ($A_t$). This action is sent to the environment, which process the action,
    evaluates the reward gained from that action ($R_t$), and then returns that reward and the next state ($S_{t+1}$).
    
    One challenge that becomes in apparent in reinforcement learning, that is not present in other machine learning methods (supervised and unsupervised), is that of \textit{exploration}
    vs \textit{exploitation}. This problem comes from the fact that to maximize any given reward signal, an agent must take the action which gains the most reward i.e. it must
    \textit{exploit} what it knows, and choose the best action. However, to have knowledge of which action is best at any given state, the agent must have \textit{explored} many different
    actions - in many different states. This challenge will be addressed in more depth later on, during the design of the agent.

    There are two main approaches to reinforcement learning problems, a model-free solution or model solution. Within model-free solutions there is another two categories of policy optimization
    and Q-learning. In policy optimization, the agent learns a policy which maps states to actions. There are two types of these policies - deterministic and stochastic. A deterministic policy
    maps without uncertainty i.e. the agent will take the same action given the same state. Stochastic policies on the other hand output a distribution which maps a state to the probability of 
    each action.

    \begin{wrapfigure}[17]{l}{0.4\linewidth}
        \centering
        \includegraphics[scale=0.4]{qlearning.PNG}
        \caption{Representation of Q-learning process}
    \end{wrapfigure}
    Q-learning can be represented as a tabular learning method, and works by learning the value of a function - usually $Q(s, a)$ - which represents how successful an action was at
    a certain state. Each of these values is stored in the 'table' and when the model next observes a given state it consolidates the table and chooses the action which was most
    successful in its past experience.

    For model-based solutions, the models 'know' the rules of the games and learn from planning and construct a functional representation of the environment. This is different to non-model
    based which aims to learn by trial-and-error and experience. AlphaZero for example would be defined as a model-based agent. To better define this, if the agent is able to forecast the reward
    of an action given any state, allowing it to plan what actions to take, it is model-based.

    Model-based solutions are best when trying to optimize the reward for a task - such as playing chess. Whereas model-free solutions are best for tasks such as self-driving cars (as a model-based
    approach may run over a pedestrian just to try and complete the journey in the shortest time).

    \subsubsubsection{AlphaZero}
    As mentioned before, AlphaZero is model-based - but how exactly it works is unique and does not have a naming convention like Q-learning.

    AlphaZero uses a process called Monte Carlo Tree Search (MCTS) to build a tree of future variations of a game. This tree is built depth first - so only begins building a new branch, when the state in the
    branch currently being explored is terminal. When building this tree, MCTS must select a child node - if all possible new actions have been exhausted. To do this a formula called the upper confidence 
    bound (UCB) is used. The UCB formula can be seen below.
    \begin{align}
        UCB = \frac{w_i}{n_i} + C\sqrt{\frac{ln(N_i)}{n_i}}
    \end{align}
    
    In this equation, $w_i$ represents the number of wins a node has, $n_i$ represents the number of visits a node has, $N-i$ represents the number of visits the parent node has, and $C$ is a coefficient which
    controls the levels of exploration in the MCTS model.
    Also, after each branch has reached a terminal state the MCTS backpropogates and records how many times each node has been visited, and how many times that node has lead to a winning terminal state. 
    Using this recorded data, the MCTS model can build a distribution which models the probability of winning the game for each action in the current state. This allows the model to 'learn' the rules
    of the game, as illegal actions will have a probability of 0. This distribution can also be called the MCTS' policy.

    To improve this process, AlphaZero uses a neural network to help prune some branches of the tree which can be assumed to lead to a losing state. This allows the MCTS to search only more effective
    lines of play. The exact algorithm for this can be found in \myhy{sec:IDoA}{Identification of Algorithms}
    


    
    \subsubsection{Similar Systems}
    \begin{wrapfigure}[14]{r}{0.5\linewidth}
        \centering
        \includegraphics[scale=0.145]{1992Tinsleymatch.jpg}
        \caption{Image of Chinook playing a game of draughts - 1992}
    \end{wrapfigure}
    Chinook is a checkers playing computer program that was developed at the University of Alberta between
    1989 and 2007. The program utilises an opening book, deep-search algorithms alongside a position evaluation function
    and a solved endgame database for positions with 8 pieces or fewer.
    All of Chinook's knowledge has been programmed in by humans, with no use of an artificial intelligence model. Despite this,
    Chinook managed to beat the draughts world champion at the time of 1995, and after this no longer competed, but instead was
    tasked with solving checkers. (A solved game is one where the outcome can be predicted from any position) - and this was achieved in 2007.

    \begin{wrapfigure}[16]{l}{0.5\linewidth}
        \centering
        \includegraphics[scale=0.7]{download.jpg}
        \caption{Chess champion Garry Kasparov loses to Chess engine - 1997}
    \end{wrapfigure}
    Other similar systems include chess engines - of which there is a multitude. Chess engines operate similarly to Chinook, creating trees
    of possible moves and using a minimax algorithm, as well as alpha-beta pruning to determine the best move. Additionally, there are some
    chess engines which use reinforcement learning, such as Google DeepMind's AlphaZero engine. This engine has also been trained on games
    such as Go and Shogi. AlphaZero is trained mainly using self-play, where the AI plays against itself and then learns from those games.
    In a 100 game match against StockFish (one of the most popular chess engines) AlphaZero won 28 games, lost 0 and drew the remaining 72.
    This shows the power of reinforcement learning as a tool for board games and its possibility to be used for draughts.

    For automatic differentiation, some of the most popular systems include PyTorch's autograd, as well as autodiff by JAX. PyTorch has a very
    well developed library for deep-learning alongside it's autograd, as well as extensive documentation. Additionally, autograd is more similar
    to regular python than JAX and may be easier to use when first delving into the area. However, JAX builds on top of autograd and accelerates
    the linear algebra using an XLA backend. This backend optimizes operations for CPU's GPU's and TPU's as well as using a just-in-time compiler
    to decrease runtimes.

    \subsubsection{Interview with third party}
    For my interview, I reached out to an external person who works in the AI industry. We discussed where my time would be most well spent,
    some industry standards I should try and implement as well as some expansionary objectives I could try to include to improve my project.

    To start with, with the large amount of work that needs to be done, if time begins to run short there are lots of out-of-the-box frameworks
    to use and implement (such as PyTorch and JAX - mentioned earlier), while still allowing me to finish my project with sufficient difficulty.
    Secondly, the large proportion of my time should be spent researching and understanding the different solutions available which would best
    suit my problem, and how they are implemented. This research should include, but not be restricted to: reinforcement learning solutions for
    other games (such as Go and Chess), different attributes of reinforcement learning models and which environments they are most capable in, as well
    as research on the overheads of similar models.

    It is very important that the correct solution is chosen, so minimal time is spent on implementation.

    As for possible objectives:
    \begin{center}
        \begin{tabular}{|m{15em}|m{15em}|}
            \hline 
            Objective & Reasoning \\
            \hline
            Leverage GPU cores for linear algebra computations & Allows parallelization and will increase computation speeds  \\      
            \hline
            Wrap the console checkers board in a GUI & Much easier for users to use and understand \\
            \hline
            Use a container such as Docker to produce logs & Much better retracing and debugging capabilities \\
            \hline
            Evaluate the model against Chinook and human players & Allows for rating of model \\
            \hline
        \end{tabular}
    \end{center}

    
    \subsection{Modelling of the problem}

    \subsubsection{Identification of Objects}

    \subsubsubsection{Draughts}
    For draughts, we will need two classes, a checkers board/environment and a checkers game. The environment will be used for training the agent and It will require methods that
    execute an action in the environment, as well as functions to compute the available moves and check for a win. An example of UML for this environment can be seen below. 

    \begin{figure}[h]
        \centering
        \includegraphics[scale = 0.24]{DraughtsUMLdrawio.png}
        \caption{UML for draughts/checkers game}
    \end{figure}

    The checkers game varies only slightly and will inherit from the checkers board. It will however add some functionality such as a GUI and the ability to choose different
    models to play against (such as just the MCTS or the full agent). Additionally, it should allow two people to play each other as checkers usually would.

    \subsubsubsection{Automatic Differentiation Engine}
    \begin{wrapfigure}{l}{0.5\linewidth}
        \centering
        \includegraphics[scale=0.02]{AutoDiffUML.drawio.png}
        \caption{Example UML for Auto Diff}
    \end{wrapfigure}
    The automatic differentiation engine will be based off of the 'Tensor' object. This object will be similar to a matrix but have some additonal
    attributes, such as its gradient and if its gradient requires computing. If the tensor does require a gradient then it will also store its parent
    tensors, which will be used for the backward DFS of the computational graph.

    Additionally, all operators for the tensor object will have to be overridden to allow for gradient computation if the tensor requires it.
    The tensor class will also have functions to zero out the gradient as well as begin the backwards traversal through the computational graph. Below is an example
    of some possible UML for the automatic differentiation library.

    Tensor operations will all have a backward and forward method - inherited from the function class. These functions will represent elementary functions (Addition, Division, Log, etc.). 
    The operations such as sum and mean, that operate on the tensor itself will also be represented by a function object.

    \subsubsubsection{Neural Network Library}
    Their will be a large collection of objects inside of the neural network library, all based around the functionality provided by the tensor object.
    Firstly, the parameter class will allow for tensors of a given shape to be created and act as parameters for a network - this will also help with backpropogation.
    The module object will have a parameters property which yields every parameter related to that module. This module class is what all layers will inherit from. It will also
    have methods to zero out the gradients of all parameters.

    Secondly, their will be classes for each different type of layer (dense and activations). The dense layer will have attributes which store the parameters required
    for computation as well as a method to complete the forward pass through the layer. Activation functions will compute the forward pass, as well as store the gradient function
    for any tensors which require it. The activation layers will not need any attributes as the have no parameters.

    The optimizer class is the base class for any optimizers such as stochastic gradient descent or momentum-based gradient descent. This class will have attributes which store
    the learning rate as well as any parameters the optimizer is related to. It will also have virtual methods - which will be overriden by the specific optimizer classes - of
    a step (which updates all parameters), a function to zero the gradient of all parameters and finally a function to add parameters.

    Below is an inheritance diagram for the Neural Network Library, without any attributes and methods.

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{NNUML (simple).drawio.png}
        \caption{Inheritance diagram for Neural Network Library}
    \end{figure}


    \pagebreak
    \subsubsubsection{Reinforcement Learning Solution}
    The final solution will also require a large set of classes including: a network class - to output the model policy and predicted value; a monte carlo tree searach (MCTS) class to compute
    a search of available actions to train the network towards - composed of a node class to make up this tree - and finally an agent class which abstracts the processes of the previous classes. \\

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{RLSolutionUML.png}
        \caption{Reinforcement Learning Solution UML}
    \end{figure}
    


    \pagebreak
    \subsubsection{Identification of algorithms}
    \label{sec:IDoA}
    \subsubsubsection{Automatic Differentiation Engine}
    The auto-diff engine will use a recursive DFS of the computational graph to calculate gradients. It uses a DFS as any of the computational graphs that built are directed acyclical graphs,
    and therefore DFS provides a way to search the whole graph in O(n) time. Some pseudocode for this search can be seen below.

    \begin{algorithm} 
        \caption{Backward Function of Tensor}\label{alg:cap}
        \begin{algorithmic}
        \Function{Backward}{Tensor self, Array grad, Tensor out}
        \Ensure t.requiresGrad is True \Comment{t is the tensor being called to start DFS}
        \If{t.Grad is None}
            \State grad $\gets \begin{bmatrix}
                1 & 1 & 1 & \hdots
            \end{bmatrix}$ in shape of t.data
        \EndIf
        
        \State t.Grad += grad

        \If{t.Operation is not None}
            \State t.Operation.backward(t.Grad, t)
        \EndIf

        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
        \caption{General Backward Function of a Tensor Operation}\label{alg:cap}
        \begin{algorithmic}
        \Function{backward}{Operation self, Array grad, Tensor t}
        \State savedTensors $\gets$ self.cache.0
        
        \For{tensor in savedTensors}
        \If{tensor.requiresGrad is True}
        \State newGrad $\gets \frac{\partial output}{\partial tensor}$ \Comment{Computed by chain rule, specific for each operation}
        \If{broadcastedDims}
        \State Remove broadcasted dims from newGrad \Comment{newGrad.shape should equal tensor.shape}
        \EndIf
        \State tensor.backward(newGrad, t)
        \EndIf
        \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \subsubsubsection{Neural Network Library}
    The two main algorithms in the neural network library are the forward pass and the backward pass through a collection of layers. Pseudocode for these can also been seen below.
    \begin{algorithm}
        \caption{Forward pass through collection of layers}\label{alg:cap}
        \begin{algorithmic}
        \Function{forward}{Module model, Tensor input}
        \State output $\gets$ input
        
        \For{layer in model.layers}
        \State output $\gets$ layer.forward(output) \Comment{layer's forward function depends on layer type (Dense/Conv)}
        \EndFor
        return output
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
        \caption{Training process of a module}\label{alg:cap}
        \begin{algorithmic}
        \Function{fit}{Module model, Optimizer optim, Loss l, Tensor x, Tensor y, int epochs = 100, int verbose = 1}
        \State startTime $\gets$ time.now()
        
        \For{epoch in epochs}
        \State prediction $\gets$ model.predict(x) \Comment{Get predictions}
        \State loss $\gets$ l(y, prediction) \Comment{Calculate how far predictions are from true}
        \State optimizer.zero_grads() \Comment{Reset gradients of parameters}
        \State loss.backwards() \Comment{Calculate gradients of loss}
        \State optimizer.step() \Comment{Update parameter values using calculated gradients}

        \If{verbose is 1}
        \State print(epoch, loss)
        \EndIf

        \EndFor
        print(time.now() - startTime) \Comment{Total time elapsed}
        \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    \pagebreak
    \subsubsubsection{Reinforcement Learning Solution}
    For the reinforcement learning, one of the most important algorithms is the MCTS which is similar to a DFS. This takes all
    the available moves in the checkers environment, selects one at random and continues this process until a terminal state is reached.
    After the terminal state is reached, reward values (+1 for win, -1 for loss, 0 for draw) are backrpopagated through the nodes.
    If all possible available moves have been exhausted at a given node then it selects one using an upper confidence bound (UCB) formula.
    The general algorithm can be seen in pseudocode below (as well as the nodes expand function).

    \begin{algorithm}
        \caption{MCTS Tree Building Function}\label{alg:cap}
        \begin{algorithmic}
        \Function{buildTree}{currentBoardState}
        
        \State self.root $\gets$ currentBoardState

        \For{seach in numberOfSearches}
        \State node $\gets$ self.root \Comment{Return to start of tree for each search}
        \If{node.brancesAvailableToExpand == 0}
        \State node $\gets$ node.select_child()
        \EndIf
        
        \While{node.terminal is False AND node.branchesAvailableToExpand $>$ 0}
        \State node $\gets$ node.expand() \Comment{Depth-first expansion until terminal state}
        \EndWhile

        \State node.backpropogate(node.reward)
        \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    The second algorithm is the process of the final agent model selecting its actions and can be seen on the next page.

    \begin{algorithm}
        \caption{Agent Policy Selection}\label{alg:cap}
        \begin{algorithmic}
        \Function{trainAgent}{self, initialNeuralNetwork, hyperParameters}
        
        \State nnPrevious $\gets$ initialNeuralNetwork
        \For{mctsEpoch in hyperParameters.mctsEpochs}
            \State trainingExamplesList $\gets$ SPV(hyperParameters.maxExamples)
            \For{episode in hyperParameters.episodes}
                \State examples $\gets$ list()
                \State game $\gets$ new CheckersGame
                \While{!game.terminated}
                    \State mcts $\gets$ MCTS(nnPrevious, game.currentBoardState, hyperParameters).buildTree()
                    \If{game.step $<$ hyperParameters.Threshold}
                        \State action $\sim$ mctsPolicy.getPolicy() \Comment{Select random move}
                    \Else
                        \State action $\gets$ mcts.getAction()
                    \EndIf
                    \State examples.append((game.currentBoardState, mcts.getPolicy(), initialNeuralNetwork.value))
                \EndWhile
                \State trainingExamplesList.append(examples)
                \For{networkEpoch in hyperParameters.networkEpochs}
                    \State batch $\gets$ trainingExamplesList.getBatch(hyperParameters.batchSize)
                    \State nnNew $\gets$ nnPrevious.train(batch, hyperParameters)
                \EndFor
                \State numberNNNewWins = 0
                \For{compareGame in hyperParameters.compareGames}
                    \State game $\gets$ new CheckersGame
                    \While{!game.terminated}
                        \If{game.player == player1}
                            \State action $\gets$ MCTS(nnPrev, game.currentBoardState, hyperParameters).getAction()
                        \ElsIf{game.player == player2}
                            \State action $\gets$ MCTS(nnNew, game.currentBoardState, hyperParameters)
                        \EndIf
                    \EndWhile
                \EndFor
                \State nnNewWinPct = numberNNNewWins/hyperParameters.compareGames
                \If{nnNewWinPct $>$ hyperParameters.replaceThresholdPct}
                    \State nnPrev $\gets$ nnNew
                \EndIf
            \EndFor
        \EndFor
        \State \Return nnPrev
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    
    \pagebreak
    \subsubsection{Mathematical Formula}
    \subsubsubsection{Automatic Differentiation Engine}
    In this section I will simply define the formulas for the forward and backward pass of many different functions that will be implemented
    into the autograd library.
    \subsubsubsubsection{Addition}
    \subsubsubsubsubsection{Forward}
    \begin{align}a + b = y\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = 1 + 0\end{align}\\
    \begin{align}\frac{\partial y}{\partial b} = 0 + 1\end{align}

    \subsubsubsubsection{Negation}
    \subsubsubsubsubsection{Forward}
    \begin{align}y = -a\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = -1\end{align}

    \subsubsubsubsection{Multiplication}
    \subsubsubsubsubsection{Forward}
    \begin{align}a \cdot b = y\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = b\end{align}
    \begin{align}\frac{\partial y}{\partial b} = a\end{align}

    \subsubsubsubsection{Division}
    \subsubsubsubsubsection{Forward}
    \begin{align}\frac{a}{b} = y\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = \frac{1}{b}\end{align}
    \begin{align}\frac{\partial y}{\partial b} = -\frac{a}{b^2}\end{align}

    \subsubsubsubsection{Matrix multiplication}
    \subsubsubsubsubsection{Forward}
    \begin{align}
        AB = Y &&\text{where A and B are matrices}
    \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial Y}{\partial A} = B^T\end{align}
    \begin{align}\frac{\partial Y}{\partial B} = A^T\end{align}

    \subsubsubsubsection{Powers}
    \subsubsubsubsubsection{Forward}
    \begin{align}a^b = y\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = ba^{b-1}\end{align}
    \begin{align}\frac{\partial y}{\partial b} = a^bln(a)\end{align}

    \subsubsubsubsection{Mean}
    \subsubsubsubsubsection{Forward}
    \begin{align}\frac{\sum A}{n} = y, && A = (x_0, x_1, x_2, \hdots x_n) \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align} \frac{\partial y}{\partial A} = B, && B = ({\frac{1}{n}}_0, {\frac{1}{n}}_1, {\frac{1}{n}}_2 \hdots, {\frac{1}{n}}_n) \end{align}
    
    \subsubsubsubsection{Sum}
    \subsubsubsubsubsection{Forward}
    \begin{align} \sum A = y, && A = (x_0, x_1, x_2, \hdots x_n) \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align} \frac{\partial y}{\partial A} = B, && B = ({1}_0, {1}_1, {1}_2 \hdots, {1}_n) \end{align}

    \subsubsubsubsection{Natural Log}
    \subsubsubsubsubsection{Forward}
    \begin{align} ln(a) = y \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align} \frac{\partial y}{\partial a} = \frac{1}{a} \end{align}

    \subsubsubsubsection{Exponentation}
    \subsubsubsubsubsection{Forward}
    \begin{align} e^a = y \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align} \frac{\partial y}{\partial a} = e^a \end{align}

    \subsubsubsubsection{Conv2D}
    \subsubsubsubsubsection{Forward}
    \begin{align} Y_i = B_i + \sum_{j=1}^{n}X_j \star K_{ij}, \hspace{1cm} i = 1 \dots d\end{align}
    Similar to the current systems, the star denotes the valid cross-correlation of the two matrices.\\
    \textbf{Example of cross-correlation}\\
    Cross correlation can be imagined as placed the kernel ontop of the input and summing the products of the elemts which overlap, and then moving the kernel and repeating the process.
    \begin{align}
        \begin{bmatrix}
            1 & 6 & 2 \\
            5 & 3 & 1 \\
            7 & 0 & 4 \\
        \end{bmatrix} \star \begin{bmatrix}
            1 & 2 \\
            -1 & 0 \\
        \end{bmatrix} &= \begin{bmatrix}
            1 \cdot 1 + 2 \cdot 6 + -1 \cdot 5 + 0 \cdot 3 & 1 \cdot 6 + 2 \cdot 2 + -1 \cdot 3 + 0 \cdot 1 \\
            1 \cdot 5 + 2 \cdot 3 + -1 \cdot 7 + 0 \cdot 0 & 1 \cdot 3 + 2 \cdot 1 + -1 \cdot 0 + 0 \cdot 4
        \end{bmatrix}\\
        &= \begin{bmatrix}
            8 & 7 \\
            4 & 5
        \end{bmatrix}
    \end{align}

    This method of cross-correlation is known as valid cross correlation (where the upper left element of the kernel 
    begins on the upper left element of the input).
    \subsubsubsubsubsection{Backward}
    
    In this backward function, $\frac{\partial Z}{\partial Y}$ will denote the gradient being recieved from upstream.
    \begin{align}
        \frac{\partial Z}{\partial B_i} &= \frac{\partial Z}{\partial Y_i}\\
        \frac{\partial Z}{\partial K_{ij}} &= X_j \star \frac{\partial Z}{\partial Y_i}\\
        \frac{\partial Z}{\partial X_i} &= \sum_{i=1}^{d}\frac{\partial Z}{\partial Y_i} * K_{ij}
    \end{align}

    The asterisk in the final expression represents the full convolution operation, which is the same as a full cross-correlation, but
    with the kernel rotated 180 degrees. Additionally, a full correlation begins the operation with the bottom left elemnt of the kernel
    being placed upon the top left element of the input - and then the sliding and summing of products process remains the same.
    
    \subsubsubsection{Neural Network Library}
    \subsubsubsubsection{Activation Layers}
    In both dense and convolutional layers, the only operation that is occuring is multiplication and/or matrix multiplication. Graphically,
    these operations only create linear transformations however in machine learning some problems require non-linearity. To allow for non-linearity
    neural networks use activation layers which are non-linear functions, normally places between layers.
    
    \subsubsubsubsubsection{Tanh}
    \begin{align}
        tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    \end{align}

    \subsubsubsubsubsection{Sigmoid}
    \begin{align}
        S(x) = \frac{1}{1 + e^{-x}}
    \end{align}

    \subsubsubsubsubsection{Softmax}
    The softmax function is used to turn a vector into a probability distribution. For example, if we had data in the shape (batches, classes) and we wanted to getAction
    the probability of each batch being in a certain class we would use the softmax function across the axis with the number of class, in this case axis 1.
    \begin{align}
        \sigma(x) = \frac{e^x}{\sum e^x} \hspace{10mm} \text{Sum across axis with number of classifications in x}
    \end{align}

    \subsubsubsection{Reinforcement Learning Solution}

    \subsection{Set of objectives}
    \subsubsection{Draughts Game}
    \begin{enumerate}
        \item An easy-to-use and understand interface for the draughts game
        \begin{enumerate}
            \item A small text tutorial on the rules of draughts
            \item A visual representation of the draughts board
            \item A list displaying all legal available moves in the given position
            \item Informative error messages if an incorrect move is attempted
            \item Error handling to prompt the user to enter another move if an invalid move is attempted
        \end{enumerate}
        \item Implementation of various decision-making agents for users to play against
        \begin{enumerate}
            \item Option for the user to choose which agent to play against (MCTS, Full Agent)
            \item Ability to adjust hyperparameters of the agents from the draughts game interface
            \item Information printed to the user as to the agent's decision-making progress
        \end{enumerate}
    \end{enumerate}
    \subsubsection{Autodifferentiation engine}
    \begin{enumerate}
        \item A unit-tested autograd engine capable of calculating derivatives on a variety of functions inlcuding:
        \begin{enumerate}
            \item Addition
            \item Negation
            \item Multiplication
            \item Division
            \item Matrix Multiplication
            \item Powers
            \item Mean of a 1D tensor
            \item Natural Log
            \item Exponentiation
            \item (1D Conv) - YET TO BE DECIDED
        \end{enumerate}
    \end{enumerate}
    \subsubsection{Neural Network Library} 

    \subsubsection{Reinforcement Learning Solution}
    
    \section{Research Log}

    \subsection{Draughts Rules}
    \url{https://www.mastersofgames.com/rules/draughts-rules.htm}

    \subsection{Neural Networks}
    \noindent \url{https://www.ibm.com/topics/neural-networks}

    \subsubsection{Vectorization of summations}
    \noindent \url{https://courses.cs.washington.edu/courses/cse446/20wi/Lecture8/08_Regularization.pdf}

    \subsubsection{Convolutional Networks}
    \noindent \url{https://www.youtube.com/watch?v=Lakz2MoHy6o}\\

    \subsection{Automatic Differentiation}
    \noindent \url{https://en.wikipedia.org/wiki/Automatic_differentiation}

    \subsection{Implementing Custom Classes using Numba}
    \noindent \url{https://numba.pydata.org/numba-doc/latest/extending/interval-example.html}
    Numba does not allow user-declared classes and therefore they must be explicity defined using Numba's types.

    \subsection{AlphaZero}
    \noindent \url{https://ai.stackexchange.com/questions/13156/does-alphazero-use-q-learning}
    \noindent \url{https://suragnair.github.io/posts/alphazero.html}
    Explanations of AlphaZero's loss function
    \noindent \url{https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf} Methods and Neural Network architecture section

    \noindent \url{https://liacs.leidenuniv.nl/~plaata1/papers/CoG2019.pdf}
    AlphaZero policy explanation

    \subsection{Existing Auto differentiation systems}
    \noindent \url{https://medium.com/@utsavstha/jax-vs-pytorch-a-comprehensive-comparison-for-deep-learning-10a84f934e17}
    PyTorch vs JAX

    \subsection{Reinforcement Learning}
    \noindent \url{https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf} What is reinforcement learning?\\
    \noindent \url{https://smartlabai.medium.com/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc} Intro to reinforcement learning\\
    \noindent \url{https://medium.com/the-official-integrate-ai-blog/understanding-reinforcement-learning-93d4e34e5698} Different types of reinforcement learning\\
    \noindent \url{https://neptune.ai/blog/model-based-and-model-free-reinforcement-learning-pytennis-case-study} Model-based vs Model-free learning\\
\end{document}