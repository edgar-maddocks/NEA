\documentclass{article}
\usepackage{amsmath}
\usepackage{breqn}
\usepackage{amssymb}
\usepackage{dirtree}
\usepackage{etoc}
\usepackage{minitoc}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\usepackage{geometry}
\geometry{margin=1in}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
}

\usepackage{wrapfig}
\usepackage{graphicx}

\usepackage{array}
\graphicspath{ {./images/} }

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{2}
\setcounter{secttocdepth}{4}


\usepackage{xcolor,soul,lipsum}

\makeatletter
\newcommand\subsubsubsection{\@startsection{paragraph}{4}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}}
\newcommand\subsubsubsubsection{\@startsection{subparagraph}{5}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}}
\newcommand\subsubsubsubsubsection{\@startsection{subparagraph}{6}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize}}
\newcommand{\myhy}[2]{\hyperref[#1]{\color{black}\setulcolor{black}\ul{#2}}}
\makeatother


\usepackage{minted}

\begin{document}
    \dosecttoc

    \begin{titlepage}
        \begin{center}
            \vspace*{1cm}
                
            \Huge
            \textbf{Playing checkers using reinforcement learning}
                
            \vspace{0.5cm}
            \LARGE
            An investigation into playing checkers using a reinforcement learning via a neural network library, with an implementation of
            automatic differentiation, built from scratch.
                
            \vspace{1.5cm}
                
            \textbf{Edgar Maddocks}            
            \vfill
                
            \vspace{0.8cm}
                            
            \Large
            Bedford School\\
            03/05/2024\\
                
        \end{center}
    \end{titlepage}

    \pagebreak
    \tableofcontents

    \section{Analysis}
    \subsection{Background}
    This project is an investigation into the use of reinforcement learning to play games (in this case checkers/checkers).
    The model will use self-play and Monte Carlo tree search algorithms, coupled with a multi-headed neural network to
    understand the game and estimate optimal actions.

    The neural network will be built using a library, that has an autograd engine implemented, all built from scratch. This
    investigation will however use some scientific computing libraries for faster simple matrix operations (such as numpy). Some more
    complex functions (such as cross-correlation and its derivative) will also be built from scratch.

    \pagebreak
    
    \subsection{Evidence of Analysis}

    \subsubsection{Current systems}

    \subsubsubsection{Checkers} \label{checkers}
    Checkers is an English board game played on an 8x8 checkered board, identical to a chessboard.
    Each player begins a game with 12 pieces, usually flat round discs.
    The pieces and board are usually black and white, and will be referred to as such.
    The board is first placed between the two players such that the bottom right-hand corner is a white square,
    for both players.
    
    A coin is tossed to decide who plays black, and that player has the first move. Each player places their pieces
    on the 12 black squares closest to themselves. The setup of the board can be seen in Figure 1. 

    \begin{wrapfigure}[15]{l}{0.3\linewidth}
        \centering
        \includegraphics[scale=0.35]{The-starting-position-for-checkers.png}
        \caption{An image showing the starting position of a game of checkers}
    \end{wrapfigure}
    The pieces only move diagonally (so will always be on black squares)
    and the aim is to take all of the opposing players pieces, or to put the opposing player in a position with no possible moves.
    Players take turns moving their shade of pieces. If at any point of the game, a player's piece reaches the opposing players edge
    of the board, the piece becomes a 'King', and another piece should be placed on top of said piece to indicate so.
    Unless a piece is crowned and a 'King' it may only move and take pieces diagonally forwards. Kings may move and take both forwards and backwards.
    
    \begin{wrapfigure}[12]{r}{0.5\linewidth}
        \centering
        \includegraphics[scale=1.15]{piece being taken.png}
        \caption{Example of a piece being taken in checkers}
    \end{wrapfigure}
    If an adjacent square has an opponents piece and the square immediately beyond the oppositions piece is empty, the opponents piece may be captured.
    If the player whose go it is, has the opportunity to capture one or more pieces, then they must do so. 
    A piece is taken by moving your own piece over the opposing player's, into the vacant square, and then removing the opposing piece from the board.
    An example of this process can be seen in Figure 3.

    Unlike a regular move, a capturing move may make more than one 'hop'. This is if the capture places the piece in a position where another capture is possible.
    In this case, the additional capture must be made. The capture sequence can only be made by one piece per move. i.e. You cannot make one capture with one piece, 
    and then another capture with another piece in the same move.
    
    However, if more than one piece can capture, the player has free choice over which piece to move. Likewise, if one piece can capture in multiple
    directions then the player has the choice in which direction to move. 
    
    
    \textbf{Note:} it is not compulsory for the player to move in the direction, or with the piece,
    that will lead to the greatest number of captures in that move.
    
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.6]{double hop.png}
        \caption{Visualization of multiple captures in one move}
    \end{figure}

    A move may only end when the position has no more captures available or an uncrowned piece becomes a King. 
    The game ends when all of a players piece's have been captured, or a player has no available moves.

    \pagebreak
    \subsubsubsection{Neural Networks}
    \subsubsubsubsection{What is a neural Network}
    A neural network is a machine learning model which aims to mimic the processes of the human brain.
    Each network contains inputs and outputs, as well as one or more layers of hidden nodes - which act as artificial neurons.

    Neural networks are a supervised learning model, meaning that they learn from labeled data (which has the objective correct answer in the data).
    They are sometimes referred to as artificial neural networks (ANNs) or simulated neural networks (SNNs).\\

    In a fully connected network, each node is connected once to each node in the next layer - an example of how one node connects
    to the next layer can be seen below.
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.2]{ConnectedNode.png}
        \caption{Example of a fully connected node and layer}
    \end{figure}

    Neural networks can be modelled as a collection linear regression units.

    \subsubsubsubsection{Linear Regression Unit}
    A single linear regression unit output has the formula:
    \begin{align}
        \hat{y} = \sum_{i=0}^{n} w_ix_i + b
    \end{align}

    Where $\hat{y}$ is the predicted output, $n$ is the number of inputs, $x_i$ is the $i$th input, $w_i$ is the weight of $x_i$, and $b$ is a bias.
    If, for example, there were 3 inputs the full equation for $\hat{y}$ would be:
    \begin{align}
        \hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + b
    \end{align}

    \subsubsubsubsection{Vectorization of processes}
    This calculation can be vectorized to improve efficiency and would be notated:
    \begin{align}
        \hat{y} = XW + b
    \end{align}

    Where we let
    \begin{align}
        W = \begin{bmatrix}
            w_1\\
            w_2\\
            \vdots\\
            w_n
        \end{bmatrix}\\
        \hspace{25px}
        X = \begin{bmatrix}
            x_1&
            x_2&
            \hdots&
            x_n
        \end{bmatrix}
    \end{align}

    $X$ here is a row vector as this is the most common format for data as an input to a network (e.g. being read from a csv).
    This parallelized computation is much faster, and can be parallelized using the GPU to further improve speed and efficiency.

    \subsubsubsubsection{Forward Pass of Dense Layer}
    In the case of neural networks, lots of these linear regression units can be combined to form a vector of outputs.
    Each of these regression units will have the same inputs, therefore $X$ can have the same definition. However, $W$ will now be
    composed of multiple vectors of weights, instead of just one.

    Here we let
    \begin{align}
        W = \begin{bmatrix}
            w_{11} & w_{21} & \dots & w_{j1}\\
            w_{12} & w_{22} & \dots & w_{j2}\\
            \vdots & \vdots & \ddots & \vdots\\
            w_{1n} & w_{2n} & \dots & w_{jn}
        \end{bmatrix}
    \end{align}

    Where $j$ is now the number of nodes in the layer. If we rewrite our forward pass equation to use this weight matrix,
    with each node as its own regression unit:
    \begin{align}
        Y = XW + B
    \end{align}

    Where:
    \begin{align}
        Y = \begin{bmatrix}
            \hat{y_1}&
            \hat{y_2}&
            \hdots&
            \hat{y_n}
        \end{bmatrix} \hspace{15mm}
        B = \begin{bmatrix}
           b_1 & b_2 & \dots & b_j
        \end{bmatrix}
    \end{align}

    With $W$ and $X$ having the same definition as most recently defined.

    \subsubsubsubsection{Forward Pass of a Convolutional Layer}
    The main function of a convolutional layer is to extract key features, and they are often used in image recognition and classification solutions.

    \begin{wrapfigure}[15]{r}{0.5\linewidth}
        \centering
        \includegraphics[scale=0.2]{ConvInputIndependentCode.png}
        \caption{Visualization of a Convolutional layer}
    \end{wrapfigure}

    A convolutional layer takes in a 3-dimensional block of data. This can be imagined as the first 2 dimensions being a grid of pixels, and the third dimension
    being the colour channels for each grid. The trainable parameters in a convolutional network are the kernels/filters and the biases. The kernels can be of any size
    but must have the same depth as the input data. Each kernel also has a bias matrix, which has the same shape as the output.

    After the forward pass, the output will also be 3-dimensional, but here the depth is equal to the number of kernels, and the size is equal to

    \begin{align}
        I - K + 1
    \end{align}

    Where $I$ is the size of the input grid and $K$ is the size of the kernel grids.

    The forward pass can then be simplified, with each output equating to the bias for that kernel plus the sum of the sum of cross correlations between a single channel
    of the input and its respective channel in the kernel. This can be seen in the figure below, where $d$ is equal to the number of kernels, and $n$ is the number of channels in the input data.


    \pagebreak
    \begin{figure}
        \centering
        \includegraphics[scale=0.3]{ConvForwardPropIndependentCode.png}
        \caption{Representation of the forward pass of a convolutional layer}
    \end{figure}

    This can be written more mathematically using a summation.

    \begin{align}
        Y_i = B_i + \sum_{j=1}^{n}X_j \star K_{ij}, \hspace{1cm} i = 1 \dots d
    \end{align}

    \subsubsubsubsection{Gradient Descent} \label{Analysis-GradientDescent}

    Now, to update the weights of our model, we can compute these values in closed form, however, it comes with a large time complexity (greater than $O(n^3)$), and therefore
    a process called gradient descent is usually employed.

    Gradient descent works to minimize the error of a model by iteratively locating a minimum in the error function.
    For example, a common cost (error) function is mean squared error.
    \begin{align}
        MSE(Y, \hat{Y}) = \frac{1}{n}\sum_{i=0}^{n}\left(y_i - \hat{y}_i\right)^2
    \end{align}

    This function effectively calculates the absolute distance between the predicted value of one data point ($\hat{y_i}$) and the true value at that same point ($y_i$),
    for every value in the dataset (of size $n$) and takes the mean of these distances.
    Again, this calculation can be vectorized using the equation below.
    \begin{align}
        MSE(Y, \hat{Y}) = \frac{1}{n}((Y - \hat{Y})^T \cdot (Y - \hat{Y}))
    \end{align}

    If we plot the MSE curve of different weights and biases - of a single regression unit, it looks as follows.
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{MSEplot.png}
        \caption{Plot of MSE in a single regression unit}
    \end{figure}

    Here the weight has been labelled $a$, but the bias has remained as $b$. As we can see, there is a clear local minimum of this error function,
    and the values that obtain this minimum, is what gradient descent aims to achieve. 
    
    \begin{wrapfigure}[14]{l}{0.4\linewidth}
        \centering
        \includegraphics[scale=0.2]{image.png}
        \caption{MSE with respect to one parameter}
    \end{wrapfigure}
    In figure 6 we can see a representation of the MSE plot with respect to only one of the parameters. This makes understanding the process of gradient
    descent much simpler. For example, the minimum of this function ($f(x) = \frac{x^2}{25}$) is at $x = 0$. So, if we were to have an initial $x$ value of say $-40$,
    and we calculated the gradient to be $3.2$, we could then subtract this from $40$. Resulting in a new value of $36.8$, which is closer to our optimal value of $0$.
    This process is completed iteratively until the minimum, or near to it, is reached.

    To denote this mathematically, we can say that 
    \begin{align}
        w_{t+1} = w_t - \alpha \frac{\partial E}{\partial w}\\
        b_{t+1} = b_t - \alpha \frac{\partial E}{\partial b}
    \end{align}

    Where $w_{t+1}$ and $b_{t+1}$ are the parameters at the next timestep, $w_t$ and $b_t$ is the value of the parameters at the current timestep, 
    $\frac{\partial E}{\partial w}$ and $\frac{\partial E}{\partial b}$ are the derivatives of the error with respect to each parameter, 
    and $\alpha$ is the learning rate. This learning rate controls the size of our 'jumps' and prevents the process from beginning to spiral away
    from the optimal values.

    That is a basic overview of gradient descent in a single regression unit, but in a dense network, the process is almost identical. An error function is evaluated, 
    the derivative with respect to the inputs calculated, and the new values updated. The main expense computation-wise is calculating all the derivatives, with respect to
    each set of parameters, as this requires passing the output from the error function backwards through all the layers and processes. To combat this, automatic differentiation
    can be used, which tracks computation dynamically at run time to compute derivatives.

    \subsubsubsection{Automatic Differentiation} \label{Autodiff-example}
    As mentioned, automatic differentiation tracks computation dynamically and then computes derivatives using a computational graph. This allows models to
    have much more complex forward passes, including decision branches and loops where the length of the loop is decided at runtime.

    Automatic differentiation operates by differentiating a complex function (that we don't know the derivative of), by treating it as a composition of more elementary functions
    (of which we do know the derivatives). Additionally, it treats each of these elementary functions as though their output is an intermediate variable when computing the
    complex function. This becomes very useful when there are multiple inputs to the function, and we only want the derivative to one of these variables. Finally, this process
    makes use of the chain rule to compute the derivative with respect to the inputs, as will be shown later on.

    Firstly, I will walk through an example of the process of automatic differentiation.\\
    \noindent Say we have a function: \begin{align}
        f(x) = ln\left(\frac{sin(x_1)}{cos(x_2)}\right)
    \end{align}

    This function has two arbitrary inputs (perhaps in the case of a neural network these could be our parameters or outputs for example). At first, this is quite a complex
    function to differentiate but, we can simplify it using intermediate variables. For example \begin{align}
        v_1 &= sin(x_1)\\
        v_2 &= cos(x_2)\\
        v_3 &= \frac{v_1}{v_2}\\
        v_4 &= ln(v_3)\\
        y &= v_4
    \end{align}

    \begin{wrapfigure}[23]{l}{0.5\linewidth}
        \centering
        \includegraphics[scale = 0.34]{ExampleCompGraph.png}
        \caption{Example Computational Graph}
    \end{wrapfigure}
    Now that we have declared our intermediate variables, we can see how automatic differentiation splits up the original function into elementary ones.
    This then allows use to easily take derivatives. Firstly, we can draw out the computational graph for this function, using our intermediate variables,
    and this can be seen in Figure 7.
    This graph visualizes the forward pass of the function, and how the intermediate variables compose to form our original function.

    Now, when we want to take the derivative with respect to one of these inputs, we can simply reverse through the graph, taking the gradient of each intermediate
    variable, and using the chain rule to give us our final derivative.

    This process would look as follows: \begin{align}
        \frac{\partial y}{\partial x_1} = \frac{\partial y}{\partial v_4} \cdot \frac{\partial v_4}{\partial v_3} \cdot \frac{\partial v_3}{\partial v_1} \cdot \frac{\partial v_1}{\partial x_1}
    \end{align}

    If we computed each of these derivatives:
    \begin{align}
        \frac{\partial y}{\partial v_4} &= 1\\
        \frac{\partial v_4}{\partial v_3} &= \frac{1}{v_3}\\
        \frac{\partial v_3}{\partial v_1} &= \frac{1}{v_2}\\
        \frac{\partial v_1}{\partial x_1} &= cos(x_1)
    \end{align}

    And finally, evaluating equation $(8)$, as well as substituting our intermediate variables in terms of $x_1$, to obtain $\frac{\partial y}{\partial x_1}$:
    \begin{align}
        \frac{\partial y}{\partial x_1} &= \frac{cos(x_1)}{v_3 \cdot v_2}\\
        &= \frac{cos(x_1)}{v_1}\\
        &= \frac{cos(x_1)}{sin(x_1)}
    \end{align}

    This process of traversing through the graph backwards is known as reverse accumulation auto differentiation, and in practice each of the gradients would be
    stored numerically at computation, rather than computing a symbolic version of the final derivative and plugging the values in.

    \subsubsubsection{Reinforcement Learning (RL)} \label{RL Existing Analysis}
    Reinforcement learning aims to capitalize on learning through an environment (similar to how infants learn by playing), without a teacher, to improve computer's
    ability at a given task. Simply put, problems in reinforcement learning involve learning how to map certain situations to certain actions to maximize a reward signal.
    The models are not told what actions to take and instead must explore the environment and available actions to learn what yields the most rewards. In some cases, actions
    may not only affect immediate rewards, but also the possibility and/or size of future rewards.

    \begin{wrapfigure}{l}{0.5\linewidth}
        \centering
        \includegraphics[scale =0.5]{markovprocess.png}
        \caption{Diagram of Markov Decision Process}
    \end{wrapfigure}
    The solution to reinforcement learning problems can be modelled using a markov decision process (A diagram of which can be seen in Figure _). This diagram shows the process of
    one iteration of training. The agent will recieve a state ($S_t$) and then decide on an action to take ($A_t$). This action is sent to the environment, which process the action,
    evaluates the reward gained from that action ($R_t$), and then returns that reward and the next state ($S_{t+1}$).
    
    One challenge that becomes in apparent in reinforcement learning, that is not present in other machine learning methods (supervised and unsupervised), is that of \textit{exploration}
    vs \textit{exploitation}. This problem comes from the fact that to maximize any given reward signal, an agent must take the action which gains the most reward i.e. it must
    \textit{exploit} what it knows, and choose the best action. However, to have knowledge of which action is best at any given state, the agent must have \textit{explored} many different
    actions - in many different states. This challenge will be addressed in more depth later on, during the design of the agent.

    There are two main approaches to reinforcement learning problems, a model-free solution or model solution. Within model-free solutions there is another two categories of policy optimization
    and Q-learning. In policy optimization, the agent learns a policy which maps states to actions. There are two types of these policies - deterministic and stochastic. A deterministic policy
    maps without uncertainty i.e. the agent will take the same action given the same state. Stochastic policies on the other hand output a distribution which maps a state to the probability of 
    each action.

    \begin{wrapfigure}[17]{l}{0.4\linewidth}
        \centering
        \includegraphics[scale=0.4]{qlearning.PNG}
        \caption{Representation of Q-learning process}
    \end{wrapfigure}
    Q-learning can be represented as a tabular learning method, and works by learning the value of a function - usually $Q(s, a)$ - which represents how successful an action was at
    a certain state. Each of these values is stored in the 'table' and when the model next observes a given state it consolidates the table and chooses the action which was most
    successful in its past experiences.

    For model-based solutions, the models 'know' the rules of the games and learn from planning and construct a functional representation of the environment. This is different to non-model
    based which aims to learn by trial-and-error and experience. AlphaZero for example would be defined as a model-based agent. To better define this, if the agent is able to forecast the reward
    of an action given any state, allowing it to plan what actions to take, it is model-based.

    Model-based solutions are best when trying to optimize the reward for a task - such as playing chess. Whereas model-free solutions are best for tasks such as self-driving cars (as a model-based
    approach may run over a pedestrian just to try and complete the journey in the shortest time).

    \subsubsubsection{AlphaZero}
    As mentioned before, AlphaZero is model-based - but how exactly it works is unique and does not have a naming convention like Q-learning.

    AlphaZero uses a process called Monte Carlo Tree Search (MCTS) to build a tree of future variations of a game. This tree is built depth first - so only begins building a new branch, when the state in the
    branch currently being explored is terminal. When building this tree, MCTS must select a child node - if all possible new actions have been exhausted. To do this a formula called the upper confidence 
    bound (UCB) is used. The UCB formula can be seen below.
    \begin{align}
        UCB = \frac{w_i}{n_i} + C\sqrt{\frac{ln(N_i)}{n_i}}
    \end{align}
    
    In this equation, $w_i$ represents the number of wins a node has, $n_i$ represents the number of visits a node has, $N-i$ represents the number of visits the parent node has, and $C$ is a coefficient which
    controls the levels of exploration in the MCTS model.
    Also, after each branch has reached a terminal state the MCTS backpropogates and records how many times each node has been visited, and how many times that node has lead to a winning terminal state. 
    Using this recorded data, the MCTS model can build a distribution which models the probability of winning the game for each action in the current state. This allows the model to 'learn' the rules
    of the game, as illegal actions will have a probability of 0. This distribution can also be called the MCTS' policy.

    To improve this process, AlphaZero uses a neural network to help prune some branches of the tree which can be assumed to lead to a losing state. This allows the MCTS to search only more effective
    lines of play. The exact algorithm for this can be found in \myhy{sec:IDoA}{Identification of Algorithms}
    
    \subsubsection{Similar Systems}
    \begin{wrapfigure}[14]{r}{0.5\linewidth}
        \centering
        \includegraphics[scale=0.145]{1992Tinsleymatch.jpg}
        \caption{Image of Chinook playing a game of checkers - 1992}
    \end{wrapfigure}
    Chinook is a checkers playing computer program that was developed at the University of Alberta between
    1989 and 2007. The program utilises an opening book, deep-search algorithms alongside a position evaluation function
    and a solved endgame database for positions with 8 pieces or fewer.
    All of Chinook's knowledge has been programmed in by humans, with no use of an artificial intelligence model. Despite this,
    Chinook managed to beat the checkers world champion at the time of 1995, and after this no longer competed, but instead was
    tasked with solving checkers. (A solved game is one where the outcome can be predicted from any position) - and this was achieved in 2007.

    \begin{wrapfigure}[16]{l}{0.5\linewidth}
        \centering
        \includegraphics[scale=0.7]{download.jpg}
        \caption{Chess champion Garry Kasparov loses to Chess engine - 1997}
    \end{wrapfigure}
    Other similar systems include chess engines - of which there is a multitude. Chess engines operate similarly to Chinook, creating trees
    of possible moves and using a minimax algorithm, as well as alpha-beta pruning to determine the best move. Additionally, there are some
    chess engines which use reinforcement learning, such as Google DeepMind's AlphaZero engine. This engine has also been trained on games
    such as Go and Shogi. AlphaZero is trained mainly using self-play, where the AI plays against itself and then learns from those games.
    In a 100 game match against StockFish (one of the most popular chess engines) AlphaZero won 28 games, lost 0 and drew the remaining 72.
    This shows the power of reinforcement learning as a tool for board games and its possibility to be used for checkers.

    For automatic differentiation, some of the most popular systems include PyTorch's autograd, as well as autodiff by JAX. PyTorch has a very
    well-developed library for deep-learning alongside it's autograd, as well as extensive documentation. Additionally, autograd is more similar
    to regular python than JAX and may be easier to use when first delving into the area. However, JAX builds on top of autograd and accelerates
    the linear algebra using an XLA backend. This backend optimizes operations for CPU's GPU's and TPU's as well as using a just-in-time compiler
    to decrease runtimes.

    \subsubsection{Interview with third party}
    For my interview, I reached out to an external person who works in the AI industry. We discussed where my time would be most well spent,
    some industry standards I should try and implement as well as some expansionary objectives I could try to include to improve my project.

    To start with, with the large amount of work that needs to be done, if time begins to run short there are lots of out-of-the-box frameworks
    to use and implement (such as PyTorch and JAX - mentioned earlier), while still allowing me to finish my project with sufficient difficulty.
    Secondly, the large proportion of my time should be spent researching and understanding the different solutions available which would best
    suit my problem, and how they are implemented. This research should include, but not be restricted to: reinforcement learning solutions for
    other games (such as Go and Chess), different attributes of reinforcement learning models and which environments they are most capable in, as well
    as research on the overheads of similar models.

    It is very important that the correct solution is chosen, so minimal time is spent on implementation.

    As for possible objectives:
    \begin{center}
        \begin{tabular}{|m{15em}|m{15em}|}
            \hline 
            Objective & Reasoning \\
            \hline
            Leverage GPU cores for linear algebra computations & Allows parallelization and will increase computation speeds  \\      
            \hline
            Wrap the console checkers board in a GUI & Much easier for users to use and understand \\
            \hline
            Use a container such as Docker to produce logs & Much better retracing and debugging capabilities \\
            \hline
            Evaluate the model against Chinook and human players & Allows for rating of model \\
            \hline
        \end{tabular}
    \end{center}

    \subsection{Modelling of the problem}

    \subsubsection{Identification of Objects}

    \subsubsubsection{Checkers}
    For checkers, we will need two classes, a checkers board/environment and a checkers game. The environment will be used for training the agent, and It will require methods that
    execute an action in the environment, as well as functions to compute the available moves and check for a win. An example of UML for this environment can be seen below. 

    \begin{figure}[h]
        \centering
        \includegraphics[scale = 0.24]{DraughtsUMLdrawio.png}
        \caption{UML for checkers/checkers game}
    \end{figure}

    The checkers game varies only slightly and will inherit from the checkers board. It will however add some functionality such as a GUI and the ability to choose different
    models to play against (such as just the MCTS or the full agent). Additionally, it should allow two people to play each other as checkers usually would.

    \subsubsubsection{Automatic Differentiation Engine}
    \begin{wrapfigure}{l}{0.5\linewidth}
        \centering
        \includegraphics[scale=0.02]{AutoDiffUML.drawio.png}
        \caption{Example UML for Auto Diff}
    \end{wrapfigure}
    The automatic differentiation engine will be based off of the 'Tensor' object. This object will be similar to a matrix but have some additonal
    attributes, such as its gradient and if its gradient requires computing. If the tensor does require a gradient then it will also store its parent
    tensors, which will be used for the backward DFS of the computational graph.

    Additionally, all operators for the tensor object will have to be overridden to allow for gradient computation if the tensor requires it.
    The tensor class will also have functions to zero out the gradient as well as begin the backwards traversal through the computational graph. Below is an example
    of some possible UML for the automatic differentiation library.

    Tensor operations will all have a backward and forward method - inherited from the function class. These functions will represent elementary functions (Addition, Division, Log, etc.). 
    The operations such as sum and mean, that operate on the tensor itself will also be represented by a function object.

    \subsubsubsection{Neural Network Library}
    Their will be a large collection of objects inside of the neural network library, all based around the functionality provided by the tensor object.
    Firstly, the parameter class will allow for tensors of a given shape to be created and act as parameters for a network - this will also help with backpropogation.
    The module object will have a parameters property which yields every parameter related to that module. This module class is what all layers will inherit from. It will also
    have methods to zero out the gradients of all parameters.

    Secondly, their will be classes for each different type of layer (dense and activations). The dense layer will have attributes which store the parameters required
    for computation as well as a method to complete the forward pass through the layer. Activation functions will compute the forward pass, as well as store the gradient function
    for any tensors which require it. The activation layers will not need any attributes as they have no parameters.

    The optimizer class is the base class for any optimizers such as stochastic gradient descent or momentum-based gradient descent. This class will have attributes which store
    the learning rate as well as any parameters the optimizer is related to. It will also have virtual methods - which will be overriden by the specific optimizer classes - of
    a step (which updates all parameters), a function to zero the gradient of all parameters and finally a function to add parameters.

    \pagebreak
    \vspace*{7mm}
    Below is an inheritance diagram for the Neural Network Library, without any attributes and methods.

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{NNUML (simple).drawio.png}
        \caption{Inheritance diagram for Neural Network Library}
    \end{figure}

    \subsubsubsection{Reinforcement Learning Solution}
    The final solution will also require a large set of classes including: a network class - to output the model policy and predicted value; a Monte Carlo tree searach (MCTS) class to compute
    a search of available actions to train the network towards - composed of a node class to make up this tree - and finally an agent class which abstracts the processes of the previous classes. \\

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{RLSolutionUML.png}
        \caption{Reinforcement Learning Solution UML}
    \end{figure}

    \pagebreak
    \subsubsection{Identification of algorithms} \label{Analysis-IoA}
    \label{sec:IDoA}
    \subsubsubsection{Automatic Differentiation Engine}
    The auto-diff engine will use a recursive DFS of the computational graph to calculate gradients. It uses a DFS as any of the computational graphs that built are directed acyclical graphs,
    and therefore DFS provides a way to search the whole graph in O(n) time. Some pseudocode for this search can be seen below.

    \begin{algorithm} 
        \caption{Backward Function of Tensor}\label{alg:cap}
        \begin{algorithmic}
        \Function{Backward}{Tensor self, Array grad, Tensor out}
        \Ensure t.requiresGrad is True \Comment{t is the tensor being called to start DFS}
        \If{t.Grad is None}
            \State grad $\gets \begin{bmatrix}
                1 & 1 & 1 & \hdots
            \end{bmatrix}$ in shape of t.data
        \EndIf
        
        \State t.Grad += grad

        \If{t.Operation is not None}
            \State t.Operation.backward(t.Grad, t)
        \EndIf

        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
        \caption{General Backward Function of a Tensor Operation}\label{alg:cap}
        \begin{algorithmic}
        \Function{backward}{Operation self, Array grad, Tensor t}
        \State savedTensors $\gets$ self.cache.0
        
        \For{tensor in savedTensors}
        \If{tensor.requiresGrad is True}
        \State newGrad $\gets \frac{\partial output}{\partial tensor}$ \Comment{Computed by chain rule, specific for each operation}
        \If{broadcastedDims}
        \State Remove broadcasted dims from newGrad \Comment{newGrad.shape should equal tensor.shape}
        \EndIf
        \State tensor.backward(newGrad, t)
        \EndIf
        \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \subsubsubsection{Neural Network Library} \label{Analysis-IofA-NNLib}
    The two main algorithms in the neural network library are the forward pass and the backward pass through a collection of layers. Pseudocode for these can also been seen below.
    \begin{algorithm}
        \caption{Forward pass through collection of layers}\label{alg:cap}
        \begin{algorithmic}
        \Function{forward}{Module model, Tensor input}
        \State output $\gets$ input
        
        \For{layer in model.layers}
        \State output $\gets$ layer.forward(output) \Comment{layer's forward function depends on layer type (Dense/Conv)}
        \EndFor
        return output
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
        \caption{Training process of a module}\label{alg:cap}
        \begin{algorithmic}
        \Function{fit}{Module model, Optimizer optim, Loss l, Tensor x, Tensor y, int epochs = 100, int verbose = 1}
        \State startTime $\gets$ time.now()
        
        \For{epoch in epochs}
        \State prediction $\gets$ model.predict(x) \Comment{Get predictions}
        \State loss $\gets$ l(y, prediction) \Comment{Calculate how far predictions are from true}
        \State optimizer.zero_grads() \Comment{Reset gradients of parameters}
        \State loss.backwards() \Comment{Calculate gradients of loss}
        \State optimizer.step() \Comment{Update parameter values using calculated gradients}

        \If{verbose is 1}
        \State print(epoch, loss)
        \EndIf

        \EndFor
        print(time.now() - startTime) \Comment{Total time elapsed}
        \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    \pagebreak
    \subsubsubsection{Reinforcement Learning Solution}
    For the reinforcement learning, one of the most important algorithms is the MCTS which is similar to a DFS. This takes all
    the available moves in the checkers environment, selects one at random and continues this process until a terminal state is reached.
    After the terminal state is reached, reward values (+1 for win, -1 for loss, 0 for draw) are backrpopagated through the nodes.
    If all possible available moves have been exhausted at a given node then it selects one using an upper confidence bound (UCB) formula.
    The general algorithm can be seen in pseudocode below (as well as the nodes expand function).

    \begin{algorithm}
        \caption{MCTS Tree Building Function}\label{alg:cap}
        \begin{algorithmic}
        \Function{buildTree}{currentBoardState}
        
        \State self.root $\gets$ currentBoardState

        \For{seach in numberOfSearches}
        \State node $\gets$ self.root \Comment{Return to start of tree for each search}
        \If{node.brancesAvailableToExpand == 0}
        \State node $\gets$ node.select_child()
        \EndIf
        
        \While{node.terminal is False AND node.branchesAvailableToExpand $>$ 0}
        \State node $\gets$ node.expand() \Comment{Depth-first expansion until terminal state}
        \EndWhile

        \State node.backpropogate(node.reward)
        \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    The second algorithm is the process of the final agent model selecting its actions and can be seen on the next page.

    \begin{algorithm}
        \caption{Agent Policy Selection}\label{AgentTrainingAlg}
        \begin{algorithmic}
        \Function{trainAgent}{self, initialNeuralNetwork, hyperParameters}
        
        \State nnPrevious $\gets$ initialNeuralNetwork
        \For{mctsEpoch in hyperParameters.mctsEpochs}
            \State trainingExamplesList $\gets$ SPV(hyperParameters.maxExamples)
            \For{episode in hyperParameters.episodes}
                \State examples $\gets$ list()
                \State game $\gets$ new CheckersGame
                \While{!game.terminated}
                    \State mcts $\gets$ MCTS(nnPrevious, game.currentBoardState, hyperParameters).buildTree()
                    \If{game.step $<$ hyperParameters.Threshold}
                        \State action $\sim$ mctsPolicy.getPolicy() \Comment{Select random move}
                    \Else
                        \State action $\gets$ mcts.getAction()
                    \EndIf
                    \State examples.append((game.currentBoardState, mcts.getPolicy(), initialNeuralNetwork.value))
                \EndWhile
                \State trainingExamplesList.append(examples)
                \For{networkEpoch in hyperParameters.networkEpochs}
                    \State batch $\gets$ trainingExamplesList.getBatch(hyperParameters.batchSize)
                    \State nnNew $\gets$ nnPrevious.train(batch, hyperParameters)
                \EndFor
                \State numberNNNewWins = 0
                \For{compareGame in hyperParameters.compareGames}
                    \State game $\gets$ new CheckersGame
                    \While{!game.terminated}
                        \If{game.player == player1}
                            \State action $\gets$ MCTS(nnPrev, game.currentBoardState, hyperParameters).getAction()
                        \ElsIf{game.player == player2}
                            \State action $\gets$ MCTS(nnNew, game.currentBoardState, hyperParameters)
                        \EndIf
                    \EndWhile
                \EndFor
                \State nnNewWinPct = numberNNNewWins/hyperParameters.compareGames
                \If{nnNewWinPct $>$ hyperParameters.replaceThresholdPct}
                    \State nnPrev $\gets$ nnNew
                \EndIf
            \EndFor
        \EndFor
        \State \Return nnPrev
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    
    \pagebreak
    \subsubsection{Mathematical Formulae} \label{Analysis-MathematicalFormulae}
    \subsubsubsection{Automatic Differentiation Engine}
    In this section I will simply define the formulas for the forward and backward pass of many different functions that will be implemented
    into the autograd library.
    \subsubsubsubsection{Addition}
    \subsubsubsubsubsection{Forward}
    \begin{align}a + b = y\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = 1 + 0\end{align}\\
    \begin{align}\frac{\partial y}{\partial b} = 0 + 1\end{align}

    \subsubsubsubsection{Negation}
    \subsubsubsubsubsection{Forward}
    \begin{align}y = -a\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = -1\end{align}

    \subsubsubsubsection{Multiplication}
    \subsubsubsubsubsection{Forward}
    \begin{align}a \cdot b = y\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = b\end{align}
    \begin{align}\frac{\partial y}{\partial b} = a\end{align}

    \subsubsubsubsection{Division}
    \subsubsubsubsubsection{Forward}
    \begin{align}\frac{a}{b} = y\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = \frac{1}{b}\end{align}
    \begin{align}\frac{\partial y}{\partial b} = -\frac{a}{b^2}\end{align}

    \subsubsubsubsection{Matrix multiplication}
    \subsubsubsubsubsection{Forward}
    \begin{align}
        AB = Y &&\text{where A and B are matrices}
    \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial Y}{\partial A} = B^T\end{align}
    \begin{align}\frac{\partial Y}{\partial B} = A^T\end{align}

    \subsubsubsubsection{Powers}
    \subsubsubsubsubsection{Forward}
    \begin{align}a^b = y\end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align}\frac{\partial y}{\partial a} = ba^{b-1}\end{align}
    \begin{align}\frac{\partial y}{\partial b} = a^bln(a)\end{align}

    \subsubsubsubsection{Mean}
    \subsubsubsubsubsection{Forward}
    \begin{align}\frac{\sum A}{n} = y, && A = (x_0, x_1, x_2, \hdots x_n) \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align} \frac{\partial y}{\partial A} = B, && B = ({\frac{1}{n}}_0, {\frac{1}{n}}_1, {\frac{1}{n}}_2 \hdots, {\frac{1}{n}}_n) \end{align}
    
    \subsubsubsubsection{Sum}
    \subsubsubsubsubsection{Forward}
    \begin{align} \sum A = y, && A = (x_0, x_1, x_2, \hdots x_n) \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align} \frac{\partial y}{\partial A} = B, && B = ({1}_0, {1}_1, {1}_2 \hdots, {1}_n) \end{align}

    \subsubsubsubsection{Natural Log}
    \subsubsubsubsubsection{Forward}
    \begin{align} ln(a) = y \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align} \frac{\partial y}{\partial a} = \frac{1}{a} \end{align}

    \subsubsubsubsection{Exponentation}
    \subsubsubsubsubsection{Forward}
    \begin{align} e^a = y \end{align}
    \subsubsubsubsubsection{Backward}
    \begin{align} \frac{\partial y}{\partial a} = e^a \end{align}

    \subsubsubsubsection{Conv2D}
    \subsubsubsubsubsection{Forward}
    \begin{align} Y_i = B_i + \sum_{j=1}^{n}X_j \star K_{ij}, \hspace{1cm} i = 1 \dots d\end{align}
    Similar to the current systems, the star denotes the valid cross-correlation of the two matrices.\\
    \textbf{Example of cross-correlation}\\
    Cross correlation can be imagined as placing the kernel ontop of the input and summing the products of the elemts which overlap, and then moving the kernel and repeating the process.
    \begin{align}
        \begin{bmatrix}
            1 & 6 & 2 \\
            5 & 3 & 1 \\
            7 & 0 & 4 \\
        \end{bmatrix} \star \begin{bmatrix}
            1 & 2 \\
            -1 & 0 \\
        \end{bmatrix} &= \begin{bmatrix}
            1 \cdot 1 + 2 \cdot 6 + -1 \cdot 5 + 0 \cdot 3 & 1 \cdot 6 + 2 \cdot 2 + -1 \cdot 3 + 0 \cdot 1 \\
            1 \cdot 5 + 2 \cdot 3 + -1 \cdot 7 + 0 \cdot 0 & 1 \cdot 3 + 2 \cdot 1 + -1 \cdot 0 + 0 \cdot 4
        \end{bmatrix}\\
        &= \begin{bmatrix}
            8 & 7 \\
            4 & 5
        \end{bmatrix}
    \end{align}

    This method of cross-correlation is known as valid cross correlation (where the upper left element of the kernel 
    begins on the upper left element of the input).
    \subsubsubsubsubsection{Backward}
    
    In this backward function, $\frac{\partial Z}{\partial Y}$ will denote the gradient being recieved from upstream.
    \begin{align}
        \frac{\partial Z}{\partial B_i} &= \frac{\partial Z}{\partial Y_i}\\
        \frac{\partial Z}{\partial K_{ij}} &= X_j \star \frac{\partial Z}{\partial Y_i}\\
        \frac{\partial Z}{\partial X_i} &= \sum_{i=1}^{d}\frac{\partial Z}{\partial Y_i} * K_{ij}
    \end{align}

    The asterisk in the final expression represents the full convolution operation, which is the same as a full cross-correlation, but
    with the kernel rotated 180 degrees. Additionally, a full correlation begins the operation with the bottom left elemnt of the kernel
    being placed upon the top left element of the input - and then the sliding and summing of products process remains the same.
    
    \subsubsubsection{Neural Network Library}
    \subsubsubsubsection{Activation Layers}
    In both dense and convolutional layers, the only operation that is occuring is multiplication and/or matrix multiplication. Graphically,
    these operations only create linear transformations however in machine learning some problems require non-linearity. To allow for non-linearity
    neural networks use activation layers which are non-linear functions, normally places between layers.
    
    \subsubsubsubsubsection{Tanh}
    \begin{align}
        tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    \end{align}

    \subsubsubsubsubsection{Sigmoid}
    \begin{align}
        S(x) = \frac{1}{1 + e^{-x}}
    \end{align}

    \subsubsubsubsubsection{Softmax}
    The softmax function is used to turn a vector into a probability distribution. For example, if we had data in the shape (batches, classes) and we wanted to getAction
    the probability of each batch being in a certain class we would use the softmax function across the axis with the number of class, in this case axis 1.
    \begin{align}
        \sigma(x) = \frac{e^x}{\sum e^x} \hspace{10mm} \text{Sum across axis with number of classifications in x}
    \end{align}

    \subsubsubsection{Reinforcement Learning Solution}
    \subsubsubsubsection{UCB Formula}
    The UCB formula is used in MCTS to select the most favourable child when the parent node has been fully expanded. The formula can be seen below, where
    $N_i$ is the number of visits the parent node has, $n_i$ is the number of visits the child node has and $w_i$ is the value count that the node holds. The
    value count is updated each iteration during backpropogation. Finally, $C$ is a coefficient which determines the levels of exploration and exploitation.

    \begin{align}
        UCB(Node_i) = \frac{w_i}{n_i} + C \sqrt{\frac{ln N_i}{n_i}}
    \end{align}

    \subsection{Set of objectives}
    \subsubsection{Checkers Game}
    \begin{enumerate}
        \item An easy-to-use and intuitive graphical interface for the checkers game
        \begin{enumerate}
            \item A working version of checkers - obeying all rules stated in \myhy{checkers}{1.2.1.1}
            \item A small text tutorial on the rules of checkers
            \item A visual representation of the checkers board
            \item A method of displaying all legal available moves in a selected position
            \item Error messages if an illegal move is attempted
        \end{enumerate}
        \item Implementation of various decision-making agents for users to play against
        \begin{enumerate}
            \item Option for the user to choose which agent to play against (MCTS, Full Agent)
            \item Ability to adjust hyperparameters of the agents from the checkers game interface (see \myhy{RLS Objs}{RLS Objectives a-e})
        \end{enumerate}
    \end{enumerate}
    \subsubsection{Autodifferentiation engine} \label{AutodifferentiationObjectives}
    \begin{enumerate}
        \item A unit-tested autograd engine capable of calculating derivatives on a variety of functions inlcuding:
        \begin{enumerate}
            \item Addition
            \item Negation
            \item Multiplication
            \item Division
            \item Matrix Multiplication
            \item Powers
            \item Mean
            \item Sum
            \item Natural Log
            \item Exponentiation
            \item Transpose
            \item Padding
            \item 2D Convolution
        \end{enumerate}
    \end{enumerate}
    \subsubsection{Neural Network Library} 
    \begin{enumerate} 
        \item A modular machine learning library including:
        \begin{enumerate}
            \item The ability to save a model
            \item Support for storing a collection of layers/modules
            \item Dense/Fully Connected Layer
            \item 2D Convolution Layer
            \item Reshape Layer
            \item Activation Layers:
            \begin{enumerate}
                \item ReLU
                \item Softmax
                \item Sigmoid
                \item Tanh
            \end{enumerate} 
            \item Gradient descent optimization
            \item Loss Functions:
            \begin{enumerate}
                \item Mean Squared error
                \item AlphaZero's Loss Function
            \end{enumerate}
        \end{enumerate}
    \end{enumerate}

    \subsubsection{Reinforcement Learning Solution} \label{RLS Objs}
    \begin{enumerate}
        \item A training algorithm with hyperparameters of:
        \begin{enumerate}
            \item Number of example games to be played
            \item Number of comparison games to be played
            \item Number of searches the MCTS completes
            \item The exploration exploitation coefficient for UCB in MCTS
            \item The percentage of comparison games the new network must win to replace the previous
            \item The number of epochs for neural network training
            \item The number of iterations where a new network is trained
        \end{enumerate}
        \item The functionality to load a saved agent and play against it
    \end{enumerate}

    \section{Design}
    \secttoc

    \subsection{File Structure and Project Organisation} \label{File-Structure}
    \dirtree{%
        .1 *ns-*eec-*te-*cg-*rt.
        .1 poetry.lock.
        .1 pyproject.toml.
    }
    At the very top level of my file structure I have a poetry.lock and a pyproject.toml file. These files are automatically generated by
    a python package called poetry. This package provides exceptionally easy dependency and virtual environment management. Allowing me to
    set up machines other than my own - to train the model - more easily. 
    The final file inside of this directory actually represents a collection of files - with each * denoting a different value that each of the 
    parameters for the final model can take (with the two letters allowing me to identify said parameter). These files are created using python's built in pickle object serialization library.
    \dirtree{%
        .1 nea-code.
        .2 AlphaMCTSSelfPlay.py.
        .2 AlphaModel.py.
        .2 CheckersGamePvP.py.
        .2 CheckersGuiGame.py.
        .2 MCTSSelfPlay.py.
        .2 PvMCTS.py.
        .2 TestNNXOR.py.
        .2 TrainAgent.py.
    }
    At one level inside my main code-storing file I have files that run main operations in my code. This includes specific game loops, such as getting the user to play 
    vs MCTS - as well as processes such as beginning agent training and testing of the Neural Network Library using a simple problem such as XOR.
    \dirtree{%
        .1 nea-code.
        .2 agent.
        .3 agent.py.
        .3 consts.py.
        .3 memory_types.py.
        .3 __init__.py.
    }
    Inside of nea-code I also create a subdirectory for each module of my project. In the above example this modeule is the agent class. As well as storing the file which defines the agent class (agent.py)
    it also contains an \_\_init\_\_.py file (which allows python to recognise the subdirectory of nea-code as a module and defines what dependencies to import when this module is called), a consts.py (which contains
    values, and functions to create values, which do not change throughout the running of the code) and a memory_types.py file, where dataclasses such as SAP and SPV (which will be defined later on. hereTODO ADD LINK) are defined.
    \dirtree{%
        .1 nea-code.
        .2 checkers_gui.
        .3 buttons.py.
        .3 checkers_gui.py.
        .3 consts.py.
        .3 helpers.py.
        .3 __init__.py.
        .4 media.
        .5 images.
        .6 black_k.png.
        .6 black_r.png.
        .6 checkers_board_no_pieces.png.
        .6 white_k.png.
        .6 white_r.png.
    }
    The next module is the GUI add-on for the checkers game. Similarly to the agent module, it contains \_\_init\_\_.py, consts.py, a main class definition file (checkers\_gui.py) and other class definitions file (buttons.py). 
    However, the checkers GUI module also contains a helpers.py file - which holds onto functions that are helpful relevant to the module as well as a subtree of media/images/ which contains the .png's of the different sprites used in the GUI.
    \dirtree{%
        .1 nea-code.
        .2 console_checkers.
        .3 checkers_game.py.
        .3 consts.py.
        .3 jit_functions.py.
        .3 utils.py.
        .3 __init__.py.
    }
    Inside the next module (console\_checkers) we have again an init, consts and main defintion file. On the other hand, instead of a helpers.py file there is a utils.py. The difference here is that
    utils.py contains utility functions not strictly relevant to the module. For example, this utils.py contains a funtion which clears the console (a process which would not be limited to the console\_checkers module).
    Another difference is the presence of the jit_functions.py file. Inside of this module there are lots of functions which run only using primitive types, and have a polynomial time complexity. To reduce the impact this 
    has on performance I use a python package called numba, which contains a just-in-time compiler (jit) to speed up these functions.
    \dirtree{%
        .1 nea-code.
        .2 mcts.
        .3 consts.py.
        .3 mcts.py.
        .3 __init__.py.
    }
    This MCTS module contains definitions of the MCTS classes, again with a consts and init file.
    \dirtree{%
        .1 nea-code.
        .2 ml.
        .3 __init__.py.
        .3 autograd.
        .4 consts.py.
        .4 convolve_funcs.py.
        .4 jit_functions.py.
        .4 tensor.py.
        .4 test_autograd.py.
        .4 helpers.py.
        .4 __init__.py.
        .3 nn.
        .4 layers.py.
        .4 optimizers.py.
        .4 __init__.py.
    }
    In the above snippet I have defined a module named ml (for machine learning), which contains two submodules of autograd and nn - which contain the autodifferentiation engine and neural
    network library respectively. The nn submodule has a similar content as the modules talked about previously, with layers.py defining the main classes, and optimizers.py then defining classes
    of optimizers to be used alongside the defined layers. The autograd submodule however has a few differences. It also has a jit_functions and helpers file but additionally contains a convolve_funcs.py and
    test_autograd.py file. These files are unique to the autograd module. The convolve_funcs has definitions for complex matrix operations such as convvolution and cross-correlation, and the test_autograd file
    contains unit tests (run using pytest) to evaluate and ensure the accuracy of the autodifferentiation engine.
    \dirtree{%
        .1 nea-code.
        .2 network.
        .3 network.py.
        .3 __init__.py.
    }
    The final module is network, which contains the definitions for compositions of different layers from the nn module - which then build to form the model that is contained within the reinforcement
    learning agent.
    \subsection{Checkers} \label{CheckersDesignStage}
    The checkers game solution will be split into a console based version (that will be the main way for the agent to interact with the game) and
    an additonal class built ontop of this console solution which provides a GUI.

    \subsubsection{Data Structures}
    When defining data structures such as tuples, a comma followed by an ellipse indicates another item which is of the same type as most recently defined item
    in the tuple. e.g. tuple[int,...] = tuple[int, int]
    \subsubsubsection{Console Solution}
    \begin{center}
        \begin{tabular}{|m{10em} | m{12em} | m{15em}|}
            \hline 
                Item & Data Structure & Reasoning and Usage \\
            \hline
            \hline
                Board & 2D Array & Easy and logical to index into specific squares as well as render. \\
            \hline
                Last Moved Piece & tuple[int,...] & Represents position of the piece most recently moved - easy to reaccess.\\
            \hline
                Action & tuple[tuple[int,...],...] & First tuple inside of the outer tuple represents the piece being moved, and the second
                inside tuple represents where it is being moved to which allows for easy indexing into the board array.\\
            \hline
        \end{tabular}
    \end{center}
    For a single game of checkers, an upper bound for the number of unique states that can be presented on the board can be calculated as follows (this is useful for getting a gauge
    of the size of training sets compared to the possible set of states):\\

    \noindent Let $0 <= w <= 12$ and $0 <= b <= 12$  where $w,b \in \mathbb{Z}$\\
    Then,
    \begin{align}
        {32}\choose{w}
    \end{align}
    Is the number of ways to play $w$ white pieces on the board, and
    \begin{align}
        {32-w}\choose{b}
    \end{align}
    Is the number of ways to play $b$ black pieces, given $w$ white pieces already occupy a square.
    And for each piece, it can be a king or a regular piece, so we multiply by a factor of $2^{w+b}$.
    Finally, to find the number of legal configurations for all $w$ and $b$:
    \begin{align}
        \sum_{b=0}^{12}\sum_{w=0}^{12} {{32}\choose{w}} {{32-w}\choose{b}} 2^{w+b} \approx 2.3\times10^{21}
    \end{align}
    Note: Not all of these positions can be reached via a sequence of legal moves - and the actual number (from
    the paper "Checkers is solved") is $\approx 5 \times 10^{20}$

    Additonally, the set of all possible actions can be written as follows:
    \begin{align}
        R &= \{0,1, ... ,7\}\\
        C &= \{A, B ..., H\}\\
        \text{Possible Actions} &= \{((x,y), (w,z)) \mid x, w \in R \text{^} y,z \in C\}
    \end{align}

    \subsubsubsection{GUI Add-on} \label{CheckersGUIDesignStage}
    \begin{center}
        \begin{tabular}{|m{10em} | m{12em} | m{15em}|}
            \hline 
                Item & Data Structure & Reasoning and Usage \\
            \hline
            \hline
                CheckersGUI & Class & Inherits from console solution and stores information about the game such as the piece the user has selected. \\
            \hline
                Selected Piece & tuple[int,...] & Represents position of the selected piece - easy to index into board array. \\
            \hline
                Main Menu & Class & Stores, displays and allows user to change information regarding the parameters and game mode that the user would like to play with. Also initiates game loops.\\
            \hline
                Button & Class & Provides a simpler to use and understand interface between pygame surfaces and user interactions. \\
            \hline
                Constants & Class[T] & Stored in classes named relevant to the constant. These classes contain constant attributes of whatever type that constant is defined as (e.g. to get the rgb code for red would require COLOURS.RED and would return (255, 0, 0)) \\
            \hline
        \end{tabular}
    \end{center}

    \subsubsection{Algorithms}
    The key algorithms for the checkers game are ones which scan the board for all possible moves (divided into two processes, for simple moves and take moves) and the step algorithm which processes an input action for a given board state.
    Below is pseudocode for the take moves and step functions can be found below.

    \begin{algorithm}
        \caption{Get Take Moves}
        \begin{algorithmic}
            \Function{getValidTakeMoves}{CheckersGame self}
                \State moves $\gets$ []
                \For {row in range(BOARD\_SIZE)}
                    \For {col in range(BOARD\_SIZE)}
                        \State piece $\gets$ self.board[row, col]
                        \If {piece is of self.currentColour}
                            \For {direction in self.piecesLegalDirections(piece)}
                                \If {self.inBounds(row + 2*direction[row]) and self.inBounds(col + 2*direction[col])}
                                    \State takingSquare $\gets$ (row + direction[row], col + direction[col])
                                    \State moveToSquare $\gets$ (row + 2 * direction[row], col + 2 * direction[col])
                                    \If {self.board[moveToSquare] = 0 and self.board[takingSquare] = opponentsPiece}
                                        \State moves.append(((row, col), moveToSquare))
                                    \EndIf
                                \EndIf
                            \EndFor
                        \EndIf
                    \EndFor
                \EndFor
                \State \Return moves
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    The above algorithm is for getting valid take moves. The differences between this algorithm and one that gets
    valid simple moves is that this multiplies all directions by 2 (representing a move over one square and landing on
    the square beyond that). For getting simple moves, these directions will not be multplied. Additionally for getting simple moves,
    the takingSquare variable is not needed - and therefore the check inside the final if statement verifying that the taking square is
    an opponents piece would also not be needed.

    \pagebreak
    \vspace*{17.5mm}
    The step algorithm is key to the console checkers solution. It provides a way for the reinforcement learning model to interact by submitting its actions - 
    having them processed with the next state and the reward for the submitted action returned. This process is exactly as defined in the \myhy{analysis section}{RL Existing Analysis}, a markov decision process.
    In the psuedocode, the return statements have been simplified, but in practice the function would most likely return a tuple containing, at minimum: the next state as a 2D array;
    whether the game is over or not as a bool; the reward for the submitted action in that state ($r, r \in \mathbb{Z} \cap [-1, 1]$)
    \vspace{5mm}

    \begin{algorithm} 
        \caption{Checkers Game Step}
        \begin{algorithmic}
        \Function{Step}{CheckersGame self, ACTION action}
        \If{self.lastPieceMoved is None}
            \State allMoves $\gets$ self.getAllMoves()
            \If{len(allMoves) = 0}
                \State \Return gameOver, loss
            \EndIf
            \If{action not in allMoves}
                \State \Return invalidMove
            \Else
                \State self.movePiece()
            \EndIf
        \ElsIf {self.lastPieceMoved is not None}
            \State allMoves $\gets$ self.getTakeMoves()
            \If {action not in allMoves}
                \State \Return invalidMove
            \Else
                \State self.movePiece()
            \EndIf
        \EndIf

        \If {self.isTakeMove(action)}
            \If {self.doubleMoveAvailable()}
                \State lastMovedPiece $\gets$ action[pieceMoved]
            \EndIf
        \EndIf

        \State self.crownPieces()
        
        \If {self.movesWithoutCapture = 40}
            \State \Return draw
        \ElsIf {self.numberBlackPieces = 1 and self.numberWhitePieces == 1}
            \State \Return draw
        \ElsIf {self.numberOppositePlayerPieces = 0}
            \State \Return win
        \ElsIf {self.movesAvailableForOpponent is False}
            \State \Return win
        \Else
            \State \Return nextState
        \EndIf

        \EndFunction
        \end{algorithmic}
    \end{algorithm}


    \pagebreak
    \subsubsection{User Interface}

    \subsubsubsection{Prototyping} \label{Design-UI-Prototyping}
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.35]{MainMenuPrototype.drawio.png}
        \caption{Prototype Main Menu Design}
    \end{figure}

    The main menu will provide a starting place for all game loops to begin from - see \myhy{2.2.3.2}{2.2.3.2} for a visualization. This should allow users to begin a game loop
    only when the needed parameters for that loop have been selected. For 'User vs User play' therefore this would not require any parameters to be selected. For
    'User vs MCTS' play only the 'EEC' and 'MCTS searches' parameters would require values to be selected, but for 'User vs Agent' play it would require a value to be selected
    for all of the listed parameters on the main menu page. Additionally, it should contain functionality so that it is obvious which parameters have been selected. However, this page does not include all the parameters that are needed for each model.Therefore, this information 
    could be found inside of the 'All Params' button - with a list of each parameter name, and a description of what it dictates.

    \subsubsubsection{Structure, Hierarchy and Flow} \label{2.2.3.2}
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.2]{MainMenuHierachyChart.drawio.png}
        \caption{Hierarchy chart for the User Interface}
    \end{figure}

    When the user interface is first shown, it displays the main menu. As seen in the most recent section, the main menu will allow
    users to select various hyperparameters for both the MCTS and complete agent models and then allow the user to begin playing a game of checkers against
    the desired model. For MCTS, the model will be instantiated inside the program, and a tree build will be run on each turn of the MCTS. For the agent objects,
    its instantiation will be stored inside a respectively named pickle file - see \myhy{File-Structure}{2.1} - and then loaded into the program.
    
    \subsection{Autodifferentiation Engine} \label{AutodifferentiationDesignStage}

    The autodifferentiation engine is based mainly around one class - the 'Tensor' class. This class replicates types such as floats, integers, and matrices while adding functionality
    for automatic differentiation of operations on these types. 

    \subsubsection{Data Structures}

    \begin{center}
        \begin{tabular}{|m{10em} | m{12em} | m{15em}|}
            \hline 
                Item & Data Structure & Reasoning and Usage \\
            \hline
            \hline
                Tensor Data & 2D Array & Can represent floats, ints and matrices - with a scalar just being a 1x1 matrix. \\
            \hline
                Tensor Shape & tuple[int,...] & Represents the shape of the data matrix. Useful for iterating through data. \\
            \hline
                Tensor Children + Parents & list[Tensor] & Allows us to store the tensors used to create and created by the given tensor - which then builds the function graph. (see \myhy{Autodiff-example}{example}) \\
            \hline
                Tensor Function Graph & Graph & Represents a graph of operations upon or between input tensors, intermediary tensors, and a final output of a composition of operations. Allows for
                backward traversal and therefore computation of partial derivatives of the output with respect to its inputs. \\
            \hline
        \end{tabular}
    \end{center}

    \subsubsection{Algorithms}

    The key algorithms for this module include the depth first search backwards through the function graph and various algorithms relating to convolution
    and correlation operations on matrices, and their derivatives. Some smaller processes include the removing of extra dimensions which are a result of numpys broadcasting.
    Psuedocode for the backward traversal through the computational graph has been seen in the analysis section - \myhy{Analysis-IoA}{here}.

    \subsubsubsection{Unbroadcasting}  
    The numpy library broadcasts dimensions to allow for vector multiplication which is more efficient than iteration.
    Using an example where we must remove broadcasted dimensions, say we have a matrix $a$, and its shape is $3 \times 1 \times 4$ and its initially computed
    gradient ($da$) has shape $5 \times 3 \times 2 \times 4$ after any broadcasting. We must first sum out any added dimensions using the algorithm below.

    \begin{algorithm} 
        \caption{Sum Added Dims}
        \begin{algorithmic}
        \Function{SumAddedDims}{Array a, Array da}
            \State numInDims $\gets$ len(a.shape)
            \State numGradDims $\gets$ len(da.shape)

            \For {n in range(numGradDims - numInDims)} \Comment{This would result in 1 loop}
                \State da $\gets$ da.sum() \Comment{This would sum out the first dimension leaving da as $3 \times 1 \times 4$}
            \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \pagebreak
    The next step is to reduce any broadcasted dimensions back to singular dimensions. To do this we can use the below algorithm.

    \begin{algorithm} 
        \caption{Sum Broadcasted Dims}
        \begin{algorithmic}
        \Function{SumBroadcastedDims}{Array a, Array da}
            \For {dim in a.shape} \Comment{a.shape would be equal to (3,1,4)}
                \If {dim == 1}
                    \State da $\gets$ da.sum(axis=dim, keepdims=True)
                \EndIf 
            \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    Note here that we pass in the axis we want to sum across, and we set 'keepdims' equal to true. Since we have removed the added dimensions using the previous
    algorithm, we now want to keep the number of dimensions the same. 'keepdims' does this by making the summed axis reduce to 1. So after both
    of these algorithms $da$ would have the same shape as $a$ - which is exactly what we want!
    
    \subsubsubsection{Cross-Correlation, Convolution and derivatives}
    The valid cross-correlation of 2D matrices has been mentioned and demonstrated in the analysis (\myhy{Analysis-MathematicalFormulae}{Mathematical Formulae}), but in the design I will provide a visual representation of
    this process. To put it into writing again, imagine an input matrix of size $(x \times x)$ and a second 'kernel' of size $(k \times k)$. The output of a cross-correlation between these matrices will have a size
    of $(y \times y), y = x - k + 1$. To compute the output, place the kernel ontop of the input so that the top left corners line up, and sum the products of the overlying elements. This number will givee the result
    for the first element of the output matrix. Slide the kernel over and repeat to process, giving the second element of the output. And repeat until the ouput matrix is full.

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.275]{Cross-Correlation-Algorithm.drawio.png}
        \caption{Valid Cross-Correlation Visualization}
    \end{figure}
    
    The above operation is used in the forward pass of a convolutional layer and its derivative includes a variation of the below version of cross-correlation. 
    As also mentioned in the analysis section, there is another form of cross-correlation called 'full' cross-correlation, and the first few steps of the 
    process for this full cross-correlation can be seen below.

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.275]{Full-Cross_Correlation-Algorithm.drawio.png}
        \caption{Initial Steps of a Full Cross-Correlation Visualization}
    \end{figure}
    
    To then complete what's known as a convolution, you would complete the cross-correlation but rotate the kernel 180 degrees. This works for both version of cross-correlation i.e.
    a full convolution is the same as a full cross-correlation with a rotated kernel and a valid convolution is the same as a valid cross-correlation with a rotated kernel. 

    \subsubsubsection{Tensor Forward and Backward Pass}
    This section will contain a visualization and paired explanation of how a set of operations on a tensor would be completed and then what would happen if we called
    the backward function on the final output tensor. 

    Firstly, say we have tensors $a,b,c$ with values $1,2$ and $3$ respectively. We then define tensors $d,e$ with values $a \times b$ and  $b \times c$ respectively, and finally
    we define our output tensor $y$ with a value of $d \times e$.

    This set of definitions and operations would produce a computational graph of that shown in Figure 22.

    \begin{wrapfigure}[15]{r}{0.52\linewidth}
        \centering
        \includegraphics[scale=0.2]{Tensor-forward-example.drawio.png}
        \caption{Computational graph for a given example}
    \end{wrapfigure}

    If we then called the backward function from the output tensor $y$. It would first set the gradient of $y$ to 1, as the gradient of a variable with respect to itself is 1.
    Next it would call the backward function on the instance of the operation class attached to that tensor. So for $y$ this would call the backward function on the specific instance
    on the multiplication operation attached to $y$. Assuming all of these tensors have the 'requires\_grad' variable set to true, this backward function of this multiplication instance would
    calculate the gradients for tensor $d$ (using processes and formulae defined \myhy{Analysis-MathematicalFormulae}{here} and \myhy{Autodiff-example}{here} respectively) and then call backward on that tensor. After the backward of function of $d$ is then completed our function would return to being inside the 
    backward function of the multiplication instance associated with tensor $y$ (seen by the red arrowhead in the tensor $y$ of the diagram below).

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.2]{Tensor-backward1-example.drawio.png}
        \caption{Part-way through a backward pass of a computational graph for a given example}
    \end{figure}

    Now, the same process occurs that happened for tensor $d$, but for tensor $e$. Leaving the computational graph in its final, and complete, state as follows:

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.2]{Tensor-backward2-example.drawio.png}
        \caption{Final state after a backward pass of a computational graph for a given example}
    \end{figure}

    \pagebreak
    \subsection{Neural Network Library} \label{NNLibDesignStage}

    The neural network library is also based manily around one class, in this case it is the 'Module' Class. 

    \subsubsection{Data Structures}

    The module class is actually an abstract class that provides an interface for different layers and collections of layers.
    It has a property called 'params' which fetches all of the parameters and tensors that compose that module. This is used by optimizers to
    update parameter values. Forward is a basic method which takes an input tensor and then applies any operations defined in the specific
    module, returning the output. Save and load allow the parametes of a module to be saved and loaded using pickle. A module list is similar to a module,
    as it inherits from module, but is also composed of modules. The main difference is inside of the params property as it loops through each
    module it is composed of to fetch its parameters. This is useful for creating a block of similar modules.

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.23]{NNLib-Design-UML.drawio.png}
        \caption{Neural Network Library Modules UML}
    \end{figure}

    Another, much smaller, element of the library is the optimizer. This library only implements Stochastic Gradient Descent
    (SGD) but there are many other optimizing models such as Adam and RMSProp. The abstract optimizer class has methods which update
    the value of each parameter using their gradients, the learning rate and the regularization (the step method). As well as a method to
    set all gradients back to 0 - as gradients accumulate with each tensor backward call, they must be reset for each backpropogation.

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.22]{NN-Optimizers-Design-UML.drawio.png}
        \caption{Neural Network Library Optimizers UML}
    \end{figure}

    \pagebreak
    \subsubsection{Algorithms}
    Three of the four key algorithms inside of the neural network library have been mentioned inside of the data structures section; the 'step',
    'zero_grad' and 'params' functions, and pseudocode for those can be seen below.

    \begin{algorithm} 
        \caption{Optimizer Step Function}
        \begin{algorithmic}
        \Function{Step}{Optimzier self}
            \For {param in self.params}
                \State param.data $\gets$ param.data - self.lr * (param.grad - self.regularization * (param.data ** 2))
            \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    This simple function completes the small steps towards a more optimal parameter value - as explained \myhy{Analysis-GradientDescent}{here} in the analysis and
    is therefore crucial to the library, and the learning process for modules.

    \begin{algorithm} 
        \caption{Optimizer Zero Grad Function}
        \begin{algorithmic}
        \Function{ZeroGrad}{Optimzier self}
            \For {param in self.params}
                \State param.grad $\gets$ 0
            \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    Again, a simple but crucial function. As gradient accumulates, if the gradients are not reset after each training iteration, the values from past iterations
    may skew the step process and therefore inhibit learning.

    \begin{algorithm} 
        \caption{Module Params Property Definition}
        \begin{algorithmic}
        \Function{Params}{Module self}
            \State params $\gets$ []
            \For {param in self.attributes}
                \If{param is Module or ModuleList}
                    \State params += param.params
                \ElsIf{param is Parameter}
                    \State params.append(param)
                \ElsIf{param is Tensor and param.requires_grad}
                    \State params.append(param)
                \EndIf
            \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    The params function gets all values associated with a module or module list that are 'trainable parameters'. This means
    any values which need to get updated during the step process. This property also uses recursion to add parameters from a module
    which has another module inside of itself.


    \begin{wrapfigure}[8]{r}{0.7\linewidth}
        \centering
        \includegraphics[scale=0.2]{ModuleTrainingFlow.drawio.png}
        \caption{Neural Network Library Modules UML}
    \end{wrapfigure}

    The final key algorithm is the general approach to training a module. This was defined in the analysis and psuedocode can be
    seen \myhy{Analysis-IofA-NNLib}{in this section}. 
    
    \vspace{1mm}
    To the right, there is also a visualization of one iteration of
    this learning process.
    

    \pagebreak
    \vspace*{7mm}
    \subsection{Reinforcement Learning Solution} \label{RLSDesignStage}
    The reinforcement learning section is the most complex of all the parts. It uses all the previously defined classes and ones that will be defined in this section,
    to build the learning agent. The final product is a model that can play checkers at a higher level with the same or less time, than a tree search.
    \subsubsection{Data Structures}
    \subsubsubsection{MCTS}
    \begin{center}
        \begin{tabular}{|m{10em} | m{12em} | m{15em}|}
            \hline 
                Item & Data Structure & Reasoning and Usage \\
            \hline
            \hline
                Parent & Node & Represents the parent node - is None if the node is the root. Used for backpropogation. \\
            \hline
                Child Nodes & list[Node] & Represents the children of a given node. Used for searching for optimal moves. \\
            \hline
                Game Tree & Rooted Tree & Using the parent and children from above, a tree is built with each node representing a different state as a result of a given action in a checkers game.\\
            \hline
        \end{tabular}
    \end{center}
    
    \subsubsubsection{Agent}
    \begin{center}
        \begin{tabular}{|m{10em} | m{12em} | m{15em}|}
            \hline 
                Item & Data Structure & Reasoning and Usage \\
            \hline
            \hline
                SAP & Dataclass & Used for storing a state, MCTS probability distribution and the corresponding player. \\
            \hline
                SPV & Dataclass & Similar to SAP but stores the 'true value' (if the game was won or lost) instead of the player. \\
            \hline
                Action Space & Array[int, int, int] & Used to represent the probability distribution of different moves.\\
            \hline
        \end{tabular}
    \end{center}

    \subsubsection{Algorithms}
    \subsubsubsection{MCTS}

    The MCTS is based off the main Node and MCTS class, with an additional AlphaNode and AlphaMCTS classes which build on the
    original Node and MCTS. This section will be split into this base and 'Alpha' section. In both cases, the Node class composes
    to build the MCTS.

    \subsubsubsubsection{Base Node and MCTS}

    The main algorithm for the base node class is the algorithm contained in the 'expand' function. This algorithm selects a random action and creates a child node
    which is then used to compose the tree. This tree can then be used to 'see ahead' and search for the most winning position that is available given a root
    state.

    \pagebreak
    \vspace*{7mm}
    \begin{algorithm} 
        \caption{Node Expansion Function}
        \begin{algorithmic}
        \Function{Expand}{Node self}
            \If {noMovesAvailable and children}
                \State \Return self.selectChild()
            \EndIf
            
            \State randomAction $\gets$ self.GetRandomAction()
            \State self.actions.remove(randomAction)
            
            \State childGame $\gets$ deepcopy(self.game)
            \State validMove, nextObs, terminal, reward $\gets$ childGame.step(randomAction)

            \State child $\gets$ new Node(
                childGame,
                parent,
                terminal,
                randomAction,
                reward
            )

            \State self.children.append(child)

            \Return child
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    The next key algorithm of the Node class is the backpropogation. This updates the value
    and the visit count which store how many times a position lead to a win, and how many times
    a position was visited respectively. These values are then used when calculating UCB and selecting
    a child node.

    \begin{algorithm} 
        \caption{Backpropogation Function}
        \begin{algorithmic}
        \Function{Backprop}{Node self, int reward}
            \State self.visitCount $\gets$ +1
            \State self.valueCount $\gets$ +1

            \If {self.parent not None}
                \If {self.parent.colour != self.colour}
                    \State reward $\gets$ *-1
                \EndIf
                \State self.parent.Backprop(reward)
            \EndIf
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    Now moving onto the two key algorithms inside the MCTS class, we have the tree building algorithm and
    the probability generating algorithm. The tree building algorithm is fairly self-explanatory, it uses the
    Node's expand function to create child nodes, constructing the tree by selecting random actions until a terminal
    state is reached, at which point it completes the backpropogation through that branch.
    
    \begin{algorithm} 
        \caption{Tree Building Function}
        \begin{algorithmic}
        \Function{BuildTree}{MCTS self, CheckersGame root}
            \State self.root $\gets$ Node(root)
            
            \For {search in range(self.kwargs.numSearches)}
                \State node $\gets$ self.root

                \If {node.numAvailableMoves is 0 and node.children}
                    \State node = node.SelectChild()
                \EndIf

                \While {not node.terminal}
                    \State node $\gets$ node.Expand()
                \EndWhile

                \State node.Backprop(node.reward)
            \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \pagebreak
    The action probability generating algorithm takes the tree that is built via the previously mentioned function and
    converts the value and visit counts into a distribution of available actions represented by an 8$\times$8$\times$8
    array - with the first dimension representing the dimensions a piece moved, and the second two dimensions representing
    the row and column the piece was moved from respectively. The position $\left(x,y,z\right)$ which has the highest value
    inside the distribution is the 'most winning' position.

    \begin{algorithm} 
        \caption{Action Probability Generating Function}
        \begin{algorithmic}
        \Function{GetActionProbs}{MCTS self}
            \State p $\gets$ arrayOfZeros(8, 8, 8) \Comment Array of zeros with shape (8 $\times$ 8 $\times$ 8)
            
            \For {child in self.root.children}
                \State movedFrom, movedTo $\gets$ child.actionTaken
                \State direction $\gets$ ConvertToDirection(movedFrom, movedTo)
                \State directionIndex $\gets$ GetDirectionIndex(direction)
                \State p[directionIndex, movedFrom[0], movedFrom[1]] $\gets$ child.visitCount
            \EndFor

            \State p $\gets$ / sum(p) \Comment{Normalization effect}
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \subsubsubsubsection{Alpha Node and MCTS}
    
    The key difference between the Alpha and base classes in this module is the use of a AlphaModel to
    generate a policy and value, which are used in the Node expansion and backpropogation functions. Since
    these values are originally computed inside the modified tree building function, this variation of the
    algorithm will be seen first, then followed by the node expansion and backpropogation functions - as opposed
    to the order used in the base class section.

    \begin{algorithm} 
        \caption{Alpha Tree Building Function}
        \begin{algorithmic}
        \Function{AlphaBuildTree}{AlphaMCTS self, CheckersGame root, Deque priorStates}
            \If {len(priorStates) < 5}
                \State self.BuildTree(root) \Comment{AlphaMCTS inherits from MCTS, so this uses the base build tree function}
                \State \Return
            \EndIf

            \State self.root $\gets$ AlphaNode(root)

            \For{search in range(self.kwargs.numSearches)}

                \State priorStatesCopy $\gets$ deepcopy(priorStates)
                \State node $\gets$ self.root
                \State policy, value $\gets$ None, None

                \While {node.numMovesAvailable is 0 and Children}
                    \State node $\gets$ node.SelectChild()
                    \State priorStatesCopy.Append(node.state)
                \EndWhile

                \If {not node.terminal}
                    \State x $\gets$ self.CreateInput(priorStatesCopy)
                    \State policy, value $\gets$ self.model(x)
                    \State policy $\gets$ * self.ValidMovesAsTensor()
                    \State policy $\gets$ / policy.Sum()
                    \State node.Expand(policy)
                \Else
                    \State value $\gets$ node.reward
                \EndIf
            node.Backprop(value)
            \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \pagebreak
    \vspace*{10mm}
    The backpropogation function for the AlphaNode is identical to the base Node, however
    the value used is sometimes generated by the AlphaModel instead of always being the
    reward value. The main difference between the nodes comes in the 'expand' function which
    can be seen below.

    \begin{algorithm} 
        \caption{AlphaNode Expansion Function}
        \begin{algorithmic}
        \Function{Expand}{AlphaNode self, array policy}
            \State child $\gets$ None
            \For {idx, prob in enumerate(policy)}
                \If {prob $>$ 0}
                    \State childGame $\gets$ deepcopy(self.game)
                    \State action $\gets$ ConvertIdxToAction(idx)
                    \State self.availableMovesLeft.remove(action)
                    \State validMove, nextObs, terminal, reward $\gets$ childGame.step(action)
                    \State child $\gets$ new AlphaNode(
                        childGame,
                        parent,
                        terminal,
                        action,
                        reward,
                        prob
                    )

                    self.children.append(child)
                \EndIf
            \EndFor
            \State \Return child
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \subsubsubsection{Alpha Network}

    The main algorithm inside of the AlphaModel (the model behind the RL agent that produces policies and values) is its forward pass.
    This is slightly more complicated than just passing data from one layer of the network to the next as it involves residual layers, which
    add the original input back onto the outputs of some layers. Additionally, the network produces two tensors (policy and value) which
    therefore requires it to have two heads - one for each of the respective tensor outputs. A diagram of the entire AlphaModel network
    can be seen on the next page.

    In this diagram, the model begins with an input of the past five states. This then enters a $3\times3\times8$ convolutional layer, followed by a
    ReLU activation layer. The last two dimensions of the output if this convolutional layer were left as is, would be $8 - 3 + 1 = 6$. However, for our
    policy output we require an $8\times8\times8$ array (similar to the MCTS distribution). To solve this dimension issue, we add padding to the input.
    Padding the input with a 0 value of size 1 would essentially outline the input array in 0's. Leaving us with a $10\times10\times10$ input and an output
    size of $10 - 3 + 1 = 8$, as required.

    After this first convolutional layer the input enters a set of residual layers, which harness the skip connection mentioned earlier. Where the original
    input to the residual layer is added onto the output halfway through the layer itself. This prevents 'forgetting' of the input.

    Finally, the network splits into the two heads. The policy head produces a softmaxxed probability distribution in an $8\times8\times8$ array, similar to MCTS
    which can be used to determine the probability that any given move will lead to a winning position, and the value head produces a value in the range $[-1, 1]$
    (and therefore uses a tanh activation) which represents if the network thinks the position is currently completely lost (value of -1), completely winning (value of 1),
    or somewhere along this scale.

    \begin{figure}
        \centering
        \includegraphics[scale=0.45]{NetworkPassThrough.drawio.png}
        \caption{A representation of the AlphaModel} 
        \label{fig:28}
    \end{figure}

    \pagebreak
    \subsubsubsection{Agent}

    The AlphaZero agent class has high levels of abstraction, and draws from all the previously defined modules.
    It has one main training algorithm (for training), psuedocode of which can be seen in the original analysis - (\myhy{AgentTrainingAlg}{here}). 

    The only other essential algorithm if the agent is that which converts the SAP dataclass to the SPV - both of which were mentioned
    in the agent section of the data structures part above. Psuedocode for this short algorithm can be seen below.

    \begin{algorithm} 
        \caption{SAP to SPV conversion}
        \begin{algorithmic}
        \Function{ConvertSAPToSPV}{deque[SAP] gameSaps, str player, float reward}
            \State gameSpvs $\gets$ deque(maxlen=len(gameSaps))
            \For {item in gameSaps}
                \If{item.player == player}
                    \State value $\gets$ reward
                \Else
                    \State value $\gets$ reward * -1
                \EndIf
                \State gameSpvs.append(SPV(item.state, item.mctsActionProbs, value))
            \EndFor
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \section{Technical Solution}
    \secttoc

    In the technical analysis I will show the code that implements any data structures and algorithms that I have described
    in the design section and any code that implements features outlined in the objectives.

    \subsection{Checkers}

    \subsubsection{Console Checkers}
    The following sections will contain code from multiple files which holds the key functionality
    of the console checkers implementation, defined in the \myhy{CheckersDesignStage}{design stage}.

    \textbf{Note: }despite these code snippets having titles of the file names, it does not contain all
    the code in the file. This can be found in the \myhy{}{appendix}. 
    \subsubsubsection{CheckersGame and MainMenu}
    \begin{minted}{python}
    from nea.console_checkers.consts import (
        BLACK,
        WHITE,
        SIZE,
        BLACKS,
        WHITES,
        ACTION,
    )

    from nea.console_checkers import jit_functions

    import numpy as np

    class CheckersGame:
        """
        Holds basic logic and console rendering of a checkers game
        """

        def __init__(self) -> None:
            """
            Creates a new CheckersGame object
            """
            self._board = self._init_board() # create numeric representation of board
            self._last_moved_piece: tuple[int, int] = None
            self._player = WHITE
            self._moves_no_capture = 0
            self._switch_player = None

        def _init_board(self) -> np.ndarray:
            """Method which returns intial state of the board

            Returns:
                np.ndarray: initial board state
            """
            return jit_functions._init_board()

        def get_all_valid_moves(self) -> dict[str, list[ACTION]]:
            """Returns a dictionary of take and simple moves.
            Does not account for if a double moves are available.

            Keys:
                Takes moves: "takes"
                Simple moves: "simple"

            Returns:
                dict[str, list[ACTION]]: Dictionary of available moves
            """
            moves = {"takes": [], "simple": []} # dictionary containing take and simple moves
            for row in range(SIZE): # for each row
                for col in range(SIZE): # and each column
                    piece = self._board[row, col]
                    if piece in WHITES and self._player == WHITE:
                        moves["simple"] += self._get_valid_simple_moves(
                            row, col, self._player
                        ) # get simple moves from that square
                        moves["takes"] += self._get_valid_take_moves(row, col, self._player) 
                        # get take moves from that square
                    elif piece in BLACKS and self._player == BLACK:
                        moves["simple"] += self._get_valid_simple_moves(
                            row, col, self._player
                        )
                        moves["takes"] += self._get_valid_take_moves(row, col, self._player)

            return moves

        def _get_valid_take_moves(self, row: int, col: int, player: str):
            """
            Gets all valid take moves available for a given square

            Args:
                row (int): row the square is on
                col (int): column the square is on
                player (str): player to check if moves are available for

            Returns:
                list: tuple of tuples
            """
            # use jit function for faster computation
            return jit_functions._get_valid_take_moves(self._board, row, col, player) 

        def step(self, action: ACTION) -> tuple[bool, np.ndarray, bool, float]:
            """Completes a step given an action in the board environment

            Args:
                action (ACTION): Desired action to take

            Returns:
                tuple[bool, np.ndarray, bool, float]: (valid_move, next_obs, done, reward)
            """
            self._switch_player = True # default switch turn at end of mvoe
            rowcol_move_from, rowcol_move_to = action[0], action[1] # break action into two tuples
            if self._last_moved_piece is None: # if this is not a second take
                all_valid_moves = self.get_all_valid_moves()
                if (
                    len(all_valid_moves["takes"]) == 0
                    and len(all_valid_moves["simple"]) == 0
                ):
                    return (True, self._board, True, -1) # return loss if no moves available

                valid_moves_for_turn = (
                    all_valid_moves["takes"]
                    if len(all_valid_moves["takes"]) > 0
                    else all_valid_moves["simple"]
                ) # get list of all valid moves

                if action not in valid_moves_for_turn:
                    return (False, self._board, False, 0) # return invalid move
                else:
                    self._board[*rowcol_move_to] = self._board[*rowcol_move_from] # update positon
                    self.clear(*rowcol_move_from) 
                    self._moves_no_capture += 1 # add to draw condition

            elif self._last_moved_piece is not None: # if this is a second take move
                valid_moves_for_turn = self._get_valid_take_moves(
                    *self._last_moved_piece, self._player
                ) # get valid actions

                if action not in valid_moves_for_turn:
                    return (False, self._board, False, 0) # return invalid moves
                else:
                    self._board[*rowcol_move_to] = self._board[*rowcol_move_from] # update position
                    self.clear(*rowcol_move_from)

            row_from, col_from = rowcol_move_from
            row_to, col_to = rowcol_move_to
            if abs(row_to - row_from) == 2: # if it was a take move
                one_row = 0.5 * (row_to - row_from) # get the direction horizontally (1 or -1)
                one_col = 0.5 * (col_to - col_from) # get direction vertically (1 or -1)
                self.clear(int(row_from + one_row), int(col_from + one_col)) # remove opponent piece
                self._moves_no_capture = 0 
                self._last_moved_piece = row_to, col_to
                double_moves = self._get_valid_take_moves(
                    *self._last_moved_piece, self._player
                ) # check if there is now a second take available
                if len(double_moves) == 0:
                    self._last_moved_piece = None
                else:
                    self._switch_player = False 

            # crown pieces if they reach opposite edge of board
            if self._board[row_to, col_to] in WHITES and row_to == 0:
                self.crown(row_to, col_to)
            if self._board[row_to, col_to] in BLACKS and row_to == 7:
                self.crown(row_to, col_to) 

            # check win and draw conditions
            if self._moves_no_capture == 40:
                return (True, self._board, True, 0)
            elif self.n_black_pieces == 1 and self.n_white_pieces == 1:
                return (True, self._board, True, 0)
            elif self.n_opposite_player_pieces == 0:
                return (True, self._board, True, 1)
            elif self.no_moves_available_for_opposite_player():
                return (True, self._board, True, 1)
            else:
                if self._switch_player:
                    self._player = self.opposite_player
                return (True, self._board, False, 0)

    
    \end{minted}

    \subsubsubsection{Jit Function}
    \begin{minted}{python}
    from numba import jit
    import numpy as np

    @jit(cache=True)
    def _get_valid_take_moves(
        board: np.ndarray, row: int, col: int, player: str
    ) -> list[ACTION]:
        # have to define constants inside the function for jit to work
        WHITES = [3, 4] 
        BLACKS = [1, 2]

        ALL_LEGAL_DIRS = [(+1, -1), (+1, +1), (-1, +1), (-1, -1)]
        BLACK_R_DIRS = [ALL_LEGAL_DIRS[0], ALL_LEGAL_DIRS[1]]
        WHITE_R_DIRS = [ALL_LEGAL_DIRS[2], ALL_LEGAL_DIRS[3]]

        piece = board[row, col]
        valid_moves = []
        if player == "black":
            if piece == 2: # if the piece is a black king
                for direction in ALL_LEGAL_DIRS:
                    if (
                        row + 2 * direction[0] in range(8)
                        and col + 2 * direction[1] in range(8)
                        and board[row + direction[0], col + direction[1]] in WHITES
                        and board[row + 2 * direction[0], col + 2 * direction[1]] == 0
                    ):
                        # if square to check is in bounds
                        # and if the square beyond the one diagonally adjacent is empty
                        # and the square diagonally adjacent contains an opponent piece
                        valid_moves.append(
                            (
                                (row, col),
                                (row + 2 * direction[0], col + 2 * direction[1]),
                            )
                        )
                        # add this take move to the list of valids
            elif piece == 1: # if piece is black regular
                for direction in BLACK_R_DIRS:
                    if (
                        row + 2 * direction[0] in range(8)
                        and col + 2 * direction[1] in range(8)
                        and board[row + direction[0], col + direction[1]] in WHITES
                        and board[row + 2 * direction[0], col + 2 * direction[1]] == 0
                    ):
                        valid_moves.append(
                            (
                                (row, col),
                                (row + 2 * direction[0], col + 2 * direction[1]),
                            )
                        )
        elif player == "white":
            if piece == 4: # if pieces is white king
                for direction in ALL_LEGAL_DIRS:
                    if (
                        row + 2 * direction[0] in range(8)
                        and col + 2 * direction[1] in range(8)
                        and board[row + direction[0], col + direction[1]] in BLACKS
                        and board[row + 2 * direction[0], col + 2 * direction[1]] == 0
                    ):
                        valid_moves.append(
                            (
                                (row, col),
                                (row + 2 * direction[0], col + 2 * direction[1]),
                            )
                        )
            elif piece == 3: # if piece is white regular
                for direction in WHITE_R_DIRS:
                    if (
                        row + 2 * direction[0] in range(8)
                        and col + 2 * direction[1] in range(8)
                        and board[row + direction[0], col + direction[1]] in BLACKS
                        and board[row + 2 * direction[0], col + 2 * direction[1]] == 0
                    ):
                        valid_moves.append(
                            (
                                (row, col),
                                (row + 2 * direction[0], col + 2 * direction[1]),
                            )
                        )

        return valid_moves
    \end{minted}

    \subsubsubsection{Constants}
    \begin{minted}{python}
    SIZE = 8

    BLACK = "black"
    WHITE = "white"
    
    EMPTY = 0
    BLACK_R = 1
    BLACK_K = 2
    WHITE_R = 3
    WHITE_K = 4
    BLACKS = [1, 2]
    WHITES = [3, 4]

    ACTION = tuple[tuple[int, int], tuple[int, int]]
    \end{minted}    

    \subsubsection{Checkers GUI}
    The next set of sections will now contain code for the data structures and algorithms
    mentioned in the \myhy{CheckersGUIDesignStage}{design stage} of the checkers user interface / GUI add-on.

    \subsubsubsection{CheckersGUI}
    \begin{minted}{python}
    import pygame
    import numpy as np

    from nea.checkers_gui.consts import COLOURS, DISPLAY, DICTS, TEXTS
    from nea.checkers_gui.buttons import Button, RectButton, _change_button_text_colour
    from nea.checkers_gui.helpers import (
        get_col_selected,
        get_row_selected,
    )
    from nea.console_checkers import CheckersGame
    from nea.console_checkers.consts import (
        ACTION,
        BLACK,
        BLACKS,
        EMPTY,
        WHITE,
        WHITES,
    )
    from nea.console_checkers.consts import (
        SIZE as BOARD_SIZE,
    )


    class CheckersGUI(CheckersGame):
        """Class that adds extra functionality to the CheckersGame
        which allows it to display a GUI
        """

        def __init__(self) -> None:
            self.piece_selected: tuple[int, int] = None
            super().__init__()

        def draw(self, screen: pygame.Surface) -> None:
            """Draws the checkers board and pieces.

            If a piece is selected it scales that piece to be slightly smaller.
            Additionally, available moves for a selected piece are displayed.

            Args:
                screen (pygame.Surface): screen to draw to
            """
            # draw the background
            for row in range(8):
                if row % 2 == 0:
                    for x in range(0, DISPLAY.SCREEN_SIZE, DISPLAY.SQUARE_SIZE * 2):
                        pygame.draw.rect(
                            screen,
                            COLOURS.WHITE,
                            pygame.Rect(
                                x,
                                row * DISPLAY.SQUARE_SIZE,
                                DISPLAY.SQUARE_SIZE,
                                DISPLAY.SQUARE_SIZE,
                            ),
                        )
                else:
                    for x in range(
                        DISPLAY.SQUARE_SIZE, DISPLAY.SCREEN_SIZE, DISPLAY.SQUARE_SIZE * 2
                    ):
                        pygame.draw.rect(
                            screen,
                            COLOURS.WHITE,
                            pygame.Rect(
                                x,
                                row * DISPLAY.SQUARE_SIZE,
                                DISPLAY.SQUARE_SIZE,
                                DISPLAY.SQUARE_SIZE,
                            ),
                        )
            # draw the pieces
            for y in range(BOARD_SIZE):
                for x in range(BOARD_SIZE):
                    piece = self.board[x, y]
                    if (x, y) == self.piece_selected:
                        self._draw_selected_piece(screen)
                        self._draw_available_moves(screen)
                    elif piece != EMPTY:
                        self._get_and_draw_piece(screen, piece=piece, x=x, y=y)

            pygame.display.set_caption(f"IT IS {self.player.upper()}'S MOVE")

        def click(self, mouse_pos: tuple[int, int]) -> ACTION:
        """Evaulates a users click and selects/deselects a piece if needed

        Args:
            mouse_pos (tuple[int, int]): mouse position when click occurs

        Returns:
            ACTION: action user wants to take - if None then a piece has been selected to move
                                              - if not None then a move of the selected piece has been made
        """
        mouse_x, mouse_y = mouse_pos 
        # convert mouse click to row and column click
        row, col = get_row_selected(mouse_y=mouse_y), get_col_selected(mouse_x=mouse_x)
        action: ACTION = None

        if self.piece_selected is None:
            valid_moves = self.get_all_valid_moves()
            valid_moves = (
                valid_moves["takes"]
                if len(valid_moves["takes"]) > 0
                else valid_moves["simple"]
            ) # get the set of valid moves 
            valid_selections = [x[0] for x in valid_moves]
            if (row, col) in valid_selections: # check piece selected is a valid piece to move
                # check user has selected their own piece
                if self.player == WHITE:
                    if self.board[row, col] in WHITES:
                        self.piece_selected = (row, col)
                elif self.player == BLACK:
                    if self.board[row, col] in BLACKS:
                        self.piece_selected = (row, col)
        else:
            if (row, col) != self.piece_selected:
                action = (self.piece_selected, (row, col))
            self.piece_selected = None

        return action


    class MainMenu:
        def __init__(self) -> None:
            pygame.init()

            self.screen = pygame.display.set_mode(
                (DISPLAY.SCREEN_SIZE, DISPLAY.SCREEN_SIZE)
            )
            self.font = lambda font_size: pygame.font.SysFont(
                pygame.font.get_default_font(), font_size
            ) # set default font

        def display(self) -> None:
            buttons: dict[Button] = {}
            self.screen.fill(COLOURS.BLACK)

            # write static text
            self._display_welcome_text()
            self._display_select_params_text()

            # define parameters that user can adjust
            params = {
                "(UvsM, UvsA) MCTS Searches": None,
                "(UvsM, UvsA)                     EEC": None,
                "(UvsA)         Examples Games": None,
                "(UvsA)     Comparison Games": None,
                "(UvsA)   % Replace Threshold": None,
            }
            self._display_param_texts(params)

            # get and display buttons
            buttons["Tutorial"] = self._display_tutorial_button()
            buttons["All Params"] = self._display_all_params_button()
            # game start buttons
            buttons["User vs User"], buttons["User vs MCTS"], buttons["User vs Agent"] = (
                self._display_submit_buttons()
            )
            # store buttons in a dic where key defines their parameter
            buttons["50ns"], buttons["100ns"], buttons["500ns"] = (
                self._display_mcts_searches_buttons()
            )
            buttons["0.75ec"], buttons["1.41ec"], buttons["2ec"] = (
                self._display_eec_buttons()
            )
            buttons["100te"], buttons["500te"], buttons["1000te"] = (
                self._display_example_games_buttons()
            )
            buttons["5cg"], buttons["10cg"] = self._display_comparison_games_buttons()
            buttons["50rt"], buttons["60rt"] = self._display_replace_threshold_buttons()

            # event loop
            open = True
            while open:
                for e in pygame.event.get():
                    if e.type == pygame.QUIT:
                        open = False
                        quit()
                    if e.type == pygame.MOUSEBUTTONDOWN:
                        mouse_pos = pygame.mouse.get_pos()
                        params = self._manage_click(mouse_pos, buttons, params)

                pygame.display.flip()

        def _manage_click(
            self,
            mouse_pos: tuple[int, int],
            buttons: dict[str, Button],
            params: dict,
        ) -> dict:
            for key, button in buttons.items():
                if button.in_bounds(mouse_pos):
                    if key in ["Tutorial", "All Params", "User vs User"]:
                        button.click_fn()
                    elif key == "User vs MCTS":
                        p = np.random.rand()
                        if (
                            not params["(UvsM, UvsA) MCTS Searches"]
                            or not params["(UvsM, UvsA)                     EEC"]
                        ): # check the needed params have been set
                            continue
                        button.click_fn(
                            n_searches=params["(UvsM, UvsA) MCTS Searches"],
                            eec=params["(UvsM, UvsA)                     EEC"],
                            player_colour=WHITE if p > 0.5 else BLACK,
                        ) # run the game
                    elif key == "User vs Agent":
                        p = np.random.rand()
                        if any(params.values()) is None: # check needed params have been set
                            continue
                        button.click_fn(
                            n_searches=params["(UvsM, UvsA) MCTS Searches"],
                            eec=params["(UvsM, UvsA)                     EEC"],
                            example_games=params["(UvsA)       Examples Games"],
                            comparison_games=params["(UvsA)     Comparison Games"],
                            replacement_threshold=params["(UvsA)   % Replace Threshold"],
                            player_colour=WHITE if p > 0.5 else BLACK,
                        )  # run the game
                    else:
                        # split dictionary key into parameter and value
                        parameter = key[-2:]
                        value = key[:-2]

                        params[DICTS.param_placeholders[parameter]] = (
                            int(value) if parameter != "ec" else float(value)
                        ) # set parameter to selected value

                        for k in buttons.keys():
                            if k[-2:] == parameter:
                                buttons[k].set_text_black(self.screen)

                        button.click_fn(
                            button, self.screen
                        )  # make it obvious button has been selected (switch text to red)

            return params
    \end{minted}

    \subsubsubsection{Helper Functions}
    \begin{minted}{python}
        from nea.checkers_gui.consts import DISPLAY

        def get_row_selected(mouse_y: int) -> int:
            return int(mouse_y / DISPLAY.SQUARE_SIZE)

        def get_col_selected(mouse_x: int) -> int:
            return int(mouse_x / DISPLAY.SQUARE_SIZE)
    \end{minted}

    \subsubsubsection{Constants}
    \begin{minted}{python}
    class DISPLAY:
        SCREEN_SIZE = 720
        SQUARE_SIZE = 90
        CIRCLE_RADIUS = 20


    class COLOURS:
        BLACK = (0, 0, 0)
        WHITE = (255, 255, 255)
        GREEN = (0, 255, 0)
        RED = (255, 0, 0)
        BLUE = (0, 0, 255)
        YELLOW = (255, 255, 0)
        BROWN = (131, 106, 76)
        BONE = (251, 220, 191)


    class DICTS:
        param_placeholders: dict[str, str] = {
            "ns": "(UvsM, UvsA) MCTS Searches",
            "ec": "(UvsM, UvsA)                     EEC",
            "te": "(UvsA)       Examples Games",
            "cg": "(UvsA)     Comparison Games",
            "rt": "(UvsA)   % Replace Threshold",
        }
        param_placeholder_values: dict[str, list[int | float]] = {
            "ns": [50, 100, 500],
            "ec": [0.75, 1.41, 2],
            "te": [100, 500, 1000],
            "cg": [5, 10],
            "rt": [50, 60],
        }


    class TEXTS:
        tutorial = [
            "1. Pieces only move diagonally.",
            "... rest can be seen in appendix",
        ]
        changeable_params = [
            "MCTS Searches: The number of different game endings the tree search sees.",
            "... rest ca be seen in appendix",
        ]
        defaulted_params = [
            "MCTS Epochs (defaulted to 3): The number of iterations where the network ",
            "... rest can be seen in appendix",
        ]
    \end{minted}

    \subsubsubsection{Buttons}
    \begin{minted}{python}
    import pygame
    from abc import ABC

    from nea.checkers_gui.consts import COLOURS


    def _change_button_text_colour(button: RectButton, screen: pygame.Surface):
        button.text_colour = (
            COLOURS.RED if button.text_colour == COLOURS.BLACK else COLOURS.BLACK
        )
        button.draw(screen)


    class Button(ABC):
        def draw(self, screen: pygame.Surface) -> None: ...

        def in_bounds(self, mouse_pos: tuple[int, int]) -> bool: ...

        def click_fn(self, *args): ...


    class RectButton(Button):
        def __init__(
            self,
            width: int,
            height: int,
            pos: tuple[int, int],
            click_func: callable,
            colour: COLOURS = COLOURS.WHITE,
            text: str = "",
            font_size: int = None,
            text_colour: COLOURS = COLOURS.BLACK,
        ) -> None:
            self.width = width
            self.height = height
            self.x, self.y = pos
            self.f = click_func
            self.colour = colour
            self.text = text
            if self.text != "":
                assert font_size, "Must have font size if text is to be shown."
                self.text_colour = text_colour
                self.font = pygame.font.SysFont(pygame.font.get_default_font(), font_size)

        def draw(self, screen: pygame.Surface) -> None:
            pygame.draw.rect(
                screen,
                self.colour,
                pygame.Rect(
                    self.x,
                    self.y,
                    self.width,
                    self.height,
                ),
            )

            if self.text != "":
                font = self.font
                text = font.render(self.text, 1, self.text_colour)
                # draw the text in the middle of the button
                screen.blit(
                    text,
                    (
                        self.x + (self.width / 2 - text.get_width() / 2),
                        self.y + (self.height / 2 - text.get_height() / 2),
                    ),
                )

        def in_bounds(self, mouse_pos: tuple[int, int]) -> bool:
            if mouse_pos[0] > self.x and mouse_pos[0] < self.x + self.width:
                if mouse_pos[1] > self.y and mouse_pos[1] < self.y + self.height:
                    return True

            return False

        def click_fn(self, *args, **kwargs):
            if self.f:
                self.f(*args, **kwargs)
    \end{minted}

    \subsection{Autodifferentiation engine}
    Please see \myhy{AutodifferentiationDesignStage}{here} for the design stage related to this following
    area. Additionally, I will include code which helps to achieve the \myhy{AutodifferentiationObjectives}{autodifferentiation engine objectives}

    \subsubsection{Constants}
    \begin{minted}{python}
    from typing import Union
    from numpy import ndarray

    Tensorable = Union[ndarray, list, tuple, float, int]
    \end{minted}

    \subsubsection{Tensor and Tensor Operations}
    \begin{minted}{python}
    from __future__ import annotations
    from abc import ABC
    import contextlib

    import numpy as np

    from nea.ml.autograd.consts import Tensorable
    from nea.ml.autograd.convolve_funcs import (
        cpu_forward_convolve2d,
        cpu_k_backward_convolve2d,
        cpu_x_backward_convolve2d,
        cpu_x_and_k_backward_convolve2d,
    )
    from nea.ml.autograd.jit_functions import fill_padded_array

    # ========
    #  TENSOR
    # ========

    _grad_is_enabled = True


    def is_grad_enabled() -> bool:
        """Returns if gradient calculations are enabled

        Returns:
            bool:
        """
        return _grad_is_enabled


    @contextlib.contextmanager
    def no_grad():
        """context manager to disable gradient calculations"""
        global _grad_is_enabled
        prev_state = _grad_is_enabled
        _grad_is_enabled = False  # Disable gradients
        try:
            yield
        finally:
            _grad_is_enabled = prev_state  # return to previous


    class Tensor:
        """
        ====================
            Tensor class
        ====================
        """

        def __init__(
            self,
            data: Tensorable,
            requires_grad: bool = False,
            operation: TensorFunction = None,
        ) -> None:
            self._data: np.ndarray = np.array(data)
            self.shape = self._data.shape
            self.operation: TensorFunction = operation

            self.requires_grad: bool = requires_grad

            if self.requires_grad:
                self.grad = np.zeros_like(self._data)

            self.children = []

        def __repr__(self) -> str:
            return f"Tensor({self._data}, shape = {self.shape})"

        def backward(self, dy=None, y=None) -> None:
            """Reverse searches the computational graph, computing and updating parent
            gradients as it goes

            Args:
                dy (Tensor, optional): _description_. Defaults to None.
                y (Tensor, optional): _description_. Defaults to None.

            Returns:
                None
            """
            if self.requires_grad is False:
                raise ValueError("Tensor has requires grad set to false")
            if not is_grad_enabled():
                raise ValueError("no_grad is enabled")

            if dy is None: # if no upstream gradient
                dy = np.ones_like(self._data) # derivative of itself is 1

            if y is not None: 
                self.children.remove(y) # remove from graph to stop duplicate calculation

            self.grad = self.grad + dy # accumulate grad

            if self.operation:
                if not self.children: # continue traversing up graph
                    self.operation.backward(self.grad, self)

        def zero_grad(self) -> None:
            """
            Zeros out the gradient of the tensor
            """
            self.grad = np.zeros_like(self._data)

        def __eq__(self, other: np.ndarray | int | float):
            if np.array_equal(self.data, np.array(other)):
                return True

        def __add__(self, other: Tensorable) -> Tensor:
            add_op = Addition()
            return add_op.forward(self, to_tensor(other))

        def __radd__(self, other: Tensorable) -> Tensor:
            add_op = Addition()
            return add_op.forward(self, to_tensor(other))

        def __iadd__(self, other: Tensorable) -> Tensor:
            add_op = Addition()
            return add_op.forward(self, to_tensor(other))

        def __neg__(self):
            neg_op = Negation()
            return neg_op.forward(self)

        def __sub__(self, other: Tensorable) -> Tensor:
            return self + -to_tensor(other)

        def __rsub__(self, other: Tensorable) -> Tensor:
            return to_tensor(other) + -self

        def __isub__(self, other: Tensorable) -> Tensor:
            return self + -to_tensor(other)

        def __mul__(self, other: Tensorable) -> Tensor:
            mul_op = Multiplication()
            return mul_op.forward(self, to_tensor(other))

        def __rmul__(self, other: Tensorable) -> Tensor:
            mul_op = Multiplication()
            return mul_op.forward(to_tensor(other), self)

        def __imul__(self, other: Tensorable) -> Tensor:
            mul_op = Multiplication()
            return mul_op.forward(self, to_tensor(other))

        def __truediv__(self, other: Tensorable) -> Tensor:
            div_op = Division()
            return div_op.forward(self, to_tensor(other))

        def __rtruediv__(self, other: Tensorable) -> Tensor:
            div_op = Division()
            return div_op.forward(to_tensor(other), self)

        def __matmul__(self, other: Tensorable) -> Tensor:
            matmul_op = MatrixMultiplication()
            return matmul_op.forward(self, to_tensor(other))

        def __pow__(self, other: Tensorable) -> Tensor:
            pow_op = Power()
            return pow_op.forward(self, to_tensor(other))

        def T(self) -> Tensor:
            """Returns a copy of the tensor, however the data has been transposed

            Returns:
                Tensor:
            """
            transpose_op = Transpose()
            return transpose_op.forward(self)

        def mean(self) -> Tensor:
            """Computes the mean of the tensor

            Returns:
                Tensor: _description_
            """
            mean_op = Mean()
            return mean_op.forward(self)

        def sum(self, dim: int = -1, keepdims: bool = False) -> Tensor:
            """Computes sum of a tensor

            Args:
                axis (int, optional): axis to sum across. Defaults to -1.
                keepdims (bool, optional): reduce summed axis to 1?. Defaults to False.

            Returns:
                Tensor:
            """
            sum_op = Sum()
            return sum_op.forward(self, dim=dim, keepdims=keepdims)

        def log(self) -> Tensor:
            """Computes element wise log of tensor

            Returns:
                Tensor: _description_
            """
            log_op = Log()
            return log_op.forward(self)

        def exp(self) -> Tensor:
            """e^

            Returns:
                Tensor:
            """
            exp_op = Exp()
            return exp_op.forward(self)

        def convolve2d(self, k: Tensorable, b: Tensorable = None) -> Tensor:
            """2D convolutional layer of the tensor

            Args:
                k (Tensor): kernel to use
                b (Tensor, optional): bias to use. Defaults to None.

            Returns:
                Tensor:
            """
            conv_op = Convolve2D()
            return conv_op.forward(self, k, b=b)

        def reshape(self, shape: tuple[int, int]) -> Tensor:
            """Reshapes a tensor

            Args:
                shape (tuple[int, int]): shape to change to

            Returns:
                Tensor:
            """
            reshape_op = Reshape()
            return reshape_op.forward(self, shape=shape)

        def pad2D(self, padding: int, value: float) -> Tensor:
            """Pads a 2D tensor

            Args:
                padding (int): how much to add to each edge
                value (float): value to pad with

            Returns:
                Tensor: padded tensor
            """
            pad_op = Pad2D()
            return pad_op.forward(self, padding=padding, value=value)

        def relu(self) -> Tensor:
            """Acts as function for relu activation layer

            Returns:
                Tensor:
            """
            relu_op = ReLU()
            return relu_op.forward(self)


    # ==================
    ## TENSOR FUNCTIONS
    # ==================


    class TensorFunction(ABC):
        """Abstract class of a TensorFunction, represents the operation performed

        Args:
            ABC (_type_): Abstrract Class
        """

        parents: tuple[Tensor] = None
        _cache: tuple[Tensor] = None


    class Addition(TensorFunction):
        """Operation to add two tensors

        Args:
            TensorFunction (_type_): _description_
        """

        def forward(self, a: Tensor, b: Tensor) -> Tensor:
            """Computes the addition of two tensors

            Args:
                a (Tensor): one tensor to add
                b (Tensor): other tensor to add

            Returns:
                Tensor: tensor where data is addition of parents
            """
            new_data = a.data + b.data

            requires_grad = a.requires_grad or b.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a, b)

                a.children.append(y)
                b.children.append(y)

                self._cache = (a, b)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes gradients for tensors stored in cache

            Args:
                dy (np.ndarray): gradient from previous backward (to be used with chain rule)
                y (Tensor): output tensor
            """
            a, b = self._cache  # get tensors used to create output

            if a.requires_grad:
                da = dy  # 1 * whatever the previous gradient is due to chain rule

                # now need to sum out broadcasted dimensions from numpy
                # make da the same shape as a

                # To remove broadcast dims first remove added dimensions
                # **EXAMPLE**

                # 1. Input and Gradient Shapes:
                # - Suppose a has shape (3, 1, 4).
                # - The gradient da has shape (5, 3, 1, 4) after operation and broadcasting.

                # 2. Adjustment Process:
                # in_dim = len(b.shape)  # 3
                # grad_dim = len(db.shape)  # 4

                # for _ in range(grad_dim - in_dim):  # 4 - 3 = 1 time
                #     db = db.sum(axis=0)

                # 3. Result:
                # - After the loop, da would have shape (3, 1, 4), matching b's shape.

                n_dims_da = len(dy.shape)
                n_dims_a = len(a.shape)
                for dim in range(n_dims_da - n_dims_a):
                    da = da.sum(axis=0)

                # Then remove singular dimensions (indicates broadcasting)
                # Summing over singular dimensions:

                # a.shape = (3, 1, 4)

                # for i, dim in enumerate(a.shape):
                #     if dim == 1:
                #         da = da.sum(axis=i, keepdims=True)

                # - This loop only executes for i=1 because dim == 1 at that position.
                # - da = da.sum(axis=1, keepdims=True)

                # Since da is already of shape (3, 1, 4), summing along `axis=1` with `keepdims=True`
                # doesn't change its shape but ensures the gradient correctly aligns with the
                # broadcast structure.

                # This is because the keepdims is a boolean parameter.
                # If this is set to True, the axes which are reduced are
                # left in the result as dimensions with size one.

                # Therefore if da had shape (3, 2, 4) for example -
                # this loop would reduce it to (3, 1 ,4)

                for i, dim in enumerate(a.shape):
                    if dim == 1:
                        da = da.sum(axis=i, keepdims=True)
                a.backward(da, y)

            if b.requires_grad:
                db = dy

                # Rescale gradient to have the same shape as "b":
                n_dims_db = len(db.shape)
                n_dims_b = len(b.shape)
                for dim in range(n_dims_db - n_dims_b):
                    db = db.sum(axis=0)

                for i, dim in enumerate(b.shape):
                    if dim == 1:
                        db = db.sum(axis=i, keepdims=True)
                b.backward(db, y)


    class Negation(TensorFunction):
        """Operation to negate a tensor

        Args:
            TensorFunction (_type_): _description_
        """

        def forward(self, a: Tensor) -> Tensor:
            """Computes the negation of a tensor

            Args:
                a (Tensor): tensor to be negated

            Returns:
                Tensor: negated tensor
            """
            new_data = -a.data

            y = Tensor(new_data, requires_grad=a.requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)
                a.children.append(y)

                self._cache = (a,)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes the gradient for tensors in cache

            Args:
                dy (np.ndarray): gradient from upstream
                y (Tensor): output tensor
            """
            (a,) = self._cache

            if a.requires_grad:
                da = -dy
                a.backward(da, y)


    class Multiplication(TensorFunction):
        """Operation to multiply two tensors

        Args:
            TensorFunction (_type_): _description_
        """

        def forward(self, a: Tensor, b: Tensor) -> Tensor:
            """Computes the multiplication of two tensors

            Args:
                a (Tensor):
                b (Tensor):

            Returns:
                Tensor: product of a and b
            """
            new_data = a.data * b.data

            requires_grad = a.requires_grad or b.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self._cache = (a, b)

                self.parents = (a, b)
                a.children.append(y)
                b.children.append(y)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes gradients for cached tensors

            Args:
                dy (Tensor): output grad
                y (Tensor):
            """

            a, b = self._cache

            if a.requires_grad:
                da = dy * b.data

                n_dims_da = len(dy.shape)
                n_dims_a = len(a.shape)
                for dim in range(n_dims_da - n_dims_a):
                    da = da.sum(axis=0)
                for i, dim in enumerate(a.shape):
                    if dim == 1:
                        da = da.sum(axis=i, keepdims=True)

                a.backward(da, y)

            if b.requires_grad:
                db = dy * a.data

                n_dims_db = len(dy.shape)
                n_dims_b = len(b.shape)
                for dim in range(n_dims_db - n_dims_b):
                    db = db.sum(axis=0)
                for i, dim in enumerate(b.shape):
                    if dim == 1:
                        db = db.sum(axis=i, keepdims=True)

                b.backward(db, y)


    class Division(TensorFunction):
        """Division operation

        Args:
            TensorFunction (_type_): _description_
        """

        def forward(self, a: Tensor, b: Tensor) -> Tensor:
            """Computes the division of two tensors

            Args:
                a (Tensor):
                b (Tensor):

            Returns:
                Tensor: tensor a / tensor b
            """
            new_data = a.data / b.data

            requires_grad = a.requires_grad or b.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a, b)
                a.children.append(y)
                b.children.append(y)

                self._cache = (a, b)

            return y

        def backward(self, dy: np.ndarray, y: Tensor):
            """Computes gradients of cached tensors

            Args:
                dy (Tensor):
                y (Tensor):
            """
            a, b = self._cache

            if a.requires_grad:
                da = dy * (1 / b.data)

                n_dims_da = len(dy.shape)
                n_dims_a = len(a.shape)
                for dim in range(n_dims_da - n_dims_a):
                    da = da.sum(axis=0)
                for i, dim in enumerate(a.shape):
                    if dim == 1:
                        da = da.sum(axis=i, keepdims=True)

                a.backward(da, y)

            if b.requires_grad:
                db = dy * -(a.data / (b.data**2))

                n_dims_db = len(dy.shape)
                n_dims_b = len(b.shape)
                for dim in range(n_dims_db - n_dims_b):
                    db = db.sum(axis=0)
                for i, dim in enumerate(b.shape):
                    if dim == 1:
                        db = db.sum(axis=i, keepdims=True)

                b.backward(db, y)


    class MatrixMultiplication(TensorFunction):
        """Matrix Multiplication operation

        Args:
            TensorFunction (_type_):
        """

        def forward(self, a: Tensor, b: Tensor) -> Tensor:
            """Computes the matrix multiplication of two tensors

            Args:
                a (Tensor):
                b (Tensor):

            Returns:
                Tensor: Tensor of a @ b
            """
            new_data = a.data @ b.data

            requires_grad = a.requires_grad or b.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a, b)

                a.children.append(y)
                b.children.append(y)

                self._cache = (a, b)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes gradients for cached tensors

            Args:
                dy (Tensor):
                y (Tensor):
            """
            a, b = self._cache

            if a.requires_grad:
                b_T = b.data.swapaxes(-1, -2)
                da = dy @ b_T

                n_dims_da = len(dy.shape)
                n_dims_a = len(a.shape)
                for _ in range(n_dims_da - n_dims_a):
                    da = da.sum(axis=0)

                a.backward(da, y)

            if b.requires_grad:
                a_T = a..data.swapaxes(-1, -2)
                db = a_T @ dy

                n_dims_db = len(dy.shape)
                n_dims_b = len(b.shape)
                for _ in range(n_dims_db - n_dims_b):
                    db = db.sum(axis=0)

                b.backward(db, y)


    class Power(TensorFunction):
        """Power function e.g. a^b

        Args:
            TensorFunction (_type_): _description_
        """

        def forward(self, a: Tensor, b: Tensor) -> Tensor:
            """Computes one tensor to the power of another

            Args:
                a (Tensor):
                b (Tensor):

            Returns:
                Tensor: Tensor with a^b
            """
            new_data = a.data**b.data

            requires_grad = a.requires_grad or b.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a, b)
                a.children.append(y)
                b.children.append(y)

                self._cache = (a, b)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes gradients of cached tensors

            Args:
                dy (Tensor): _description_
                y (Tensor): _description_
            """
            a, b = self._cache

            if a.requires_grad:
                da = dy * (b.data * (a.data ** (b.data - 1)))

                n_dims_da = len(dy.shape)
                n_dims_a = len(a.shape)
                for dim in range(n_dims_da - n_dims_a):
                    da = da.sum(axis=0)
                for i, dim in enumerate(a.shape):
                    if dim == 1:
                        da = da.sum(axis=i, keepdims=True)

                a.backward(da, y)

            if b.requires_grad:
                db = dy * ((a.data**b.data) * np.log(a.data))

                n_dims_db = len(dy.shape)
                n_dims_b = len(b.shape)
                for dim in range(n_dims_db - n_dims_b):
                    db = db.sum(axis=0)
                for i, dim in enumerate(b.shape):
                    if dim == 1:
                        db = db.sum(axis=i, keepdims=True)

                b.backward(dy, y)


    class Mean(TensorFunction):
        """Mean operation

        Args:
            TensorFunction (_type_):
        """

        def forward(self, a: Tensor) -> Tensor:
            """Computes the mean of a 1D tensor

            Args:
                a (Tensor):

            Returnws:
                Tensor: mean of a
            """
            new_data = a.data.mean()

            requires_grad = a.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)

                a.children.append(y)

                self._cache = (a,)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes gradients of cached tensors

            Args:
                dy (Tensor):
                y (Tensor):
            """
            (a,) = self._cache

            if a.requires_grad:
                da = dy * np.ones(len(a.data))
                da /= len(a.data)

                a.backward(da, y)


    class Sum(TensorFunction):
        """Sum of a tensor

        Args:
            TensorFunction (_type_): _description_
        """

        def forward(self, a: Tensor, dim: int, keepdims: bool) -> Tensor:
            """Computes sum of a tensor

            Args:
                a (Tensor):
                axis (int): axis to take sum across
                keepdims (bool): keepdims maintains the number of dimensions 
                - reduces summed axis to 1

            Returns:
                Tensor: _description_
            """
            new_data = a.data.sum(axis=dim, keepdims=keepdims)

            requires_grad = a.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)

                a.children.append(y)

                self._cache = (a,)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes gradients of sum function

            Args:
                dy (np.ndarray): gradient from upstream
                y (Tensor):
            """
            (a,) = self._cache

            if a.requires_grad:
                da = dy * np.ones(a.shape)
                a.backward(da, y)


    class Log(TensorFunction):
        """Log operation

        Args:
            TensorFunction (_type_): _description_
        """

        def forward(self, a: Tensor) -> Tensor:
            """Element wise log of a tensor

            Args:
                a (Tensor):

            Returns:
                Tensor:
            """

            new_data = np.log(a.data)

            requires_grad = a.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)

                a.children.append(y)

                self._cache = (a,)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes gradient of cached tensor

            Args:
                dy (np.ndarray): gradient from upstream
                y (Tensor):
            """
            (a,) = self._cache
            if a.requires_grad:
                da = dy * (1 / a.data)

                a.backward(da, y)


    class Exp(TensorFunction):
        """e^

        Args:
            TensorFunction (_type_):
        """

        def forward(self, a: Tensor) -> Tensor:
            """e^a

            Args:
                a (Tensor):

            Returns:
                Tensor:
            """
            new_data = np.exp(a.data)

            requires_grad = a.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)

                a.children.append(y)

                self._cache = (a, new_data)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes grads

            Args:
                dy (np.ndarray):
                y (Tensor):
            """
            a, new_data = self._cache

            if a.requires_grad:
                da = dy * new_data
                a.backward(da, y)


    class Transpose(TensorFunction):
        def forward(self, a: Tensor) -> Tensor:
            """Transposes the data

            Args:
                x (Tensor):

            Returns:
                Tensor:
            """
            new_data = a.data.T

            requires_grad = a.requires_grad

            y = Tensor(new_data, requires_grad=requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)

                a.children.append(y)

                self._cache = (a,)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            (a,) = self._cache

            if a.requires_grad:
                da = dy.T

                a.backward(da, y)


    class Convolve2D(TensorFunction):
        """2D convolution layer as tensor function

        Args:
            TensorFunction (_type_):
        """

        def forward(self, x: Tensor, k: Tensor, b: Tensor = None) -> Tensor:
            """2D Convolution layer of X as input

            Args:
                x (Tensor): Input to layer
            """
            self.n_kernels, self.x_samples, self.kernel_size, _ = k.shape
            x_samples, x_width, x_height = x.shape

            self.output_shape = (
                self.n_kernels,
                x_width - self.kernel_size + 1,
                x_height - self.kernel_size + 1,
            )

            new_data = np.zeros(self.output_shape)
            if b:
                new_data += b.data

            new_data = cpu_forward_convolve2d(new_data, x.data, k.data, self.n_kernels)

            y = Tensor(new_data, requires_grad=True, operation=self)

            if is_grad_enabled():
                self.parents = [
                    x,
                    k,
                ]
                if b:
                    self.parents.append(b)

                x.children.append(y)
                k.children.append(y)
                if b:
                    b.children.append(y)

                self._cache = (x, k, b)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            """Computes gradients of a convolutional layer process

            Args:
                dy (np.ndarray): upstream grad
                y (Tensor):
            """
            x, k, b = self._cache

            if b:
                if b.requires_grad:
                    b.backward(dy, y)
            if x.requires_grad and k.requires_grad:
                dx = np.zeros(x.shape)
                dk = np.zeros(k.shape)
                dx, dk = cpu_x_and_k_backward_convolve2d(
                    dx, dk, x.data, k.data, dy, x.shape[0], self.n_kernels
                )

                x.backward(dx, y)
                k.backward(dk, y)
            else:
                if x.requires_grad:
                    dx = np.zeros(x.shape)
                    dx = cpu_x_backward_convolve2d(
                        dx, k.data, dy, x.shape[0], self.n_kernels
                    )

                    x.backward(dx, y)

                if k.requires_grad:
                    dk = np.zeros(k.shape)
                    dk = cpu_k_backward_convolve2d(dk, x.data, dy, self.n_kernels)

                    k.backward(dk, y)


    class Reshape(TensorFunction):
        """Reshapes a tensor

        Args:
            TensorFunction :
        """

        def forward(self, a: Tensor, shape: tuple[int]) -> Tensor:
            """Reshapes the tensor

            Args:
                x (Tensor): tensor to reshape

            Returns:
                Tensor: reshaped tensor
            """
            new_data = np.reshape(a.data, shape)

            y = Tensor(new_data, requires_grad=a.requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)

                a.children.append(y)

                self._cache = (a,)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            (a,) = self._cache

            if a.requires_grad:
                da = np.reshape(dy, a.shape)
                a.backward(da, y)


    class Pad2D(TensorFunction):
        def forward(self, a: Tensor, padding: int, value: float) -> Tensor:
            a_samples, a_rows, a_cols = a.shape

            new_rows = a_rows + 2 * padding
            new_cols = a_cols + 2 * padding

            arr = np.full((a_samples, new_rows, new_cols), fill_value=value)

            arr = fill_padded_array(arr, a.data, padding)

            y = Tensor(arr, a.requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)

                a.children.append(y)

                self._cache = (
                    a,
                    padding,
                )

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            (
                a,
                padding,
            ) = self._cache

            if a.requires_grad:
                if len(dy.shape) == 3:
                    da = dy[:, padding:-padding, padding:-padding]
                elif len(dy.shape) == 3:
                    da = dy[padding:-padding, padding:-padding]
                a.backward(da, y)


    class ReLU(TensorFunction):
        def forward(self, a: Tensor) -> Tensor:
            """Specialised function for relu activation

            Args:
                a (Tensor):

            Returns:
                Tensor:
            """
            new_data = np.maximum(0.0, a.data)

            y = Tensor(new_data, requires_grad=a.requires_grad, operation=self)

            if is_grad_enabled():
                self.parents = (a,)

                a.children.append(y)

                self._cache = (a,)

            return y

        def backward(self, dy: np.ndarray, y: Tensor) -> None:
            (a,) = self._cache

            if a.requires_grad:
                da = np.greater(dy, 0.0).astype(np.float64)
                a.backward(da, y)
    \end{minted}

    \subsubsection{Jit Convolve Specific Functions}
    \begin{minted}{python}
    import numpy as np
    from numba import jit
    import time as t


    @jit(nopython=True, cache=True)
    def cpu_forward_convolve2d(
        output: np.ndarray, x: np.ndarray, k: np.ndarray, n_kernels: int
    ) -> np.ndarray:
        """Performs the forward pass of the Convolve2D Tensor operation

        Args:
            output (np.ndarray):
            x (np.ndarray):
            k (np.ndarray):
            n_kernels (int):

        Returns:
            np.ndarray:
        """
        n_samples = x.shape[0]
        for i in range(n_kernels):
            for j in range(n_samples):
                output[i] = _jit_cpu_valid_cross_correlate2d(x[j], k[i, j])

        return output

    @jit(nopython=True, cache=True)
    def cpu_x_and_k_backward_convolve2d(
        x_output: np.ndarray,
        k_output: np.ndarray,
        x: np.ndarray,
        k: np.ndarray,
        dy: np.ndarray,
        n_samples: int,
        n_kernels: int,
    ) -> tuple[np.ndarray, np.ndarray]:
        """Calculate the gradients for both input and kernels in the same loop

        Should be used if both x and k requires grad.

        Reduces runtime from [O(n_samples*n_kernels)]^2 to O(n_samples*n_kernels)

        Args:
            x_output (np.ndarray): Array to fill with input grads
            k_output (np.ndarray): Array to fill with kernel grads
            x (np.ndarray): input
            k (np.ndarray): kernels
            dy (np.ndarray): upstream grad
            n_samples (int): number of samples in x (x.shape[0])
            n_kernels (int): number of kernels

        Returns:
            tuple[np.ndarray, np.ndarray]: x_grads, k_grads
        """
        for i in range(n_kernels):
            for j in range(n_samples):
                x_output[j] += _jit_cpu_convolve2d(dy[i], k[i, j])
                k_output[i, j] = _jit_cpu_valid_cross_correlate2d(x[j], dy[i])

        return x_output, k_output

    @jit(nopython=True, cache=True)
    def cpu_k_backward_convolve2d(
        output: np.ndarray, x: np.ndarray, dy: np.ndarray, n_kernels: int
    ) -> np.ndarray:
        """Get input gradients for a convolutional layer

        Args:
            output (np.ndarray): array to fill with gradients
            x (np.ndarray): input
            dy (np.ndarray): upstream gradient
            n_kernels (int): number of kernels

        Returns:
            np.ndarray: gradients
        """
        n_samples = x.shape[0]
        for i in range(n_kernels):
            for j in range(n_samples):
                output[i, j] = _jit_cpu_valid_cross_correlate2d(x[j], dy[i])

        return output

    @jit(nopython=True, cache=True)
    def cpu_x_backward_convolve2d(
        output: np.ndarray, k: np.ndarray, dy: np.ndarray, n_samples: int, n_kernels: int
    ) -> np.ndarray:
        """Get input gradients for a convolutional layer

        Args:
            output (np.ndarray): array to fill with gradients
            k (np.ndarray): kernels
            dy (np.ndarray): upstream gradient
            n_samples (int): number of samples in input
            n_kernels (int): number of kernels

        Returns:
            np.ndarray: gradients
        """
        for i in range(n_kernels):
            for j in range(n_samples):
                output[j] += _jit_cpu_convolve2d(dy[i], k[i, j])

        return output

    @jit(nopython=True, cache=True)
    def _jit_cpu_convolve2d(a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """Returns the 2d convolution of two matrices

        (which is equivalent to full cross correlation of a and rot180(b))

        Args:
            a (np.ndarray):
            b (np.ndarray):

        Returns:
            np.ndarray:
        """
        return _jit_cpu_full_cross_correlate2d(a, _jit_rotate_180(b))

    @jit(nopython=True, cache=True)
    def _jit_rotate_180(b: np.ndarray) -> np.ndarray:
        """Rotates a given matrix by 180 degrees

        Args:
            b (np.ndarray): matrix to rotate

        Returns:
            np.ndarray: rotated matrix
        """
        rot90 = np.rot90(b)
        return np.rot90(rot90)

    @jit(nopython=True, cache=True)
    def _jit_cpu_full_cross_correlate2d(a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """Performs FULL cross-correlation between two numpy arrays.

        Args:
            a (np.ndarray):
            b (np.ndarray):

        Returns:
            np.ndarray:
        """
        a_rows, a_cols = a.shape
        b_rows, b_cols = b.shape

        # calculate output sizing (note: a+b-1 instead of a -b+1 for valid)
        out_rows = a_rows + b_rows - 1
        out_cols = a_cols + b_cols - 1

        out = np.zeros((out_rows, out_cols), dtype=np.float64)

        # slide b over a - including only partial coverage
        for m in range(out_rows):
            for n in range(out_cols):
                s = 0.0  # sum value

                # compute dot product of a and b that overlap
                for p in range(b_rows):
                    for q in range(b_cols):
                        # get positions in a that correspond with b[p, q]
                        a_row = m - p
                        a_col = n - q

                        # check the kernel is in bounds
                        if 0 <= a_row < a_rows and 0 <= a_col < a_cols:
                            s += a[a_row, a_col] * b[p, q]

                out[m, n] = s  # add sum to output

        return out

    @jit(nopython=True, cache=True)
    def _jit_cpu_valid_cross_correlate2d(a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """Performs VALID cross correlation between two numpy arrays

        Args:
            a (np.ndarray):
            b (np.ndarray):

        Returns:
            np.ndarray:
        """
        a_rows, a_cols = a.shape
        b_rows, b_cols = b.shape

        # calculate output sizing
        out_rows = a_rows - b_rows + 1
        out_cols = a_cols - b_cols + 1

        out = np.zeros((out_rows, out_cols), dtype=np.float64)

        out_rows, out_cols = out.shape

        # slide b over a
        for m in range(out_rows):
            for n in range(out_cols):
                sub_matrix = a[
                    m : m + b_rows, n : n + b_cols
                ]  # get the parts of a which overlap with b at that time

                s = 0.0  # sum (numpy sum function was producing errors)
                for p in range(b_rows):
                    for q in range(b_cols):
                        s += (
                            sub_matrix[p, q] * b[p, q]
                        )  # computes the dot product of sub_matrix and b

                out[m, n] = s  # add to output array

        return out
    \end{minted}

    \subsubsection{Jit Functions}
    \begin{minted}{python}
    from numba import jit
    import numpy as np

    @jit(nopython=True, cache=True)
    def fill_padded_array(
        array_to_fill: np.ndarray, array_fill_with: np.ndarray, padding: int
    ) -> np.ndarray:
        samples, _, _ = array_fill_with.shape
        for sample in range(samples):
            array_to_fill[sample, padding:-padding, padding:-padding] = array_fill_with[
                sample
            ]

        return array_to_fill
    \end{minted}

    \subsection{Neural Network Library}

    Again, the code below will show the implementation of any classes, data structures and algorithms
    mentioned and described in the \myhy{NNLibDesignStage}{design stage} as well as any code regarding
    the \myhy{}{neural network library objectives}.

    \subsubsection{Modules}
    \begin{minted}{python}
    from abc import ABC
    import collections
    import numpy as np
    import pickle

    from nea.ml.autograd import Tensor, to_tensor, TensorFunction, tensor_exp, tensor_sum
    from nea.ml.autograd.consts import Tensorable


    class Parameter(Tensor):
        """
        Represents a parameter
        """

        def __init__(
            self,
            shape: tuple[int] = None,
            requires_grad: bool = True,
            operation: TensorFunction = None,
        ) -> None:
            if shape is not None:
                data = np.random.randn(*shape)
                super().__init__(data, requires_grad=requires_grad, operation=operation)
            else:
                raise ValueError("shape must be specified and cannot be left as None")


    class Module(ABC):
        """Basis of all layers"""

        def __call__(self, x: Tensorable) -> Tensor:
            return self.forward(to_tensor(x))

        def forward(self, x: Tensorable) -> Tensor:
            """Forward propogation of module

            Args:
                x (Tensorable): Input data

            Returns:
                Tensor:
            """
            raise NotImplementedError("Cannot call forward on raw module")

        @property
        def params(self) -> list[Parameter | Tensor]:
            """Gets all parameters inside a modules from the self.__dict__
            Also gets any tensors with requires_grad = True,
            and the parameters from any other module

            Returns:
                list[Parameter | Tensor]:
            """

            params = []

            #get all params associated with the module
            for param in self.__dict__.values():
                if isinstance(param, Module):
                    params += param.params
                elif isinstance(param, ModuleList):
                    params += param.params
                elif isinstance(param, Parameter):
                    params.append(param)
                elif isinstance(param, Tensor):
                    if param.requires_grad:
                        params.append(param)

            return params

        def save(self, file_path: str) -> None:
            with open(file_path, "wb") as fh:
                pickle.dump(self, fh)

        @staticmethod
        def load(self, file_path: str) -> None:
            with open(file_path, "rb") as fh:
                return pickle.load(fh)

    # allows multiple modules to be stored in a list (useful for the residual layers)
    class ModuleList(Module, collections.abc.Sequence):
        def __init__(self, modules: list[Module]) -> None:
            super().__init__()
            self.modules = modules

        @property
        def params(self) -> list[Parameter | Tensor]:
            params = []

            for module in self.modules:
                for param in module.__dict__.values():
                    if isinstance(param, Module):
                        params += param.params
                    elif isinstance(param, ModuleList):
                        params += param.params
                    elif isinstance(param, Parameter):
                        params.append(param)
                    elif isinstance(param, Tensor):
                        if param.requires_grad:
                            params.append(param)

                return params

        def __getitem__(self, index: int) -> Module:
            return self.modules[index]

        def __len__(self) -> int:
            return len(self.modules)


    class Dense(Module):
        """Fully connected layer

        Args:
            Module (_type_):
        """

        def __init__(self, n_inputs: int, n_outputs: int, bias: bool = True) -> None:
            """Instantiates a new dense layer

            Args:
                n_inputs (int): number of inputs to layer
                n_outputs (int): desired number of output neurons
                bias (bool, optional): whether to add a bias layer or not. 
                Defaults to True.
            """
            super().__init__()
            self.weights = Parameter((n_inputs, n_outputs))
            if bias:
                self.bias = Parameter((n_outputs,))

        def forward(self, x: Tensorable) -> Tensor:
            """Propogates input data through dense layer

            Args:
                x (Tensorable): input data

            Returns:
                Tensor:
            """
            y = x @ self.weights
            if self.bias:
                y = y + self.bias

            return y


    class Conv2D(Module):
        """A Convolutional layer

        Args:
            Module (_type_):
        """

        def __init__(
            self,
            x_shape: tuple[int, int, int],
            kernel_size: int,
            n_kernels: int,
            bias: bool = False,
            padding: int = None,
            padding_value: float = None,
        ) -> None:
            super().__init__()
            assert len(x_shape) == 3, "Input must be of shape (n_samples, *, *)"
            if padding_value:
                assert padding, """Must define amount of padding 
                if padding value is not None"""

            self.n_kernels = n_kernels
            self.x_shape = x_shape

            x_samples, x_width, x_height = self.x_shape

            self.padding = padding
            self.padding_value = padding_value
            if self.padding:
                self.x_shape = (
                    x_samples,
                    x_width + 2 * self.padding,
                    x_height + 2 * self.padding,
                )
                x_samples, x_width, x_height = self.x_shape

            self.output_shape = (
                self.n_kernels,
                x_width - kernel_size + 1,
                x_height - kernel_size + 1,
            )
            self.kernels_shape = (n_kernels, x_samples, kernel_size, kernel_size)

            self.kernels = Parameter(self.kernels_shape)

            self.bias = bias
            self.biases = None
            if self.bias:
                self.biases = Parameter(self.output_shape)

        def forward(self, x: Tensor) -> Tensor:
            if self.padding:
                x = x.pad2D(padding=self.padding, value=self.padding_value)
            return x.convolve2d(k=self.kernels, b=self.biases)


    class Reshape(Module):
        def __init__(self, desired_shape: tuple[int]) -> None:
            super().__init__()
            self.desired_shape = desired_shape

        def forward(self, x: Tensor) -> Tensor:
            return x.reshape(self.desired_shape)


    class MinMaxNormalization(Module):
        def __init__(self) -> None:
            super().__init__()

        def forward(self, x: Tensor) -> Tensor:
            x_max = np.max(x.data)
            x_min = np.min(x.data)

            return (x - x_min) / ((x_max - x_min) + 10**-100)

    # ==========================
    #        Activations
    # ==========================


    class Tanh(Module):
        """Tanh activation layer

        Args:
            Module (_type_):
        """

        def __init___(self) -> None:
            super().__init__()

        def forward(self, x: Tensor) -> Tensor:
            output = (
                (tensor_exp(x) - tensor_exp(-x)) 
                / (tensor_exp(x) + tensor_exp(-x))
            )
            return output


    class Sigmoid(Module):
        """Sigmoid activation layer

        Args:
            Module (_type_):
        """

        def __init__(self) -> None:
            super().__init__()

        def forward(self, x: Tensor) -> Tensor:
            output = 1. / (1. + tensor_exp(-x))
            return output


    class Softmax(Module):
        def __init__(self) -> None:
            super().__init__()

        def forward(self, x: Tensor, dim: int = -1) -> Tensor:
            z = tensor_exp(x)
            output = z / tensor_sum(z, dim=dim, keepdims=True)
            return output


    class ReLU(Module):
        def __init__(self) -> None:
            super().__init__()

        def forward(self, x: Tensor) -> Tensor:
            return x.relu()


    # ==========================
    #          Losses
    # ==========================


    class MSE(Module):
        def __init__(self) -> None:
            super().__init__()

        def __call__(self, predicted: Tensor, true: Tensorable) -> Tensor:
            return self.forward(predicted=predicted, true=true)

        def forward(self, predicted: Tensor, true: Tensorable) -> Tensor:
            loss: Tensor = predicted - true
            loss = (1 / true.shape[0]) * (loss.T() @ loss)
            return loss


    class CrossEntropy(Module):
        def __init__(self) -> None:
            super().__init__()

        def __call__(self, predicted: Tensor, true: Tensorable) -> Tensor:
            return self.forward(predicted=predicted, true=true)

        def forward(self, predicted: Tensor, true: Tensorable) -> Tensor:
            prod = true * predicted.log()
            return -(prod.sum(dim=0))


    class AlphaLoss(Module):
        def __init__(self) -> None:
            super().__init__()

        def __call__(
            self,
            true_value: Tensor,
            predicted_value: Tensor,
            mcts_pol: Tensor,
            net_pol: Tensor,
        ) -> Tensor:
            return self.forward(
                true_value=true_value,
                predicted_value=predicted_value,
                mcts_pol=mcts_pol,
                net_pol=net_pol,
            )

        def forward(
            self,
            true_value: Tensor,
            predicted_value: Tensor,
            mcts_pol: Tensor,
            net_pol: Tensor,
        ) -> Tensor:
            val_sqe = (true_value - predicted_value) ** 2
            mcts_pol_t = mcts_pol.T()
            net_pol_log = net_pol.log()
            pol_bcel = mcts_pol_t @ net_pol_log
            pol_bcel = pol_bcel.sum().sum().sum()
            return val_sqe - pol_bcel

    \end{minted}

    \subsubsection{Optimizers}
    \begin{minted}{python}
    from abc import ABC

    from nea.ml.autograd import Tensor
    from nea.ml.nn import Parameter


    class Optimizer(ABC):
        def __init__(
            self,
            params: list[Tensor | Parameter],
            lr: float = 0.001,
            regulization: float = 0,
        ) -> None:
            self.params = params
            self.lr = lr
            self.regulization = regulization

        def step(self) -> None:
            """
            Updates each parameters value depending on gradients
            """
            raise NotImplementedError("Cannot call step on base 'Optimizer' class")

        def zero_grad(self) -> None:
            """
            Resets the gradient of each parameter
            """
            for param in self.params:
                param.zero_grad()


    class SGD(Optimizer):
        def __init__(
            self,
            params: list[Tensor | Parameter],
            lr: float = 0.0001,
            regulization: float = 0.0001,
        ) -> None:
            super().__init__(params, lr, regulization)

        def step(self) -> None:
            for param in self.params:
                # update parameter with regularization to prevent overfitting
                param._data = (
                    param._data
                    - (self.lr * param.grad)
                    - (self.lr * self.regulization * (param._data ** 2))
                )
    \end{minted}

    \subsection{Reinforcement Learning Solution}
    The same as previous sections, a link to the design stage can be found \myhy{RLSDesignStage}{here}, and
    related objectives can also be found \myhy{RLS Objs}{here}.

    \subsubsection{Node and MCTS}
    \begin{minted}{python}
    from __future__ import annotations
    from copy import deepcopy
    from collections import deque

    import numpy as np

    from nea.console_checkers import CheckersGame
    from nea.mcts.consts import ACTION, ACTION_TO_IDX, IDX_TO_ACTION


    class Node:
        """Creates a new Node object"""

        def __init__(
            self,
            game: CheckersGame,
            parent: Node = None,
            terminal: bool = False,
            action_taken: ACTION = None,
            reward: float = None,
            **kwargs,
        ) -> None:
            self._game = game
            self.colour = self._game.player

            self._state = self._game.board
            self._parent = parent
            self.children: list["Node"] = []
            self._action_taken = action_taken
            self.reward = reward
            self._available_moves_left = self._init_available_moves()
            self.terminal = terminal

            self.visit_count, self.value_count = 0, 0

            self.kwargs = kwargs

        def select_child(self) -> Node:
            """Selects the best child from a fully expanded node using UCB

            Returns:
                Node: Best child
            """
            best_child = None
            best_ucb = -np.inf

            for child in self.children:
                ucb = self._calculate_ucb(child)
                if ucb > best_ucb:
                    best_child = child
                    best_ucb = ucb

            return best_child

        def expand(self) -> Node:
            """Random expansion of a node

            Returns:
                Node: New child
            """
            if self.n_available_moves_left == 0 and self.children:
                return self.select_child()

            # get a random action
            random_move_idx = np.random.choice(self.n_available_moves_left)
            random_action = self._available_moves_left[random_move_idx]

            self._available_moves_left.remove(random_action)

            child_game = deepcopy(self._game)
            # complete the action
            _, _, terminal, reward = child_game.step(random_action)

            # create new node for tree after this action
            child = Node(
                child_game,
                parent=self,
                terminal=terminal,
                action_taken=random_action,
                reward=reward,
                eec=self.kwargs["eec"],
            )
            self.children.append(child)

            return child

        def backprop(self, reward: int) -> None:
            """Backpropgates through graph, updating value count, visit count.

            Args:
                reward (int): reward of terminal state
            """
            self.visit_count += 1
            self.value_count += reward

            if self._parent is not None:
                if self._parent.colour != self.colour:
                    reward *= -1
                self._parent.backprop(reward)


    class MCTS:
        """
        Monte Carlo Tree Search class used to search for lines until termination in a given state
        """

        def __init__(self, **kwargs) -> None:
            """Creates a new MCTS object

            Keyword Args:
                eec: exploration constant -> Higher EEC = more exploration
                n_searches: number of searches
            """

            self.kwargs = kwargs
            self._root: Node = None

        def build_tree(self, root: CheckersGame) -> None:
            """Builds a new tree

            Args:
                root (CheckersGame): New state to root the tree from
            """
            self._root = Node(root, eec=self.kwargs["eec"])

            for _ in range(int(self.kwargs["n_searches"])):
                node = self._root

                if node.n_available_moves_left == 0 and node.children:
                    node = node.select_child()

                while not node.terminal:
                    node = node.expand()

                node.backprop(node.reward)

        def get_action_probs(self) -> np.ndarray:
            """Gets array of probabilities of action based on tree

            Returns:
                np.ndarray: Array of probabilities
            """
            p = np.zeros(
                (8, 8, 8)
            )  # (8x8) shows where to take piece from. Final 8 shows what direc e.g.
            # idx 0 = row+1,col+1, idx 1 = row+1, col-1 etc.
            for child in self._root.children:
                piece_moved, moved_to = child.action_taken
                row_change = moved_to[0] - piece_moved[0]
                col_change = moved_to[1] - piece_moved[1]
                direc_idx = ACTION_TO_IDX[(row_change, col_change)]
                p[direc_idx, piece_moved[0], piece_moved[1]] = child.visit_count

            p /= np.sum(p)
            return p
    \end{minted}

    \pagebreak
    
    \subsubsection{AlphaNode and AlphaMCTS}
    
    \begin{minted}{python}
    class AlphaNode(Node):
        def __init__(
            self,
            game: CheckersGame,
            parent: Node = None,
            terminal: bool = False,
            action_taken: ACTION = None,
            reward: float = None,
            prior_prob: float = None,
            **kwargs,
        ) -> None:
            super().__init__(
                game=game,
                parent=parent,
                terminal=terminal,
                action_taken=action_taken,
                reward=reward,
                **kwargs,
            )
            self.prior_prob = prior_prob

        def expand(self, policy: np.ndarray) -> AlphaNode:
            child = None
            for action, prob in np.ndenumerate(policy):
                if prob > 0: # for every move where the model thinks it could win
                    child_game = deepcopy(self._game)
                    action = AlphaNode._convert_action_idx_to_action_game(action)
                    self._available_moves_left.remove(action)
                    # take this move
                    _, _, terminal, reward = child_game.step(action)

                    # add node to tree after this move was taken
                    child = AlphaNode(
                        game=child_game,
                        parent=self,
                        terminal=terminal,
                        action_taken=action,
                        reward=reward,
                        prior_prob=prob,
                        **self.kwargs,
                    )

                    self.children.append(child)

            return child


    class AlphaMCTS(MCTS):
        def __init__(self, model: AlphaModel, **kwargs) -> None:
            super().__init__(**kwargs)
            self.model = model

        def alpha_build_tree(self, root: CheckersGame, prior_states: deque) -> None:
            """_summary_

            Args:
                root (CheckersGame): _description_
                prior_states (list[np.ndarray]): _description_
            """
            if len(prior_states) < 5:
                self.build_tree(root)
                return

            self._root = AlphaNode(root, eec=self.kwargs["eec"])

            for _ in range(int(self.kwargs["n_searches"])):
                prior_states_temp = deepcopy(prior_states) # make a copy of original 
                node = self._root
                policy, value = None, None

                while node.n_available_moves_left == 0 and node.children:
                    node = node.select_child()
                    prior_states_temp.append(node._state) 
                    # states is deque so automatically enforces maxlen

                if not node.terminal:
                    input_tensor = self._create_input_tensor(prior_states_temp)
                    with no_grad():
                        policy, value = self.model(input_tensor) # get policy and value
                    policy *= self._get_valid_moves_as_action_tensor(node=node)
                    policy /= policy.sum().sum().sum()

                    node.expand(policy.data)

                    value = value.data
                else:
                    value = node.reward

                node.backprop(value)
    \end{minted}

    \subsubsection{Alpha Network}
    The code in this section specifically implements the model shown in \hyperlink{page.38}{\underline{Figure 28}}.
    \begin{minted}{python}
    import numpy as np

    from nea.ml import nn
    from nea.ml.autograd import Tensor, Tensorable
    
    # num hidden conv can be changed to preference
    class ResidualLayer(nn.Module):
        def __init__(self, num_hidden_conv: int) -> None:
            super().__init__()
            # create components that will be used in forward
            self.Conv1 = nn.Conv2D(
                (num_hidden_conv, 8, 8),
                kernel_size=3,
                n_kernels=num_hidden_conv,
                padding=1,
                padding_value=0,
            )
            self.ReLU1 = nn.ReLU()
            self.Conv2 = nn.Conv2D(
                (num_hidden_conv, 8, 8),
                kernel_size=3,
                n_kernels=num_hidden_conv,
                padding=1,
                padding_value=0,
            )
            self.ReLU2 = nn.ReLU()

            self.MinMaxNorm = nn.MinMaxNormalisation()

        def forward(self, x: Tensorable) -> Tensor:
            """Input Shape of (128, 8, 8)

            Output Shape of (128, 8, 8)

            Args:
                x (Tensorable): input tensor

            Returns:
                Tensor:
            """
            original_input = x.data
            x = self.Conv1(x)
            x = self.ReLU1(x)
            x = self.Conv2(x)
            x += original_input
            x = self.ReLU2(x)
            # normalisation
            x = self.MinMaxNorm(x)            
            return x


    class ConvolutionalLayer(nn.Module):
        """This is only used as the starting block

        Args:
            nn (_type_):
        """

        def __init__(self, num_hidden_conv: int) -> None:
            super().__init__()
            # create components that will be used in forward
            self.Conv1 = nn.Conv2D(
                (5, 8, 8),
                n_kernels=num_hidden_conv,
                kernel_size=3,
                padding=1,
                padding_value=0,
            )
            self.ReLU = nn.ReLU()

        def forward(self, x: Tensor) -> Tensor:
            """Input shape of (5, 8, 8)

            Output shape of (128, 8, 8)

            Args:
                x (Tensor): input tensor

            Returns:
                Tensor:
            """
            x = self.Conv1(x)
            x = self.ReLU(x)
            return x


    class PolicyHead(nn.Module):
        def __init__(self, num_hidden_conv: int) -> None:
            super().__init__()
            # create components that will be used in forward
            self.Conv1 = nn.Conv2D((num_hidden_conv, 8, 8), kernel_size=1, n_kernels=8)
            self.ReLU1 = nn.ReLU()
            self.Softmax = nn.Softmax()

        def forward(self, x: Tensor) -> Tensor:
            """Input shape of (128, 8, 8)

            Output shape of (8, 8, 8)

            Args:
                x (Tensor): input tensor

            Returns:
                Tensor:
            """
            x = self.Conv1(x)
            x = self.ReLU1(x)
            x = self.Softmax(x)
            return x


    class ValueHead(nn.Module):
        def __init__(self, num_hidden_conv: int) -> None:
            super().__init__()
            # create components that will be used in forward
            self.Conv1 = nn.Conv2D((num_hidden_conv, 8, 8), kernel_size=1, n_kernels=8)
            self.ReLU1 = nn.ReLU()
            self.Reshape = nn.Reshape((1, 8 * 8 * 8))
            self.Dense1 = nn.Dense(8 * 8 * 8, 256)
            self.ReLU2 = nn.ReLU()
            self.Dense2 = nn.Dense(256, 1)
            self.Tanh = nn.Tanh()

        def forward(self, x: Tensor) -> Tensor:
            """Input shape of (128, 8, 8)

            Output is scalar [-1, 1]

            Args:
                x (Tensor): input tensor

            Returns:
                Tensor:
            """
            x = self.Conv1(x)
            x = self.ReLU1(x)
            x = self.Reshape(x)
            x = self.Dense1(x)
            x = self.ReLU2(x)
            x = self.Dense2(x)
            x = self.Tanh(x)
            return x


    class AlphaModel(nn.Module):
        def __init__(self, n_res_layers: int = 5, num_hidden_conv: int = 32) -> None:
            super().__init__()
            # combine previously defined modules to form new more complex network
            self.first_layer = ConvolutionalLayer(num_hidden_conv=num_hidden_conv)
            self.res_layers = nn.ModuleList(
                [
                    ResidualLayer(num_hidden_conv=num_hidden_conv)
                    for r in range(n_res_layers)
                ]
            )
            # define two output heads
            self.policy_head = PolicyHead(num_hidden_conv=num_hidden_conv)
            self.value_head = ValueHead(num_hidden_conv=num_hidden_conv)

        def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:
            """Input shape (5, 8, 8)

            Output of tuple (policy, value)

            Policy has shape (8, 8, 8)

            Value is scalar

            Args:
                x (Tensor): input tensor

            Returns:
                tuple[Tensor, Tensor]: (policy, value)
            """
            x = self.first_layer(x)
            for res in self.res_layers:
                x = res(x)
            pol = self.policy_head(x)
            val = self.value_head(x)
            return pol, val
    \end{minted}

    \subsubsection{Agent}

    \subsubsubsection{Memory Types}
    \begin{minted}{python}
    from dataclasses import dataclass


    @dataclass
    class SAP:
        state: np.ndarray
        mcts_action_probs: np.ndarray
        player: str


    @dataclass
    class SPV:
        state: np.ndarray
        mcts_action_probs: np.ndarray
        true_value: float
    \end{minted}

    \subsubsubsection{Agent}
    \begin{minted}{python}
    from collections import deque
    import gc
    import itertools
    import random
    import math

    from nea.ml.nn import Optimizer, SGD, AlphaLoss
    from nea.ml.autograd import Tensor
    from nea.mcts import AlphaMCTS, AlphaNode
    from nea.network import AlphaModel
    from nea.console_checkers import CheckersGame
    from nea.agent.memory_types import SAP, SPV
    from nea.agent.consts import ACTION_SPACE


    class AlphaZero:
        def __init__(
            self,
            optimizer: Optimizer = SGD,
            mcts_epochs: int = 3,
            n_example_games: int = 10,
            max_training_examples: int = 500,
            nn_epochs: int = 3,
            n_compare_games: int = 10,
            eec: float = 1.41,
            n_mcts_searches: int = 100,
            replace_win_pct_threshold: int = 60,
            save: bool = False,
        ) -> None:
            self.prev_model = None
            self.new_model = None
            self.optimizer = optimizer
            self.hyperparams = {
                "mcts_epochs": mcts_epochs,
                "n_example_games": n_example_games,
                "nn_epochs": nn_epochs,
                "n_compare_games": n_compare_games,
                "eec": eec,
                "n_mcts_searches": n_mcts_searches,
                "replace_win_pct_threshold": replace_win_pct_threshold,
            }

            self.loss = AlphaLoss()
            self.save = save

            def train(self, initial_model: AlphaModel) -> None:
                self.prev_model = initial_model
                if self.new_model is None:
                    self.new_model = initial_model

                for mcts_epoch in range(self.hyperparams["mcts_epochs"]):
                    training_examples = deque()

                    gc.collect()

                    print(f"MCTS EPOCH: {mcts_epoch + 1}")
                    print("GETTING EXAMPLE GAMES")
                    for example_game in tqdm(range(self.hyperparams["n_example_games"])):
                        game_saps, reward, player = self._get_example_saps()
                        game_spvs = self._convert_saps_to_spvs(game_saps, player, reward)

                        for item in game_spvs:
                            training_examples.append(item)

                    print(f"BEGINNING NN TRAINING ON {len(training_examples)} EXAMPLES")
                    for epoch in range(int(self.hyperparams["nn_epochs"])):
                        gc.collect()
                        self._train_nn(training_examples)

                    print("PLAYING COMPARISON GAMES")
                    if self._play_compare_games():
                        self.prev_model = self.new_model

                        if self.save:
                            file_path = (
                                f"{self.hyperparams["n_mcts_searches"]}ns-"
                                + f"{self.hyperparams["eec"]}ec-"
                                + f"{self.hyperparams["n_example_games"]}te-"
                                + f"{self.hyperparams["n_compare_games"]}cg-"
                                + f"{self.hyperparams["replace_win_pct_threshold"]}rt"
                            )
                            self.new_model.save(file_path=file_path)

                            print(f"Model saved to file {file_path}.pkl")
                    gc.collect()

                return self.prev_model

        def _convert_saps_to_spvs(
            self, game_saps: deque[SAP], player: str, reward: float
        ) -> deque[SPV]:
            """
            Converts SAPs to SPVs
            """
            len_game_saps = len(game_saps)
            game_spvs = deque(maxlen=len_game_saps)

            for item in game_saps:
                value = reward if item.player == player else reward * -1
                game_spvs.append(SPV(item.state, item.mcts_action_probs, value))

            return game_spvs
    \end{minted}

    \section{Testing}
    \secttoc

    \subsection{Checkers Game}

    Please see the 'Checkers Game Testing' video.

    \subsection{Autodifferentiation engine}

    To test the autodifferentiation module I decided to write some unit tests by computating some example
    derivatives for each operation by hand and then checking the module produces these values inside a function.
    To improve upon this process, I used pytest. A unit-testing python library which allowed me to run all of my tests
    at once, raised flags on any tests that failed and allowed me to debug these failed tests individually.

    Below I will put some example testing functions (ranging in difficulty of derivative calculation) and there will
    also be an image displaying the final result of my testing for this module.

    \subsubsection{Example tests}

    \begin{minted}{python}
        def test_subtraction_funcs():
        a = Tensor(1, requires_grad=True)
        b = Tensor(2, requires_grad=True)

        y = b - a
        assert y == 1

        y.backward()
        assert a.grad == -1

        b -= 1
        b.zero_grad()
        b.backward()
        assert b.grad == 1

    def test_multiplication_funcs():
        a = Tensor(1, requires_grad=True)
        b = Tensor(2, requires_grad=True)

        y = a * b
        assert y == 2

        y.backward()
        assert a.grad == 2
        assert b.grad == 1

    def test_matmul_funcs():
        a = Tensor([[1, 2], [2, 1]], requires_grad=True)
        b = Tensor([[2, 1], [1, 2]], requires_grad=True)

        y = a @ b

        assert y.shape == (2, 1) or (2,)
        assert np.array_equal(y.data, np.array([[4, 5], [5, 4]]))

        y.backward()

        assert np.array_equal(y.grad, np.array([[1, 1], [1, 1]]))
        assert np.array_equal(a.grad, np.array([[3, 3], [3, 3]]))

    def test_pad2d_funcs():
        a = Tensor([1], requires_grad=True)
        a = a.reshape((1, 1, 1))
        b = a.pad2D(1, 0)

        assert (b.data == [[0, 0, 0], [0, 1, 0], [0, 0, 0]]).all()

        b.backward()

        assert b.grad.shape == (1, 3, 3)
        assert a.grad.shape == (1, 1, 1)

    def test_convolve2d_backward_funcs():
        x = Tensor([[1, 6, 2], [5, 3, 1], [7, 0, 4]], requires_grad=True)
        k = Tensor([[1, 2], [-1, 0]], requires_grad=True)

        x = x.reshape((1, 3, 3))  # define 1 sample of 3x3
        k = k.reshape((1, 1, 2, 2))  # define 1 kernel for 1 sample with 2x2 kernel

        y = x.convolve2d(k=k)

        y.backward()

        # calculated by hand
        assert (x.grad == [[[0, -1, -1], [2, 2, 0], [2, 3, 1]]]).all()
        assert (k.grad == [[[[15, 12], [15, 8]]]]).all()
    \end{minted}

    In this final function - during the forward and backward passes, the function makes call to 
    a jit compiled function (using the numba library). This caused an error when run in
    the same file as the other tests (due to the VSCode debugger) - and so was moved in to a separate file.

    \subsubsection{Final results}

    Below is a screenshot of the final output from pytest. It lists the name of all the test functions,
    and their completion status (passed or failed).

    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{passing_tests.png}
        \caption{Pytesting results}
    \end{figure}
     
    \subsection{Neural Network Library}

    Please see the 'Neural Network Library Testing' video.

    \subsection{Reinforcement Learning Solution}

    Please see the 'Reinforcement Learning Testing' video.

    
    \pagebreak
    \section{Evaluation}

    \subsection{Checkers Game}
    \subsubsection{Objective Achievement}
    Below is a copy of the objectives for easy referencing.
    \begin{enumerate}
        \item An easy-to-use and intuitive graphical interface for the checkers game
        \begin{enumerate}
            \item A working version of checkers - obeying all rules stated in \myhy{checkers}{1.2.1.1}
            \item A small text tutorial on the rules of checkers
            \item A visual representation of the checkers board
            \item A method of displaying all legal available moves in a selected position
            \item Error messages if an illegal move is attempted
        \end{enumerate}
        \item Implementation of various decision-making agents for users to play against
        \begin{enumerate}
            \item Option for the user to choose which agent to play against (MCTS, Full Agent)
            \item Ability to adjust hyperparameters of the agents from the checkers game interface (see \myhy{RLS Objs}{RLS Objectives a-e})
        \end{enumerate}
    \end{enumerate}

    \subsubsection{Solution Improvements}



    \subsection{Auto-differentiation engine}
    \subsubsection{Objective Achievement}
    \begin{enumerate}
        \item A unit-tested autograd engine capable of calculating derivatives on a variety of functions inlcuding:
        \begin{enumerate}
            \item Addition
            \item Negation
            \item Multiplication
            \item Division
            \item Matrix Multiplication
            \item Powers
            \item Mean
            \item Sum
            \item Natural Log
            \item Exponentiation
            \item Transpose
            \item Padding
            \item 2D Convolution
        \end{enumerate}
    \end{enumerate}
    
    \subsubsection{Solution Improvements}



    \subsection{Neural Network Library}
    \subsubsection{Objective Achievement}
    \begin{enumerate} 
        \item A modular machine learning library including:
        \begin{enumerate}
            \item The ability to save a model
            \item Support for storing a collection of layers/modules
            \item Dense/Fully Connected Layer
            \item 2D Convolution Layer
            \item Reshape Layer
            \item Activation Layers:
            \begin{enumerate}
                \item ReLU
                \item Softmax
                \item Sigmoid
                \item Tanh
            \end{enumerate} 
            \item Gradient descent optimization
            \item Loss Functions:
            \begin{enumerate}
                \item Mean Squared error
                \item AlphaZero's Loss Function
            \end{enumerate}
        \end{enumerate}
    \end{enumerate}

    \subsubsection{Solution Improvements}

    \subsection{Reinforcement Learning Solution}
    \subsubsection{Objective Achievement}
    \begin{enumerate}
        \item A training algorithm with hyperparameters of:
        \begin{enumerate}
            \item Number of example games to be played
            \item Number of comparison games to be played
            \item Number of searches the MCTS completes
            \item The exploration exploitation coefficient for UCB in MCTS
            \item The percentage of comparison games the new network must win to replace the previous
            \item The number of epochs for neural network training
            \item The number of iterations where a new network is trained
        \end{enumerate}
        \item The functionality to load a saved agent and play against it
    \end{enumerate}

    \subsubsection{Solution Improvements}



    \subsection{Third Party Reflection}



    \section{Research Log}

    \subsection{Checkers}
    \subsubsection{Rules}
    \noindent \url{https://www.mastersofgames.com/rules/checkers-rules.htm}
    \subsubsection{Possible States}
    \noindent \url{https://math.stackexchange.com/questions/2240519/how-to-find-the-amount-of-possible-piece-configurations-in-the-game-of-checkers}
    
    \subsection{Neural Networks}
    \noindent \url{https://www.ibm.com/topics/neural-networks}

    \subsubsection{Vectorization of summations}
    \noindent \url{https://courses.cs.washington.edu/courses/cse446/20wi/Lecture8/08_Regularization.pdf}

    \subsubsection{Convolutional Networks}
    \noindent \url{https://www.youtube.com/watch?v=Lakz2MoHy6o}\\

    \subsection{Automatic Differentiation}
    \noindent \url{https://en.wikipedia.org/wiki/Automatic_differentiation}

    \subsection{AlphaZero}
    \noindent \url{https://ai.stackexchange.com/questions/13156/does-alphazero-use-q-learning}
    \noindent \url{https://suragnair.github.io/posts/alphazero.html}
    Explanations of AlphaZero's loss function
    \noindent \url{https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf} Methods and Neural Network architecture section

    \noindent \url{https://liacs.leidenuniv.nl/~plaata1/papers/CoG2019.pdf}
    AlphaZero policy explanation

    \subsection{Existing Auto differentiation systems}
    \noindent \url{https://medium.com/@utsavstha/jax-vs-pytorch-a-comprehensive-comparison-for-deep-learning-10a84f934e17}
    PyTorch vs JAX

    \subsection{Reinforcement Learning}
    \noindent \url{https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf} What is reinforcement learning?\\
    \noindent \url{https://smartlabai.medium.com/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc} Intro to reinforcement learning\\
    \noindent \url{https://medium.com/the-official-integrate-ai-blog/understanding-reinforcement-learning-93d4e34e5698} Different types of reinforcement learning\\
    \noindent \url{https://neptune.ai/blog/model-based-and-model-free-reinforcement-learning-pytennis-case-study} Model-based vs Model-free learning\\
\end{document}